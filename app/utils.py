import os
import torch
import numpy as np
import random
import logging
import json
import yaml
import gradio as gr  # type: ignore[import-untyped]
from typing import Callable, Iterator, Tuple, List, Optional, Any, Dict
from pathlib import Path
from transformers import PreTrainedTokenizerBase

# -----------------------------------------------------------------------------
# Path: app/utils.py
# Title: SNNãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ (System Utilities)
# Description: ãƒ‡ãƒã‚¤ã‚¹ç®¡ç†ã€ãƒ­ã‚®ãƒ³ã‚°ã€Gradio UIã€ãƒ‡ãƒ¼ã‚¿å‡¦ç†(collate_fn)ãªã©ã®å…±é€šæ©Ÿèƒ½ã€‚
#              Mac (MPS) ç’°å¢ƒã§ã®SpikingJellyã®ãƒ­ã‚°æŠ‘åˆ¶æ©Ÿèƒ½ã‚’å«ã¿ã¾ã™ã€‚
#              ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ workspace ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é›†ç´„ã™ã‚‹ã‚ˆã†ä¿®æ­£ã€‚
# -----------------------------------------------------------------------------

# --- Logging Setup for SpikingJelly Noise Reduction ---
# SpikingJellyã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§INFOãƒ¬ãƒ™ãƒ«ã§cupyã®ä¸åœ¨ã‚’è­¦å‘Šã™ã‚‹ãŸã‚ã€WARNINGä»¥ä¸Šã«å¼•ãä¸Šã’ã¾ã™ã€‚
logging.getLogger('spikingjelly').setLevel(logging.WARNING)
logging.getLogger('spikingjelly.activation_based.base').setLevel(
    logging.WARNING)
logging.getLogger('spikingjelly.activation_based.auto_cuda').setLevel(
    logging.WARNING)

# --- Device & Reproducibility ---


def get_device(use_cuda: bool = True) -> torch.device:
    """
    åˆ©ç”¨å¯èƒ½ãªæœ€é©ãªãƒ‡ãƒã‚¤ã‚¹ã‚’å–å¾—ã—ã¾ã™ã€‚
    Priority: CUDA > MPS (Apple Silicon) > CPU
    """
    if use_cuda and torch.cuda.is_available():
        return torch.device("cuda")
    elif torch.backends.mps.is_available():
        return torch.device("mps")
    else:
        return torch.device("cpu")

# äº’æ›æ€§ã®ãŸã‚ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ãƒ»ãƒ©ãƒƒãƒ‘ãƒ¼ (æ—¢å­˜ã‚³ãƒ¼ãƒ‰ãŒ get_auto_device ã‚’å‘¼ã‚“ã§ã„ã‚‹ãŸã‚)


def get_auto_device() -> str:
    """å®Ÿè¡Œç’°å¢ƒã«æœ€é©ãªãƒ‡ãƒã‚¤ã‚¹åã‚’æ–‡å­—åˆ—ã§è¿”ã™ (äº’æ›æ€§ç¶­æŒç”¨)ã€‚"""
    device = get_device()
    return device.type


def set_all_seeds(seed: int = 42, deterministic: bool = True) -> None:
    """
    å…¨ã¦ã®ä¹±æ•°ç”Ÿæˆå™¨ã®ã‚·ãƒ¼ãƒ‰ã‚’å›ºå®šã—ã€å­¦ç¿’ã®å†ç¾æ€§ã‚’ç¢ºä¿ã™ã‚‹ã€‚
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)

    os.environ['PYTHONHASHSEED'] = str(seed)

    if deterministic and torch.cuda.is_available():
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
        print(f"ğŸ”’ All seeds set to {seed} (Deterministic mode: ON)")
    else:
        print(f"ğŸ”’ All seeds set to {seed} (Deterministic mode: OFF/CPU/MPS)")


# set_seed ã‚‚ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã¨ã—ã¦ç”¨æ„
set_seed = set_all_seeds

# --- File I/O Utils (Added for Health Check) ---


def load_config(config_path: str) -> Dict[str, Any]:
    """YAMLè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚"""
    path = Path(config_path)
    if not path.exists():
        raise FileNotFoundError(f"Config file not found: {config_path}")

    with open(path, 'r', encoding='utf-8') as f:
        try:
            return yaml.safe_load(f)
        except yaml.YAMLError as e:
            raise ValueError(f"Error parsing YAML file {config_path}: {e}")


def save_json(data: Any, path: str) -> None:
    """ãƒ‡ãƒ¼ã‚¿ã‚’JSONã¨ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚"""
    target_path = Path(path)
    target_path.parent.mkdir(parents=True, exist_ok=True)
    with open(target_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=4, ensure_ascii=False)


def setup_logging(log_dir: str = "workspace/logs", log_name: str = "app.log", level: int = logging.INFO) -> logging.Logger:
    """ãƒ­ã‚®ãƒ³ã‚°ã‚’è¨­å®šã—ã¾ã™ã€‚"""
    Path(log_dir).mkdir(parents=True, exist_ok=True)
    log_path = Path(log_dir) / log_name

    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_path, encoding='utf-8'),
            logging.StreamHandler()
        ],
        force=True  # æ—¢å­˜ã®è¨­å®šã‚’ä¸Šæ›¸ã
    )

    # ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ãƒã‚¤ã‚ºæŠ‘åˆ¶
    logging.getLogger('matplotlib').setLevel(logging.WARNING)
    logging.getLogger('PIL').setLevel(logging.WARNING)

    return logging.getLogger("SNN_Project")

# --- Gradio & Data Processing (Restored from Original) ---


def get_avatar_svgs():
    """
    Gradioãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆç”¨ã®ã‚¢ãƒã‚¿ãƒ¼SVGã‚¢ã‚¤ã‚³ãƒ³ã®ã‚¿ãƒ—ãƒ«ã‚’è¿”ã™ã€‚
    """
    user_avatar_svg = r"""
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-user"><path d="M20 21v-2a4 4 0 0 0-4-4H8a4 4 0 0 0-4 4v2"></path><circle cx="12" cy="7" r="4"></circle></svg>
    """
    assistant_avatar_svg = r"""
    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-zap"><polygon points="13 2 3 14 12 14 11 22 21 10 12 10 13 2"></polygon></svg>
    """
    return user_avatar_svg, assistant_avatar_svg


def collate_fn(tokenizer: PreTrainedTokenizerBase, is_distillation: bool) -> Callable[[List[Any]], Any]:
    """
    ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ç”¨ã® Collate é–¢æ•°ã€‚
    ãƒ†ã‚­ã‚¹ãƒˆ(input_ids) ã¨ ç”»åƒ(pixel_values) ã®ä¸¡æ–¹ã‚’ãƒãƒƒãƒåŒ–ã™ã‚‹ã€‚
    """
    def collate(batch: List[Any]) -> Any:
        padding_val = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0

        inputs: List[torch.Tensor] = []
        targets: List[torch.Tensor] = []
        images: List[torch.Tensor] = []
        logits: List[torch.Tensor] = []

        for item in batch:
            if isinstance(item, dict):
                inp = item.get('input_ids')
                tgt = item.get('labels')
                img = item.get('pixel_values')

                if inp is not None:
                    inputs.append(torch.tensor(inp) if not isinstance(
                        inp, torch.Tensor) else inp)
                if tgt is not None:
                    targets.append(torch.tensor(tgt) if not isinstance(
                        tgt, torch.Tensor) else tgt)
                if img is not None:
                    images.append(torch.tensor(img) if not isinstance(
                        img, torch.Tensor) else img)

                if is_distillation:
                    lg = item.get('teacher_logits')
                    if lg is not None:
                        logits.append(torch.tensor(lg) if not isinstance(
                            lg, torch.Tensor) else lg)
                    else:
                        logits.append(torch.empty(0))

            elif isinstance(item, tuple) and len(item) >= 2:
                inp = item[0]
                tgt = item[1]
                inputs.append(torch.tensor(inp) if not isinstance(
                    inp, torch.Tensor) else inp)
                targets.append(torch.tensor(tgt) if not isinstance(
                    tgt, torch.Tensor) else tgt)
                if is_distillation:
                    if len(item) >= 3:
                        lg = item[2]
                        if lg is not None:
                            logits.append(torch.tensor(lg) if not isinstance(
                                lg, torch.Tensor) else lg)
                        else:
                            logits.append(torch.empty(0))
                    else:
                        logits.append(torch.empty(0))
            else:
                print(
                    f"Warning: Skipping unsupported batch item type: {type(item)}")
                continue

        if not inputs:
            return {}

        batch_data: Dict[str, Any] = {}
        padded_inputs = torch.nn.utils.rnn.pad_sequence(
            inputs, batch_first=True, padding_value=padding_val)

        if targets and targets[0].ndim == 0:
            padded_targets = torch.stack(targets, dim=0)
        else:
            padded_targets = torch.nn.utils.rnn.pad_sequence(
                targets, batch_first=True, padding_value=-100)

        attention_mask = torch.ones_like(padded_inputs)
        attention_mask[padded_inputs == padding_val] = 0

        batch_data["input_ids"] = padded_inputs
        batch_data["attention_mask"] = attention_mask
        batch_data["labels"] = padded_targets

        if images:
            batch_data["input_images"] = torch.stack(images, dim=0)

        if is_distillation:
            padded_logits = torch.nn.utils.rnn.pad_sequence(
                logits, batch_first=True, padding_value=0.0)
            if padded_targets.ndim > 1:
                seq_len = padded_inputs.shape[1]
                if padded_targets.shape[1] < seq_len:
                    pad = torch.full((padded_targets.shape[0], seq_len - padded_targets.shape[1]
                                      ), -100, dtype=padded_targets.dtype, device=padded_targets.device)
                    padded_targets = torch.cat([padded_targets, pad], dim=1)
                if padded_logits.shape[1] < seq_len:
                    pad = torch.full((padded_logits.shape[0], seq_len - padded_logits.shape[1],
                                     padded_logits.shape[2]), 0.0, dtype=padded_logits.dtype, device=padded_logits.device)
                    padded_logits = torch.cat([padded_logits, pad], dim=1)
            return padded_inputs, attention_mask, padded_targets, padded_logits

        return batch_data

    return collate


def build_gradio_ui(
    stream_fn: Callable[[str, List[List[Optional[str]]]], Iterator[Tuple[List[List[Optional[str]]], str]]],
    title: str,
    description: str,
    chatbot_label: str,
    theme: gr.themes.Base
) -> gr.Blocks:
    """å…±é€šã®Gradio Blocks UIã‚’æ§‹ç¯‰ã™ã‚‹"""
    with gr.Blocks(theme=theme) as demo:
        gr.Markdown(f"# {title}\n{description}")

        initial_stats_md = """
        **Inference Time:** `N/A`
        **Tokens/Second:** `N/A`
        ---
        **Total Spikes:** `N/A`
        **Spikes/Second:** `N/A`
        """

        with gr.Row():
            with gr.Column(scale=2):
                chatbot = gr.Chatbot(label=chatbot_label, height=500)
            with gr.Column(scale=1):
                stats_display = gr.Markdown(
                    value=initial_stats_md, label="ğŸ“Š Inference Stats")

        with gr.Row():
            msg_textbox = gr.Textbox(
                show_label=False,
                placeholder="ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›...",
                container=False,
                scale=6,
            )
            submit_btn = gr.Button("Send", variant="primary", scale=1)
            clear_btn = gr.Button("Clear", scale=1)

        gr.Markdown(
            "<footer><p>Â© 2025 SNN System Design Project. All rights reserved.</p></footer>")

        def clear_all():
            return [], "", initial_stats_md

        submit_event = msg_textbox.submit(
            fn=stream_fn,
            inputs=[msg_textbox, chatbot],
            outputs=[chatbot, stats_display],
            queue=False
        )
        submit_event.then(fn=lambda: "", inputs=None, outputs=msg_textbox)

        button_submit_event = submit_btn.click(
            fn=stream_fn,
            inputs=[msg_textbox, chatbot],
            outputs=[chatbot, stats_display],
            queue=False
        )
        button_submit_event.then(
            fn=lambda: "", inputs=None, outputs=msg_textbox)

        clear_btn.click(
            fn=clear_all,
            inputs=None,
            outputs=[chatbot, msg_textbox, stats_display],
            queue=False
        )

    return demo
