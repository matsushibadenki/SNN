# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: app/unified_perception_demo.py
# æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: Unified Perception Interactive Demo (Gradio) [Type Fixed]
# ç›®çš„ãƒ»å†…å®¹:
#   mypyã‚¨ãƒ©ãƒ¼ä¿®æ­£ç‰ˆã€‚
#   - gradioã‚¤ãƒ³ãƒãƒ¼ãƒˆæ™‚ã®å‹ãƒã‚§ãƒƒã‚¯ã‚’ç„¡è¦–è¨­å®šã«è¿½åŠ ã€‚
#   - trainãƒ¡ã‚½ãƒƒãƒ‰å†…ã®å¤‰æ•° log ã«å‹ãƒ’ãƒ³ãƒˆã‚’è¿½åŠ ã€‚

import gradio as gr  # type: ignore[import-untyped]
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt


from typing import Tuple, Dict  # Added: List

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã¸ã®ãƒ‘ã‚¹è¿½åŠ ã‚³ãƒ¼ãƒ‰ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚

from snn_research.core.networks.liquid_association_cortex import LiquidAssociationCortex
from snn_research.learning_rules.stdp import STDP
from snn_research.io.universal_encoder import UniversalSpikeEncoder


class BrainSystem:
    """ãƒ‡ãƒ¢ç”¨ã®è„³ã‚·ã‚¹ãƒ†ãƒ ãƒ©ãƒƒãƒ‘ãƒ¼"""

    def __init__(self):
        self.device = "cpu"
        self.time_steps = 50

        # Dimensions
        self.dim_visual = 100
        self.dim_audio = 50
        self.reservoir_size = 500

        # Hyperparameters (Optimized from Phase 9-10)
        self.input_scale = 20.0
        self.threshold = 0.5

        # Initialize Components
        self.stdp_rule = STDP(learning_rate=0.02, a_plus=0.1,
                              a_minus=0.05, tau_trace=20.0)

        self.lac = LiquidAssociationCortex(
            num_visual_inputs=self.dim_visual,
            num_audio_inputs=self.dim_audio,
            num_text_inputs=10,
            num_somato_inputs=10,
            reservoir_size=self.reservoir_size,
            sparsity=0.2,
            tau=5.0,
            threshold=self.threshold,
            input_scale=self.input_scale,
            learning_rule=self.stdp_rule
        ).to(self.device)

        self.encoder = UniversalSpikeEncoder(
            time_steps=self.time_steps, device=self.device)

        # Concept Prototypes (Fixed Patterns)
        torch.manual_seed(42)
        self.vis_A = (torch.rand(1, self.dim_visual) > 0.7).float()
        self.aud_A = (torch.rand(1, self.dim_audio) > 0.7).float()
        self.vis_B = (torch.rand(1, self.dim_visual) > 0.7).float()
        self.aud_B = (torch.rand(1, self.dim_audio) > 0.7).float()

        # Training State
        self.is_trained = False
        self.training_log = "Not trained yet."

        # Target Activities (for similarity check)
        self.target_A_activity = None
        self.target_B_activity = None

        # åˆæœŸçŠ¶æ…‹ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ´»å‹•ã‚’è¨˜éŒ²
        self._record_targets()

    def _record_targets(self):
        """æ¦‚å¿µA, Bã«å¯¾å¿œã™ã‚‹ç†æƒ³çš„ãªãƒªã‚¶ãƒ¼ãƒæ´»å‹•ã‚’è¨˜éŒ²"""
        self.lac.eval()

        # Record A (Vis A + Aud A)
        self.lac.reset_state()
        spikes_A = []
        vis_spikes = self.encoder.encode(self.vis_A, 'image', 'rate')
        aud_spikes = self.encoder.encode(self.aud_A, 'audio', 'rate')
        for t in range(self.time_steps):
            out = self.lac(
                visual_spikes=vis_spikes[:, t], audio_spikes=aud_spikes[:, t])
            spikes_A.append(out.detach())
        self.target_A_activity = torch.stack(
            spikes_A).squeeze(1).mean(dim=0)  # (Neurons,)

        # Record B (Vis B + Aud B)
        self.lac.reset_state()
        spikes_B = []
        vis_spikes = self.encoder.encode(self.vis_B, 'image', 'rate')
        aud_spikes = self.encoder.encode(self.aud_B, 'audio', 'rate')
        for t in range(self.time_steps):
            out = self.lac(
                visual_spikes=vis_spikes[:, t], audio_spikes=aud_spikes[:, t])
            spikes_B.append(out.detach())
        self.target_B_activity = torch.stack(spikes_B).squeeze(1).mean(dim=0)

    def train(self, epochs: int) -> str:
        """é€£åˆå­¦ç¿’ã‚’å®Ÿè¡Œ"""
        self.lac.train()
        # ä¿®æ­£: å‹ãƒ’ãƒ³ãƒˆã‚’è¿½åŠ 
        # log: List[str] = []

        for epoch in range(epochs):
            # Train Concept A
            self.lac.reset_state()
            vis_spikes = self.encoder.encode(self.vis_A, 'image', 'rate')
            aud_spikes = self.encoder.encode(self.aud_A, 'audio', 'rate')
            for t in range(self.time_steps):
                self.lac(
                    visual_spikes=vis_spikes[:, t], audio_spikes=aud_spikes[:, t])

            # Train Concept B (Optional: Multiple concepts)
            # æ··ä¹±ã‚’é¿ã‘ã‚‹ãŸã‚ã€ã¾ãšã¯Aã®ã¿ã‚’å¼·åŠ›ã«å­¦ç¿’ã•ã›ã‚‹ãƒ‡ãƒ¢ã«ã™ã‚‹ã‹ã€
            # ã‚ã‚‹ã„ã¯Aã¨Bã‚’äº¤äº’ã«å­¦ç¿’ã•ã›ã‚‹ã‹ã€‚
            # ã“ã“ã§ã¯ã€ŒAã®é€£åˆã€ã«é›†ä¸­ã™ã‚‹ãŸã‚Aã®ã¿å­¦ç¿’ã€‚

        self.is_trained = True
        self._record_targets()  # å­¦ç¿’å¾Œã®ãƒªã‚¶ãƒ¼ãƒçŠ¶æ…‹ã‚’æ–°ãŸãªã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨ã—ã¦æ›´æ–°
        return f"âœ… Training Completed ({epochs} Epochs). Association A-A formed."

    def infer(self, vis_amp_A: float, aud_amp_A: float, vis_amp_B: float, aud_amp_B: float) -> Tuple[plt.Figure, Dict[str, float]]:
        """
        æ¨è«–å®Ÿè¡Œã€‚å…¥åŠ›å¼·åº¦ã«å¿œã˜ã¦æ··åˆå…¥åŠ›ã‚’ç”Ÿæˆã—ã€ãƒªã‚¶ãƒ¼ãƒã®å¿œç­”ã‚’å¯è¦–åŒ–ã€‚
        """
        self.lac.eval()
        self.lac.reset_state()

        # Input Mixing
        # Input = amp * Pattern
        mixed_vis = (self.vis_A * vis_amp_A +
                     self.vis_B * vis_amp_B).clamp(0, 1)
        mixed_aud = (self.aud_A * aud_amp_A +
                     self.aud_B * aud_amp_B).clamp(0, 1)

        vis_spikes = self.encoder.encode(mixed_vis, 'image', 'rate')
        aud_spikes = self.encoder.encode(mixed_aud, 'audio', 'rate')

        history = []
        for t in range(self.time_steps):
            # å…¥åŠ›ãŒãªã„å ´åˆã¯Noneã‚’æ¸¡ã™ (Encoderã¯0ã‚¹ãƒ‘ã‚¤ã‚¯ã‚’ç”Ÿæˆã™ã‚‹ãŒã€LACå´ã§Noneãƒã‚§ãƒƒã‚¯ã—ã¦ã‚‚ã‚ˆã„)
            # ã“ã“ã§ã¯0ã‚¹ãƒ‘ã‚¤ã‚¯ã‚’æ¸¡ã™
            v_in = vis_spikes[:, t] if (
                vis_amp_A > 0 or vis_amp_B > 0) else None
            a_in = aud_spikes[:, t] if (
                aud_amp_A > 0 or aud_amp_B > 0) else None

            out = self.lac(visual_spikes=v_in, audio_spikes=a_in)
            history.append(out.detach())

        activity_tensor = torch.stack(history).squeeze(1)  # (Time, Neurons)
        mean_activity = activity_tensor.mean(dim=0)

        # Similarity Check
        sim_A = F.cosine_similarity(mean_activity.unsqueeze(
            0), self.target_A_activity.unsqueeze(0)).item()
        sim_B = F.cosine_similarity(mean_activity.unsqueeze(
            0), self.target_B_activity.unsqueeze(0)).item()

        # Visualization
        fig = plt.figure(figsize=(10, 6))

        # 1. Raster Plot
        ax1 = plt.subplot(2, 1, 1)
        heatmap = ax1.imshow(activity_tensor.T.numpy(
        ), aspect='auto', interpolation='nearest', cmap='inferno', vmin=0, vmax=1)
        ax1.set_title("Liquid Association Cortex Activity (Spike Raster)")
        ax1.set_ylabel("Neurons")
        ax1.set_xlabel("Time Step")
        plt.colorbar(heatmap, ax=ax1)

        # 2. Concept Activation
        ax2 = plt.subplot(2, 1, 2)
        concepts = ['Concept A (Bell)', 'Concept B (Whistle)']
        scores = [sim_A, sim_B]
        colors = ['blue', 'green']

        bars = ax2.bar(concepts, scores, color=colors)
        ax2.set_ylim(0, 1.0)
        ax2.set_title("Concept Recall Strength (Cosine Similarity)")
        ax2.axhline(y=0.0, color='k', linestyle='-')

        # å€¤ã‚’è¡¨ç¤º
        for bar in bars:
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height,
                     f'{height:.3f}', ha='center', va='bottom')

        plt.tight_layout()

        return fig, {"Concept A": sim_A, "Concept B": sim_B}

# --- Gradio UI Setup ---


brain = BrainSystem()


def train_brain(epochs):
    return brain.train(int(epochs))


def run_inference(vis_A, aud_A, vis_B, aud_B):
    return brain.infer(vis_A, aud_A, vis_B, aud_B)


with gr.Blocks(title="SNN Unified Perception Demo") as demo:
    gr.Markdown("# ğŸ§  SNN Unified Perception: 'Hearing Colors' Demo")
    gr.Markdown("""
    **Phase 9-6**: æ¶²çŠ¶é€£åˆé‡ (Liquid Association Cortex) ã«ã‚ˆã‚‹å…±æ„Ÿè¦šçš„æƒ³èµ·ã®ãƒ‡ãƒ¢ã€‚
    
    1. **Train**: [Train Association] ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ã€è„³ã«ã€Œè¦–è¦šAã¨éŸ³å£°Aã€ã®çµã³ã¤ãã‚’å­¦ç¿’ã•ã›ã¾ã™ã€‚
    2. **Test**: éŸ³å£°ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã ã‘ã‚’ä¸Šã’ã¦ã¿ã¦ãã ã•ã„ã€‚è¦–è¦šå…¥åŠ›ãŒã‚¼ãƒ­ã§ã‚‚ã€ã€ŒConcept Aã€ãŒæƒ³èµ·ã•ã‚Œã‚‹ï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒç™ºç«ã—é¡ä¼¼åº¦ãŒä¸ŠãŒã‚‹ï¼‰æ§˜å­ãŒè¦³å¯Ÿã§ãã¾ã™ã€‚
    """)

    with gr.Row():
        with gr.Column(scale=1):
            gr.Markdown("### 1. Training Phase")
            epochs_slider = gr.Slider(
                minimum=5, maximum=50, value=30, step=5, label="Training Epochs")
            train_btn = gr.Button(
                "ğŸ”— Train Association (Visual A + Audio A)", variant="primary")
            train_status = gr.Textbox(label="Status", value="Not trained.")

        with gr.Column(scale=1):
            gr.Markdown("### 2. Sensory Input Control")
            gr.Markdown("å…¥åŠ›å¼·åº¦ã‚’èª¿æ•´ã—ã¦è„³ã®åå¿œã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚")

            with gr.Row():
                vis_A_slider = gr.Slider(
                    0, 1, value=0, label="ğŸ‘ï¸ Visual A (Bell)")
                aud_A_slider = gr.Slider(
                    0, 1, value=0, label="ğŸ‘‚ Audio A (Bell Sound)")

            with gr.Row():
                vis_B_slider = gr.Slider(
                    0, 1, value=0, label="ğŸ‘ï¸ Visual B (Whistle)")
                aud_B_slider = gr.Slider(
                    0, 1, value=0, label="ğŸ‘‚ Audio B (Whistle Sound)")

            infer_btn = gr.Button("ğŸ§  Process Inputs", variant="secondary")

    with gr.Row():
        plot_output = gr.Plot(label="Brain Activity State")
        label_output = gr.Label(label="Detected Concept")

    # Event Handlers
    train_btn.click(train_brain, inputs=[
                    epochs_slider], outputs=[train_status])

    # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§ã‚’é«˜ã‚ã‚‹ãŸã‚ã€ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼å¤‰æ›´æ™‚ã«ã‚‚ç™ºç«ã•ã›ã‚‹è¨­å®š
    inputs = [vis_A_slider, aud_A_slider, vis_B_slider, aud_B_slider]
    outputs = [plot_output, label_output]

    infer_btn.click(run_inference, inputs=inputs, outputs=outputs)
    # ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã‚’å‹•ã‹ã—ã¦å³åº§ã«è¦‹ãŸã„å ´åˆã¯ä»¥ä¸‹ã‚’æœ‰åŠ¹åŒ– (é‡ã„å ´åˆã¯OFF)
    # for inp in inputs:
    #     inp.change(run_inference, inputs=inputs, outputs=outputs)

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7860, share=False)
