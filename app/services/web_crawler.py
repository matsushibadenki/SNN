# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: app/services/web_crawler.py
# Title: Web Crawler Service
# ä¿®æ­£: crawlãƒ¡ã‚½ãƒƒãƒ‰ã®è¿½åŠ 

from typing import List
import random

class WebCrawler:
    def __init__(self):
        # æ¨¡æ“¬ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆç©ºé–“
        self.knowledge_base = {
            "pattern": [
                "Patterns are regularities in the world.",
                "Fractals serve as efficient compression of visual data.",
                "Temporal patterns indicate causality."
            ],
            "snn": [
                "Spiking Neural Networks replicate brain dynamics.",
                "Energy efficiency comes from event-driven processing.",
                "STDP is a local learning rule."
            ],
            "ai": [
                "AI is transforming industries.",
                "Deep learning requires massive data.",
                "Neuromorphic computing is the next wave."
            ],
            "default": [
                "The world is full of unknown data.",
                "Learning is the process of reducing surprise.",
                "Curiosity drives exploration."
            ]
        }

    def search(self, query: str) -> List[str]:
        """ã‚¯ã‚¨ãƒªã«åŸºã¥ã„ã¦æƒ…å ±ã‚’æ¤œç´¢ã™ã‚‹ã€‚"""
        print(f"    ğŸŒ [Web] Searching for: '{query}'...")
        results = []
        query_lower = query.lower()
        
        hit = False
        for key, facts in self.knowledge_base.items():
            if key in query_lower:
                results.extend(facts)
                hit = True
        
        if not hit:
            results.extend(self.knowledge_base["default"])
            
        random.shuffle(results)
        return results[:2]

    def crawl(self, start_url: str, max_pages: int = 5) -> str:
        """
        æŒ‡å®šã•ã‚ŒãŸURLã‹ã‚‰ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã‚’è¡Œã„ã€çµæœã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹ï¼ˆãƒ¢ãƒƒã‚¯ï¼‰ã€‚
        RunWebLearningã‚¹ã‚¯ãƒªãƒ—ãƒˆç”¨ã€‚
        """
        print(f"    ğŸ•·ï¸ [Web] Crawling from: '{start_url}' (Max: {max_pages})")
        
        # ãƒ¢ãƒƒã‚¯ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ
        content = f"Source: {start_url}\n"
        content += "--- Extracted Content ---\n"
        
        # æ¤œç´¢æ©Ÿèƒ½ã‚’åˆ©ç”¨ã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆ
        topics = ["ai", "snn", "pattern"]
        for topic in topics:
            res = self.search(topic)
            for r in res:
                content += f"- {r}\n"
                
        output_path = "crawled_data_mock.txt"
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(content)
            
        print(f"    ğŸ“„ [Web] Saved crawled data to {output_path}")
        return output_path