# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: app/services/web_crawler.py
# ã‚¿ã‚¤ãƒˆãƒ«: å¼·åŒ–ç‰ˆ Web Crawler Service
# ç›®çš„: å®Ÿéš›ã®Webãƒšãƒ¼ã‚¸ã‹ã‚‰ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºã€ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã€ãŠã‚ˆã³æ¥ç¶šä¸å¯æ™‚ã®é«˜åº¦ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ã‚’æä¾›ã™ã‚‹ã€‚
# å†…å®¹:
#   - requestsã¨BeautifulSoupã‚’ç”¨ã„ãŸå®Ÿéš›ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æ©Ÿèƒ½
#   - HTMLã‚¿ã‚°é™¤å»ã€ãƒã‚¤ã‚ºé™¤å»ãªã©ã®ãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ–
#   - çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’ç”¨ã„ãŸæ¨¡æ“¬æ¤œç´¢æ©Ÿèƒ½ï¼ˆã‚ªãƒ•ãƒ©ã‚¤ãƒ³/ã‚¨ãƒ©ãƒ¼æ™‚ç”¨ï¼‰
#   - è¤‡æ•°ã®ãƒˆãƒ”ãƒƒã‚¯ã«é–¢é€£ã™ã‚‹æƒ…å ±ã‚’åé›†ãƒ»çµ±åˆã™ã‚‹æ©Ÿèƒ½

from typing import List, Optional, Set
import random
import re
import os
import time

# å¤–éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆç’°å¢ƒã«ãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰
try:
    import requests
    from bs4 import BeautifulSoup
    HAS_WEB_ACCESS = True
except ImportError:
    HAS_WEB_ACCESS = False
    print(
        "âš ï¸ [WebCrawler] 'requests' or 'bs4' not found. Running in Offline Mock Mode.")


class WebCrawler:
    def __init__(self, user_agent: str = "Mozilla/5.0 (compatible; SNN-Bot/1.0)"):
        self.user_agent = user_agent
        self.visited_urls: Set[str] = set()

        # æ¨¡æ“¬ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆç©ºé–“ï¼ˆã‚ªãƒ•ãƒ©ã‚¤ãƒ³/ãƒ†ã‚¹ãƒˆç”¨ï¼‰
        self.knowledge_base = {
            "pattern": [
                "Patterns are regularities in the world defined by mathematical structures.",
                "Fractals serve as efficient compression of visual data in nature.",
                "Temporal patterns indicate causality and sequence in neural processing.",
                "Symmetry breaking leads to diverse pattern formation in biology."
            ],
            "snn": [
                "Spiking Neural Networks replicate brain dynamics using discrete events.",
                "Energy efficiency comes from event-driven processing and sparse coding.",
                "STDP (Spike-Timing-Dependent Plasticity) is a local learning rule for synapses.",
                "Neuromorphic hardware minimizes the Von Neumann bottleneck."
            ],
            "ai": [
                "Artificial Intelligence is transforming industries through automation.",
                "Deep learning requires massive data and computational resources.",
                "Neuromorphic computing is the next wave of AI efficiency.",
                "Neuro-symbolic AI bridges the gap between logic and neural networks."
            ],
            "brain": [
                "The brain operates on approximately 20 watts of power.",
                "Synaptic plasticity is the biological basis of learning and memory.",
                "Sleep plays a crucial role in memory consolidation.",
                "Predictive coding suggests the brain constantly generates models of the world."
            ],
            "default": [
                "The world is full of unknown data waiting to be structured.",
                "Learning is the process of reducing surprise (free energy minimization).",
                "Curiosity drives exploration towards novel information.",
                "Feedback loops are essential for self-regulating systems."
            ]
        }

    def _clean_text(self, text: str) -> str:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ä¸è¦ãªç©ºç™½ã‚„ç‰¹æ®Šæ–‡å­—ã‚’é™¤å»ã™ã‚‹ã€‚"""
        # æ”¹è¡Œã‚’ã‚¹ãƒšãƒ¼ã‚¹ã«ç½®æ›
        text = text.replace("\n", " ")
        text = text.replace("\r", " ")
        # é€£ç¶šã™ã‚‹ã‚¹ãƒšãƒ¼ã‚¹ã‚’1ã¤ã«
        text = re.sub(r'\s+', ' ', text)
        # å‰å¾Œã®ç©ºç™½å‰Šé™¤
        return text.strip()

    def _fetch_page(self, url: str) -> Optional[str]:
        """æŒ‡å®šã•ã‚ŒãŸURLã®HTMLã‚’å–å¾—ã™ã‚‹ï¼ˆå®Ÿé€šä¿¡ï¼‰ã€‚"""
        if not HAS_WEB_ACCESS:
            return None

        try:
            # å®‰å…¨ã®ãŸã‚ã€ç‰¹å®šã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚„éåº¦ãªã‚¢ã‚¯ã‚»ã‚¹ã‚’åˆ¶é™ã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã“ã“ã«æŒŸã‚€ã®ãŒç†æƒ³
            headers = {'User-Agent': self.user_agent}
            response = requests.get(url, headers=headers, timeout=5)
            response.raise_for_status()
            return response.text
        except Exception as e:
            print(f"    âš ï¸ [Web] Connection failed for {url}: {e}")
            return None

    def _extract_content_from_html(self, html: str) -> List[str]:
        """HTMLã‹ã‚‰æœ‰ç›Šãªãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºã™ã‚‹ã€‚"""
        if not HAS_WEB_ACCESS or not html:
            return []

        soup = BeautifulSoup(html, 'html.parser')

        # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ã‚¹ã‚¿ã‚¤ãƒ«ã‚’é™¤å»
        for script in soup(["script", "style", "nav", "footer", "header", "aside"]):
            script.decompose()

        # æœ¬æ–‡ã‚‰ã—ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆpã‚¿ã‚°ã‚„liã‚¿ã‚°ã‚’ä¸­å¿ƒã«ï¼‰
        lines = []
        for tag in soup.find_all(['p', 'li', 'h1', 'h2', 'h3']):
            text = self._clean_text(tag.get_text())
            # çŸ­ã™ãã‚‹è¡Œã‚„æ„å‘³ã®ãªã„è¡Œã‚’é™¤å¤–
            if len(text) > 30:
                lines.append(text)

        return lines

    def _extract_links(self, html: str, base_url: str) -> List[str]:
        """HTMLã‹ã‚‰æ¬¡ã®æ¢ç´¢å€™è£œã¨ãªã‚‹ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºã™ã‚‹ã€‚"""
        if not HAS_WEB_ACCESS or not html:
            return []

        soup = BeautifulSoup(html, 'html.parser')
        links = []
        for a_tag in soup.find_all('a', href=True):
            href = a_tag['href']
            # ç°¡æ˜“çš„ãªURLæ­£è¦åŒ–ï¼ˆç›¸å¯¾ãƒ‘ã‚¹å¯¾å¿œãªã©ãŒå¿…è¦ã ãŒã“ã“ã§ã¯çœç•¥ï¼‰
            if href.startswith('http'):
                links.append(href)

        # ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ã¦è¿”ã™
        random.shuffle(links)
        return links

    def search(self, query: str) -> List[str]:
        """
        ã‚¯ã‚¨ãƒªã«åŸºã¥ã„ã¦æƒ…å ±ã‚’æ¤œç´¢ã™ã‚‹ã€‚
        ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãªã‚‰æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã®çµæœï¼ˆã‚’æ¨¡å€£ï¼‰ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãªã‚‰çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’ä½¿ç”¨ã€‚
        """
        print(f"    ğŸ” [Web] Searching knowledge for: '{query}'...")
        results = []
        query_lower = query.lower()

        # 1. çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã®æ¤œç´¢ï¼ˆé«˜é€Ÿãƒ»ç¢ºå®Ÿï¼‰
        hit = False
        for key, facts in self.knowledge_base.items():
            if key in query_lower:
                results.extend(facts)
                hit = True

        # 2. ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã§ãƒ’ãƒƒãƒˆã—ãªã‹ã£ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if not hit:
            # é–¢é€£ã—ãã†ãªã‚­ãƒ¼ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸ã¶ï¼ˆé€£æƒ³ï¼‰
            random_key = random.choice(list(self.knowledge_base.keys()))
            results.extend(self.knowledge_base[random_key])
            print(
                f"    ğŸ’¡ [Web] No direct hit. Associating with '{random_key}'...")

        random.shuffle(results)
        return results[:5]

    def crawl(self, start_url: str, max_pages: int = 5, topic_filter: Optional[str] = None) -> str:
        """
        æŒ‡å®šã•ã‚ŒãŸURLã‹ã‚‰ã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã‚’è¡Œã„ã€çµæœã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹ã€‚
        ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ç’°å¢ƒã§ã‚ã‚Œã°å®Ÿéš›ã«Webã‚¢ã‚¯ã‚»ã‚¹ã‚’è©¦ã¿ã€å¤±æ•—ã™ã‚Œã°ãƒ¢ãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã™ã‚‹ã€‚

        Args:
            start_url: é–‹å§‹URL
            max_pages: ã‚¯ãƒ­ãƒ¼ãƒ«ã™ã‚‹æœ€å¤§ãƒšãƒ¼ã‚¸æ•°
            topic_filter: ç‰¹å®šã®ãƒˆãƒ”ãƒƒã‚¯ï¼ˆæ–‡å­—åˆ—ï¼‰ã«é–¢é€£ã™ã‚‹æƒ…å ±ã®ã¿ã‚’å„ªå…ˆã™ã‚‹ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

        Returns:
            ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        print(
            f"    ğŸ•·ï¸ [Web] Crawling started. Root: '{start_url}' (Max: {max_pages})")

        collected_data = []
        queue = [start_url]
        pages_crawled = 0

        # ãƒ¢ãƒƒã‚¯åˆ¤å®š: URLãŒãƒ€ãƒŸãƒ¼ã£ã½ã„ã€ã¾ãŸã¯ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒãªã„å ´åˆ
        is_mock_url = "http" not in start_url or "example.com" in start_url
        use_mock_mode = not HAS_WEB_ACCESS or is_mock_url

        if use_mock_mode:
            print("    ğŸ¤– [Web] Running in SIMULATION mode.")
            # ãƒ¢ãƒƒã‚¯: ãƒˆãƒ”ãƒƒã‚¯ã«é–¢é€£ã™ã‚‹æƒ…å ±ã‚’ç”Ÿæˆ
            base_topics = ["ai", "snn", "brain", "pattern"]
            if topic_filter:
                base_topics.insert(0, topic_filter)

            for _ in range(max_pages):
                topic = random.choice(base_topics)
                facts = self.search(topic)
                collected_data.append(
                    f"\n--- Simulated Page about {topic.upper()} ---\n")
                collected_data.extend([f"- {fact}" for fact in facts])
                pages_crawled += 1

        else:
            # å®Ÿã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ãƒ«ãƒ¼ãƒ—
            while queue and pages_crawled < max_pages:
                url = queue.pop(0)
                if url in self.visited_urls:
                    continue

                print(f"      Reading: {url} ...")
                html = self._fetch_page(url)
                self.visited_urls.add(url)

                if html:
                    content_lines = self._extract_content_from_html(html)
                    if content_lines:
                        # ãƒˆãƒ”ãƒƒã‚¯ãƒ•ã‚£ãƒ«ã‚¿ãŒã‚ã‚‹å ´åˆã€é–¢é€£ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹ã‹ç°¡æ˜“ãƒã‚§ãƒƒã‚¯
                        if topic_filter and topic_filter.lower() not in html.lower():
                            pass  # é–¢é€£æ€§ãŒä½ãã†ãªã‚‰ã‚¹ã‚­ãƒƒãƒ—ã€ã¾ãŸã¯å„ªå…ˆåº¦ã‚’ä¸‹ã’ã‚‹
                        else:
                            collected_data.append(f"\n--- Source: {url} ---\n")
                            collected_data.extend(content_lines)
                            pages_crawled += 1

                    # æ¬¡ã®ãƒªãƒ³ã‚¯ã‚’å–å¾—ã—ã¦ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
                    new_links = self._extract_links(html, url)
                    queue.extend(new_links[:3])  # 1ãƒšãƒ¼ã‚¸ã‚ãŸã‚Šæœ€å¤§3ãƒªãƒ³ã‚¯ã‚’è¿½åŠ 

                time.sleep(1)  # ãƒãƒŠãƒ¼ã®ãŸã‚ã®å¾…æ©Ÿ

        # çµæœã®ä¿å­˜
        if not collected_data:
            print(
                "    âš ï¸ [Web] No data collected. Generating default knowledge.")
            collected_data = self.search("default")

        output_dir = "data/crawled"
        os.makedirs(output_dir, exist_ok=True)
        timestamp = int(time.time())
        filename = f"web_knowledge_{timestamp}.txt"
        output_path = os.path.join(output_dir, filename)

        with open(output_path, "w", encoding="utf-8") as f:
            f.write(f"Crawl Root: {start_url}\n")
            f.write(f"Topic Filter: {topic_filter}\n")
            f.write(f"Date: {time.ctime()}\n")
            f.write("========================================\n\n")
            f.write("\n".join(collected_data))

        print(
            f"    ğŸ“„ [Web] Saved {len(collected_data)} lines to {output_path}")
        return output_path
