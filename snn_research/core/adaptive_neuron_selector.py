# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/core/adaptive_neuron_selector.py
# Title: é©å¿œçš„ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚»ãƒ¬ã‚¯ã‚¿ (Adaptive Neuron Selector) - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¼•ç¶™ãå¼·åŒ–ç‰ˆ
# Description:
#   SNNã®å­¦ç¿’ä¸­ã®æŒ¯ã‚‹èˆã„ã‚’ç›£è¦–ã—ã€ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®ã‚¿ã‚¤ãƒ—ã‚’å‹•çš„ã«åˆ‡ã‚Šæ›¿ãˆã‚‹ãƒ¡ã‚¿ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ã€‚
#   ä¿®æ­£ç‚¹:
#   - ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ç½®æ›æ™‚ã«ã€å¤ã„ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‹ã‚‰æ–°ã—ã„ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¸é‡ã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆweight, biasç­‰ï¼‰ã‚’
#     å¯èƒ½ãªé™ã‚Šã‚³ãƒ”ãƒ¼ã™ã‚‹ã‚ˆã†ã«ä¿®æ­£ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€åˆ‡ã‚Šæ›¿ãˆã«ã‚ˆã‚‹æ€¥æ¿€ãªç²¾åº¦ä½ä¸‹ï¼ˆå¿˜å´ï¼‰ã‚’é˜²ãã€‚
#   - å½¢çŠ¶ãŒä¸€è‡´ã—ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ç„¡è¦–ã—ã€è­¦å‘Šã‚’ãƒ­ã‚°å‡ºåŠ›ã™ã‚‹ã€‚

import torch
import torch.nn as nn
from typing import List, Deque, Dict, Tuple, Type, cast, Any, Optional
from collections import deque
import logging

# å¿…è¦ãªãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from .neurons import AdaptiveLIFNeuron, BistableIFNeuron

# ãƒ­ã‚¬ãƒ¼è¨­å®š
logger = logging.getLogger(__name__)

class AdaptiveNeuronSelector(nn.Module):
    """
    å­¦ç¿’ä¸­ã®æŒ¯ã‚‹èˆã„ï¼ˆæå¤±ã€ã‚¹ãƒ‘ã‚¤ã‚¯ç‡ï¼‰ã‚’ç›£è¦–ã—ã€
    LIFã¨BIFãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãªã©ã‚’å‹•çš„ã«åˆ‡ã‚Šæ›¿ãˆã‚‹ãƒ¡ã‚¿ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ã€‚
    """

    def __init__(
        self,
        module_to_wrap: nn.Module,
        layer_name_to_monitor: str,
        lif_params: Dict[str, Any],
        bif_params: Dict[str, Any],
        monitor_window: int = 20,
        loss_plateau_threshold: float = 0.001,
        low_spike_rate_threshold: float = 0.05,
        high_spike_rate_threshold: float = 0.95
    ) -> None:
        """
        Args:
            module_to_wrap (nn.Module): å†…éƒ¨ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹å¯¾è±¡ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚
            layer_name_to_monitor (str): module_to_wrapå†…ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³å±¤ã®åå‰ã€‚
            lif_params (Dict[str, Any]): LIFãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®åˆæœŸåŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚
            bif_params (Dict[str, Any]): BIFãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®åˆæœŸåŒ–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚
            monitor_window (int): ç›£è¦–ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã€‚
            loss_plateau_threshold (float): æå¤±åœæ»ã®é–¾å€¤ã€‚
            low_spike_rate_threshold (float): ä½ã‚¹ãƒ‘ã‚¤ã‚¯ç‡ã®é–¾å€¤ã€‚
            high_spike_rate_threshold (float): é«˜ã‚¹ãƒ‘ã‚¤ã‚¯ç‡ã®é–¾å€¤ã€‚
        """
        super().__init__()
        self.module_to_wrap: nn.Module = module_to_wrap
        self.layer_name_to_monitor: str = layer_name_to_monitor
        self.lif_params: Dict[str, Any] = lif_params
        self.bif_params: Dict[str, Any] = bif_params
        self.monitor_window: int = monitor_window
        self.loss_plateau_threshold: float = loss_plateau_threshold
        self.low_spike_rate_threshold: float = low_spike_rate_threshold
        self.high_spike_rate_threshold: float = high_spike_rate_threshold

        # ç›£è¦–ç”¨ã®å±¥æ­´ãƒãƒƒãƒ•ã‚¡
        self.loss_history: Deque[float] = deque(maxlen=monitor_window)
        self.spike_rate_history: Deque[float] = deque(maxlen=monitor_window)
        
        # åˆæœŸçŠ¶æ…‹ã®ç¢ºèª
        try:
            self.monitored_neuron: nn.Module = self._find_layer(layer_name_to_monitor)
            self.current_neuron_type: Type[nn.Module] = type(self.monitored_neuron)
            logger.info(f"âœ… AdaptiveNeuronSelectorãŒå±¤ '{layer_name_to_monitor}' ({self.current_neuron_type.__name__}) ã®ç›£è¦–ã‚’é–‹å§‹ã—ã¾ã—ãŸã€‚")
        except AttributeError:
            logger.error(f"âŒ '{layer_name_to_monitor}' ãŒ 'module_to_wrap' ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            self.monitored_neuron = nn.Identity() 
            self.current_neuron_type = nn.Identity

    def _find_layer(self, layer_name: str) -> nn.Module:
        """æŒ‡å®šã•ã‚ŒãŸåå‰ã®ã‚µãƒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’è¦‹ã¤ã‘ã‚‹"""
        current_module: nn.Module = self.module_to_wrap
        for name in layer_name.split('.'):
            if not hasattr(current_module, name):
                raise AttributeError(f"ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« '{type(current_module).__name__}' ã«å±æ€§ '{name}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            current_module = getattr(current_module, name)
        return current_module

    def _transfer_weights(self, old_module: nn.Module, new_module: nn.Module) -> None:
        """
        å¤ã„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰æ–°ã—ã„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¸ã€å½¢çŠ¶ãŒä¸€è‡´ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼ã™ã‚‹ã€‚
        ã“ã‚Œã«ã‚ˆã‚Šã€å­¦ç¿’æ¸ˆã¿ã®ç‰¹å¾´æŠ½å‡ºèƒ½åŠ›ã‚’ç¶­æŒã™ã‚‹ã€‚
        """
        old_state = old_module.state_dict()
        new_state = new_module.state_dict()
        
        transferred_keys = []
        
        for key, old_param in old_state.items():
            if key in new_state:
                new_param = new_state[key]
                if old_param.shape == new_param.shape:
                    with torch.no_grad():
                        new_param.copy_(old_param)
                    transferred_keys.append(key)
                else:
                    logger.debug(f"Skipping parameter '{key}': Shape mismatch ({old_param.shape} vs {new_param.shape})")
            else:
                logger.debug(f"Skipping parameter '{key}': Not in new module")
        
        if transferred_keys:
            logger.info(f"ğŸ”„ Transferred parameters: {transferred_keys}")
        else:
            logger.warning("âš ï¸ No parameters were transferred during neuron replacement. Weights are re-initialized.")

    def _replace_neuron_layer(self, target_class: Type[nn.Module], params: Dict[str, Any]) -> None:
        """ç›£è¦–å¯¾è±¡ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³å±¤ã‚’æ–°ã—ã„ã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«ç½®ãæ›ãˆã‚‹"""
        if self.current_neuron_type == target_class:
            return

        try:
            # å…ƒã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®ç‰¹å¾´é‡æ•°ã‚’å–å¾—
            original_features: int = 0
            if hasattr(self.monitored_neuron, 'features'):
                original_features = cast(int, getattr(self.monitored_neuron, 'features'))
            elif hasattr(self.monitored_neuron, 'n_neurons'): 
                original_features = cast(int, getattr(self.monitored_neuron, 'n_neurons'))
            
            if original_features == 0:
                logger.warning(f"å±¤ '{self.layer_name_to_monitor}' ã®ç‰¹å¾´é‡æ•°ãŒå–å¾—ã§ãã¾ã›ã‚“ã€‚ç½®ãæ›ãˆã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
                return

            # æ–°ã—ã„ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
            params_no_features: Dict[str, Any] = params.copy()
            params_no_features.pop('features', None)
            
            new_neuron: nn.Module = target_class(features=original_features, **params_no_features)
            
            # ãƒ‡ãƒã‚¤ã‚¹ã‚’åˆã‚ã›ã‚‹
            device_param = next(self.monitored_neuron.parameters(), None)
            if device_param is not None:
                original_device = device_param.device
                new_neuron.to(original_device)
            else:
                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒãªã„å ´åˆï¼ˆç¨€ã ãŒï¼‰ã¯CPUãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¾ãŸã¯å‰ã®è¦ªã®ãƒ‡ãƒã‚¤ã‚¹
                pass

            # --- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¼•ãç¶™ã ---
            self._transfer_weights(self.monitored_neuron, new_neuron)

            # è¦ªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å–å¾—ã—ã¦å±æ€§ã‚’ç½®ãæ›ãˆ
            parent_module: nn.Module = self.module_to_wrap
            layer_name_parts: List[str] = self.layer_name_to_monitor.split('.')
            if len(layer_name_parts) > 1:
                for name in layer_name_parts[:-1]:
                    parent_module = getattr(parent_module, name)
            
            final_layer_name: str = layer_name_parts[-1]
            setattr(parent_module, final_layer_name, new_neuron)
            
            # å‚ç…§ã¨ã‚¿ã‚¤ãƒ—ã‚’æ›´æ–°
            self.monitored_neuron = new_neuron
            self.current_neuron_type = target_class
            logger.info(f"ğŸ§¬ ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³é€²åŒ–: å±¤ '{self.layer_name_to_monitor}' ãŒ '{target_class.__name__}' ã«åˆ‡ã‚Šæ›¿ã‚ã‚Šã¾ã—ãŸã€‚")

        except Exception as e:
            logger.error(f"ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³å±¤ã®ç½®ãæ›ãˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}", exc_info=True)


    def step(self, current_loss: float) -> Tuple[bool, str]:
        """
        å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«å‘¼ã³å‡ºã•ã‚Œã€çµ±è¨ˆã‚’æ›´æ–°ã—ã€åˆ‡ã‚Šæ›¿ãˆã‚’åˆ¤æ–­ã™ã‚‹ã€‚

        Args:
            current_loss (float): ç¾åœ¨ã®ãƒãƒƒãƒã®æå¤±ã€‚

        Returns:
            Tuple[bool, str]: (åˆ‡ã‚Šæ›¿ãˆãŒç™ºç”Ÿã—ãŸã‹, ç†ç”±)
        """
        # 1. çµ±è¨ˆã®åé›†
        self.loss_history.append(current_loss)
        
        spike_rate: float = 0.0
        if hasattr(self.monitored_neuron, 'spikes'):
            spikes_tensor: Optional[torch.Tensor] = getattr(self.monitored_neuron, 'spikes')
            if spikes_tensor is not None and spikes_tensor.numel() > 0:
                spike_rate = spikes_tensor.mean().item()
        self.spike_rate_history.append(spike_rate)

        # å±¥æ­´ãŒæºœã¾ã‚‹ã¾ã§å¾…æ©Ÿ
        if len(self.loss_history) < self.monitor_window:
            return False, "Initializing history"

        # 2. çŠ¶æ…‹ã®è¨ºæ–­
        avg_spike_rate: float = float(torch.tensor(list(self.spike_rate_history)).mean().item())
        loss_std_dev: float = float(torch.tensor(list(self.loss_history)).std().item())
        
        # æœ€è¿‘ã®å¹³å‡æå¤±
        recent_loss_mean = float(torch.tensor(list(self.loss_history)[-5:]).mean().item())

        # åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯
        if avg_spike_rate < self.low_spike_rate_threshold:
            # ç—‡çŠ¶: Dead Neuron (ä½ç™ºç«)
            # å¯¾ç­–: BIFã®åŒå®‰å®šæ€§ã¾ãŸã¯ãƒã‚¤ã‚ºè€æ€§ã§æ´»æ€§åŒ–ã‚’è©¦ã¿ã‚‹
            if self.current_neuron_type != BistableIFNeuron:
                self._replace_neuron_layer(BistableIFNeuron, self.bif_params)
                return True, f"Low spike rate ({avg_spike_rate:.3f} < {self.low_spike_rate_threshold}): Switched to BIF"
            
        elif avg_spike_rate > self.high_spike_rate_threshold:
            # ç—‡çŠ¶: Over-excitation (éå‰°ç™ºç«)
            # å¯¾ç­–: å®‰å®šã—ãŸLIFã«æˆ»ã™ï¼ˆé–¾å€¤é©å¿œãŒåƒãã‚„ã™ã„ï¼‰
            if self.current_neuron_type != AdaptiveLIFNeuron:
                self._replace_neuron_layer(AdaptiveLIFNeuron, self.lif_params)
                return True, f"High spike rate ({avg_spike_rate:.3f} > {self.high_spike_rate_threshold}): Switched to LIF"

        elif loss_std_dev > self.loss_plateau_threshold * 5.0:
            # ç—‡çŠ¶: å­¦ç¿’ãŒä¸å®‰å®šãƒ»ç™ºæ•£å‚¾å‘
            if self.current_neuron_type != AdaptiveLIFNeuron:
                self._replace_neuron_layer(AdaptiveLIFNeuron, self.lif_params)
                return True, f"Loss unstable (std={loss_std_dev:.4f}): Switched to LIF for stability"

        elif loss_std_dev < self.loss_plateau_threshold and recent_loss_mean > 0.5:
            # ç—‡çŠ¶: æå¤±ãŒé«˜ã„ã¾ã¾åœæ» (Local Minima)
            # å¯¾ç­–: BIFã§è¡¨ç¾åŠ›ã‚’é«˜ã‚ã€ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã‚’å¤‰ãˆã‚‹
            if self.current_neuron_type != BistableIFNeuron:
                self._replace_neuron_layer(BistableIFNeuron, self.bif_params)
                return True, f"Loss plateau (std={loss_std_dev:.4f}): Switched to BIF for exploration"
        
        return False, "Stable"

    def forward(self, *args: Any, **kwargs: Any) -> Any:
        return self.module_to_wrap(*args, **kwargs)