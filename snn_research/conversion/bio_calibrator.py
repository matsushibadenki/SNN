# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/conversion/bio_calibrator.py
# Title: Deep Bio-Calibrator (HSEO-based SNN Fine-Tuning) - ãƒ­ã‚¸ãƒƒã‚¯ä¿®æ­£ç‰ˆ
# Description:
#   Roadmap v14.0 "Deep Bio-Calibration" ã®å®Ÿè£…ã€‚
#   HSEO (Hybrid Swarm Evolution Optimization) ã‚’ç”¨ã„ã¦ã€
#   å¤‰æ›å¾Œã®SNNãƒ¢ãƒ‡ãƒ«ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆä¸»ã«é–¾å€¤ï¼‰ã‚’
#   ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦è‡ªå‹•æœ€é©åŒ–ã™ã‚‹ã€‚
#   ä¿®æ­£: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®One-Hotåˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯ã‚’ä¿®æ­£ã—ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿(LongTensor)ãŒ
#   èª¤ã£ã¦argmaxã§æ¬¡å…ƒå‰Šæ¸›ã•ã‚Œã‚‹ãƒã‚°ã‚’è§£æ¶ˆã€‚

import torch
import torch.nn as nn
import logging
import numpy as np
from typing import Dict, Any, List, cast

from snn_research.core.neurons import (
    AdaptiveLIFNeuron, DualThresholdNeuron, ScaleAndFireNeuron, IzhikevichNeuron
)
from snn_research.optimization.hseo import optimize_with_hseo

logger = logging.getLogger(__name__)


class DeepBioCalibrator:
    """
    SNNãƒ¢ãƒ‡ãƒ«ã®ç”Ÿç‰©å­¦çš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆé–¾å€¤ãªã©ï¼‰ã‚’HSEOã‚’ç”¨ã„ã¦è‡ªå‹•æ ¡æ­£ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚
    """

    def __init__(
        self,
        model: nn.Module,
        calibration_loader: Any,
        device: str = "cpu",
        hseo_particles: int = 10,
        hseo_iterations: int = 15,
        metric_weight_accuracy: float = 1.0,
        metric_weight_sparsity: float = 0.1
    ):
        self.model = model
        self.calibration_loader = calibration_loader
        self.device = device
        self.hseo_particles = hseo_particles
        self.hseo_iterations = hseo_iterations
        self.metric_weight_accuracy = metric_weight_accuracy
        self.metric_weight_sparsity = metric_weight_sparsity

        # æœ€é©åŒ–å¯¾è±¡ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³å±¤ã‚’ç‰¹å®š
        self.target_layers: List[nn.Module] = []
        self.layer_names: List[str] = []
        self._identify_target_layers()

        logger.info(
            f"ğŸ§¬ DeepBioCalibrator initialized. Targets: {len(self.target_layers)} layers.")

    def _identify_target_layers(self) -> None:
        """æœ€é©åŒ–å¯èƒ½ãªãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³å±¤ã‚’åé›†ã™ã‚‹"""
        for name, module in self.model.named_modules():
            if isinstance(module, (AdaptiveLIFNeuron, DualThresholdNeuron, ScaleAndFireNeuron, IzhikevichNeuron)):
                self.target_layers.append(module)
                self.layer_names.append(name)

    @torch.no_grad()
    def _evaluate_params(self, scaling_factors: np.ndarray) -> float:
        """
        ç›®çš„é–¢æ•°: æŒ‡å®šã•ã‚ŒãŸã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°ã‚’é–¾å€¤ã«é©ç”¨ã—ã€ã‚¹ã‚³ã‚¢ï¼ˆæå¤±ï¼‰ã‚’è¨ˆç®—ã™ã‚‹ã€‚
        HSEOã¯æœ€å°åŒ–ã‚’è¡Œã†ãŸã‚ã€(1 - Accuracy) + SparsityPenalty ã‚’è¿”ã™ã€‚
        """
        # 1. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¸€æ™‚çš„ãªé©ç”¨
        original_thresholds = []

        try:
            for i, layer in enumerate(self.target_layers):
                factor = float(scaling_factors[i])

                if isinstance(layer, AdaptiveLIFNeuron):
                    orig = layer.base_threshold.data.clone()
                    original_thresholds.append((layer.base_threshold, orig))
                    layer.base_threshold.data.mul_(factor)

                elif isinstance(layer, DualThresholdNeuron):
                    orig_h = layer.threshold_high.data.clone()
                    orig_l = layer.threshold_low.data.clone()
                    original_thresholds.append((layer.threshold_high, orig_h))
                    original_thresholds.append((layer.threshold_low, orig_l))
                    layer.threshold_high.data.mul_(factor)
                    layer.threshold_low.data.mul_(factor)

                elif isinstance(layer, ScaleAndFireNeuron):
                    orig = layer.thresholds.data.clone()
                    original_thresholds.append((layer.thresholds, orig))
                    layer.thresholds.data.mul_(factor)

            # 2. æ¨è«–ã¨è©•ä¾¡
            self.model.eval()
            total_correct = 0
            total_samples = 0
            total_spikes = 0.0

            # é«˜é€ŸåŒ–ã®ãŸã‚å°‘æ•°ã®ãƒãƒƒãƒã®ã¿ä½¿ç”¨
            max_batches = 5
            for i, batch in enumerate(self.calibration_loader):
                if i >= max_batches:
                    break

                # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‰
                inputs: torch.Tensor
                targets: torch.Tensor

                if isinstance(batch, (list, tuple)):
                    inputs, targets = batch[0].to(
                        self.device), batch[1].to(self.device)
                elif isinstance(batch, dict):
                    key_in = 'input_ids' if 'input_ids' in batch else 'input_images'
                    inputs_raw = batch.get(key_in)
                    targets_raw = batch.get('labels')

                    if inputs_raw is None or targets_raw is None:
                        continue

                    inputs = cast(torch.Tensor, inputs_raw).to(self.device)
                    targets = cast(torch.Tensor, targets_raw).to(self.device)
                else:
                    continue

                # ãƒªã‚»ãƒƒãƒˆ
                if hasattr(self.model, 'reset_spike_stats'):
                    cast(Any, self.model).reset_spike_stats()

                # é †ä¼æ’­
                outputs = self.model(inputs)
                if isinstance(outputs, tuple):
                    logits = outputs[0]
                else:
                    logits = outputs

                # ç²¾åº¦è¨ˆç®—
                # logits: (Batch, SeqLen, VocabSize) or (Batch, NumClasses)
                preds = logits.argmax(dim=-1)

                # --- ä¿®æ­£: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®å½¢çŠ¶èª¿æ•´ãƒ­ã‚¸ãƒƒã‚¯ ---
                # One-hotã‹ã©ã†ã‹ã¯å‹(float)ã§åˆ¤æ–­ã™ã‚‹ã®ãŒå®‰å…¨
                if targets.ndim > 1 and targets.shape[-1] > 1 and targets.is_floating_point():
                    targets = targets.argmax(dim=-1)
                # ------------------------------------

                # ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‹ã‚¹ã‚«ãƒ©ãƒ¼ã‹ã«é–¢ã‚ã‚‰ãšãƒ•ãƒ©ãƒƒãƒˆåŒ–ã—ã¦æ¯”è¼ƒ
                if preds.shape != targets.shape:
                    preds = preds.view(-1)
                    targets = targets.view(-1)

                total_correct += (preds == targets).sum().item()
                total_samples += targets.numel()

                # ã‚¹ãƒ‘ã‚¤ã‚¯æ•°å–å¾—
                if hasattr(self.model, 'get_total_spikes'):
                    total_spikes += cast(Any, self.model).get_total_spikes()

            accuracy = total_correct / total_samples if total_samples > 0 else 0.0
            avg_spikes = total_spikes / total_samples if total_samples > 0 else 0.0

            # 3. ã‚¹ã‚³ã‚¢è¨ˆç®— (æœ€å°åŒ–å¯¾è±¡)
            score = (1.0 - accuracy) * self.metric_weight_accuracy + \
                    (avg_spikes * 0.0001) * self.metric_weight_sparsity

            return score

        finally:
            # 4. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¾©å…ƒ
            for param_tensor, orig_data in original_thresholds:
                param_tensor.data.copy_(orig_data)

    def calibrate(self) -> Dict[str, Any]:
        """
        ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’æœ€é©åŒ–ã™ã‚‹ã€‚
        """
        logger.info("ğŸš€ Starting Deep Bio-Calibration with HSEO...")

        num_params = len(self.target_layers)
        if num_params == 0:
            logger.warning("No target layers found for calibration.")
            return {"status": "skipped"}

        # æ¢ç´¢ç¯„å›²: é–¾å€¤ã‚’ 0.5å€ ã€œ 2.0å€ ã®ç¯„å›²ã§èª¿æ•´
        bounds = [(0.5, 2.0) for _ in range(num_params)]

        # ç›®çš„é–¢æ•°ã®ãƒ©ãƒƒãƒ—
        def objective_wrapper(particles: np.ndarray) -> np.ndarray:
            scores = []
            for p in particles:
                scores.append(self._evaluate_params(p))
            return np.array(scores)

        # HSEOå®Ÿè¡Œ
        best_scales, best_score = optimize_with_hseo(
            objective_function=objective_wrapper,
            dim=num_params,
            num_particles=self.hseo_particles,
            max_iterations=self.hseo_iterations,
            exploration_range=bounds,
            verbose=True
        )

        logger.info(f"âœ… Calibration Complete. Best Score: {best_score:.4f}")

        # æœ€é©ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®é©ç”¨ï¼ˆæ°¸ç¶šåŒ–ï¼‰
        applied_config = {}
        for i, layer in enumerate(self.target_layers):
            factor = float(best_scales[i])
            layer_name = self.layer_names[i]
            applied_config[layer_name] = factor

            if isinstance(layer, AdaptiveLIFNeuron):
                layer.base_threshold.data.mul_(factor)
            elif isinstance(layer, DualThresholdNeuron):
                layer.threshold_high.data.mul_(factor)
                layer.threshold_low.data.mul_(factor)
            elif isinstance(layer, ScaleAndFireNeuron):
                layer.thresholds.data.mul_(factor)

        return {
            "best_score": best_score,
            "scaling_factors": applied_config
        }
