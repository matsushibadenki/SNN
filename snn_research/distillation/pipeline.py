# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/distillation/pipeline.py
# æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: Advanced Distillation Pipeline (Specialist & CoT)
# ç›®çš„ãƒ»å†…å®¹:
#   ROADMAP v16.5 "Distillation and T=1 Integration" ãŠã‚ˆã³ Phase 5-H ã®å®Ÿè£…ã€‚
#   1. Specialist Distillation: è¤‡æ•°ã®å°‚é–€å®¶ãƒ¢ãƒ‡ãƒ«ã‚’å˜ä¸€ã®ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã«çµ±åˆã€‚
#   2. CoT Distillation: ReasoningEngineã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹(System 2)ã‚’
#      è»½é‡ãƒ¢ãƒ‡ãƒ«(System 1)ã«è’¸ç•™ã—ã€ç›´æ„Ÿçš„ãªæ¨è«–èƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
from typing import List, Dict, Any
import logging
from tqdm import tqdm

from snn_research.cognitive_architecture.reasoning_engine import ReasoningEngine

logger = logging.getLogger(__name__)


class DistillationDataset(Dataset):
    """
    ã‚ªãƒ³ãƒ¡ãƒ¢ãƒªã¾ãŸã¯ç”Ÿæˆã•ã‚ŒãŸè’¸ç•™ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
    """

    def __init__(self, data: List[Dict[str, Any]]):
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]


def collate_distillation_batch(batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:
    """
    è’¸ç•™ç”¨ãƒãƒƒãƒå‡¦ç†ã€‚å…¥åŠ›IDã€æ³¨æ„ãƒã‚¹ã‚¯ã€æ•™å¸«ã®å‡ºåŠ›ï¼ˆLogits/Thoughtsï¼‰ã‚’ã¾ã¨ã‚ã‚‹ã€‚
    """
    # ç°¡æ˜“å®Ÿè£…: ã™ã¹ã¦ã®è¦ç´ ãŒTensorã§ã‚ã‚‹ã“ã¨ã‚’å‰æã«stack/padã™ã‚‹
    # å®Ÿéš›ã«ã¯paddingå‡¦ç†ãŒå¿…è¦
    input_ids = [item['input_ids'] for item in batch]
    labels = [item.get('labels', item['input_ids']) for item in batch]

    # Padding (ç°¡æ˜“çš„)
    max_len = max(x.size(0) for x in input_ids)
    padded_inputs = torch.stack(
        [F.pad(x, (0, max_len - x.size(0))) for x in input_ids])
    padded_labels = torch.stack(
        [F.pad(x, (0, max_len - x.size(0))) for x in labels])

    batch_out = {
        'input_ids': padded_inputs,
        'labels': padded_labels
    }

    if 'teacher_logits' in batch[0]:
        teacher_logits = [item['teacher_logits'] for item in batch]
        padded_logits = torch.stack(
            [F.pad(x, (0, 0, 0, max_len - x.size(0))) for x in teacher_logits])
        batch_out['teacher_logits'] = padded_logits

    if 'thought_trace' in batch[0]:
        # ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç­‰ã¯ãƒªã‚¹ãƒˆã®ã¾ã¾æ¸¡ã™ã‹ã€åˆ¥é€”ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ãŒå¿…è¦
        pass

    return batch_out


class AdvancedDistillationPipeline:
    """
    é«˜åº¦ãªçŸ¥è­˜è’¸ç•™ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€‚
    Specialistçµ±åˆã¨CoTè’¸ç•™ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ã€‚
    """

    def __init__(
        self,
        student_model: nn.Module,
        device: str = "cpu",
        tokenizer: Any = None
    ):
        self.student_model = student_model.to(device)
        self.device = device
        self.tokenizer = tokenizer
        logger.info(
            f"âš—ï¸ AdvancedDistillationPipeline initialized on {device}.")

    def run_specialist_distillation(
        self,
        specialist_models: List[nn.Module],
        unlabeled_dataset: Dataset,
        epochs: int = 3,
        batch_size: int = 16,
        lr: float = 1e-4,
        temperature: float = 2.0,
        alpha: float = 0.5
    ):
        """
        [Multi-Teacher Distillation]
        è¤‡æ•°ã®å°‚é–€å®¶ãƒ¢ãƒ‡ãƒ«ï¼ˆä¾‹: ç”»åƒèªè­˜ã€è¨€èªå‡¦ç†ã€éŸ³å£°è§£æï¼‰ã®å‡ºåŠ›ã‚’çµ±åˆã—ã€
        å˜ä¸€ã®ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ï¼ˆæ±ç”¨SNNï¼‰ã«å­¦ç¿’ã•ã›ã‚‹ã€‚

        Args:
            specialist_models: æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆã€‚å„ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã¾ãŸã¯æ‹…å½“åˆ†ã‘ã§æ¨è«–ã™ã‚‹ã€‚
            unlabeled_dataset: ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆè’¸ç•™ç”¨ï¼‰ã€‚
            alpha: è’¸ç•™æå¤±ã¨ã‚¿ã‚¹ã‚¯æå¤±ã®ãƒãƒ©ãƒ³ã‚¹ä¿‚æ•°ã€‚
        """
        logger.info(
            f"ğŸ‘¨â€ğŸ« Starting Specialist Distillation with {len(specialist_models)} teachers...")

        optimizer = torch.optim.AdamW(self.student_model.parameters(), lr=lr)
        dataloader = DataLoader(
            unlabeled_dataset, batch_size=batch_size, shuffle=True)

        # æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«
        for model in specialist_models:
            model.to(self.device).eval()

        self.student_model.train()

        for epoch in range(epochs):
            total_loss = 0.0
            pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}")

            for batch in pbar:
                # ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ (Datasetã®å®Ÿè£…ä¾å­˜ã ãŒã€ã“ã“ã§ã¯input_idsã‚’æƒ³å®š)
                if isinstance(batch, dict):
                    inputs = batch['input_ids'].to(self.device)
                else:
                    inputs = batch.to(self.device)  # Tupleç­‰ã®å ´åˆ

                # 1. æ•™å¸«ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¨è«–
                teacher_logits_sum = None
                with torch.no_grad():
                    for teacher in specialist_models:
                        # å„æ•™å¸«ã®æ¨è«– (APIãŒçµ±ä¸€ã•ã‚Œã¦ã„ã‚‹å‰æ)
                        # SFormer/ReasoningEngineäº’æ›ã® forward -> (logits, ...)
                        out = teacher(inputs)
                        logits = out[0] if isinstance(out, tuple) else out

                        if teacher_logits_sum is None:
                            teacher_logits_sum = logits
                        else:
                            teacher_logits_sum += logits

                    if teacher_logits_sum is None:
                        # æ•™å¸«ãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã—ãªã„ã€ã¾ãŸã¯å‡ºåŠ›ãŒå¾—ã‚‰ã‚Œãªã‹ã£ãŸå ´åˆ
                        logger.warning(
                            "No teacher logits computed. Skipping batch.")
                        continue

                    # å¹³å‡åŒ– (Soft Target)
                    teacher_avg_logits = teacher_logits_sum / \
                        len(specialist_models)  # type: ignore

                # 2. ç”Ÿå¾’ã®æ¨è«–
                student_out = self.student_model(inputs)
                student_logits = student_out[0] if isinstance(
                    student_out, tuple) else student_out

                # 3. æå¤±è¨ˆç®— (KL Divergence)
                loss_distill = self._compute_distillation_loss(
                    student_logits, teacher_avg_logits, temperature
                )

                # (Optional) Hard Label Loss (ã‚‚ã—ãƒ©ãƒ™ãƒ«ãŒã‚ã‚Œã°)
                loss = loss_distill  # ã“ã“ã§ã¯è’¸ç•™æå¤±ã®ã¿

                # 4. æ›´æ–°
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                total_loss += loss.item()
                pbar.set_postfix({'loss': total_loss / (pbar.n + 1)})

            logger.info(
                f"âœ… Epoch {epoch+1} completed. Avg Loss: {total_loss / len(dataloader):.4f}")

    def run_cot_distillation(
        self,
        reasoning_engine: ReasoningEngine,
        prompts: List[str],
        epochs: int = 3,
        batch_size: int = 4,
        lr: float = 1e-5
    ):
        """
        [Chain-of-Thought Distillation]
        System 2 (ReasoningEngine) ãŒç”Ÿæˆã—ãŸã€Œæ€è€ƒéç¨‹ã¨çµè«–ã€ã‚’æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ç”Ÿæˆã—ã€
        System 1 (ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«) ã«å­¦ç¿’ã•ã›ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ç”Ÿå¾’ã¯ã€Œç›´æ„Ÿçš„ã«æ­£ã—ã„æ¨è«–ã€ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã€‚

        Process:
        1. Promptã‚’å…¥åŠ›ã€‚
        2. ReasoningEngineãŒæ€è€ƒ(Thought)ã¨å›ç­”(Answer)ã‚’ç”Ÿæˆã€‚
        3. ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã« `Prompt -> Thought -> Answer` ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’å­¦ç¿’ã•ã›ã‚‹ã€‚
        """
        logger.info(
            f"ğŸ§  Starting CoT Distillation (System 2 -> System 1) with {len(prompts)} prompts...")

        if not self.tokenizer:
            raise ValueError("Tokenizer is required for CoT distillation.")

        # 1. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ (Rollout)
        logger.info("   Generating reasoning traces...")
        cot_data = []

        for prompt in tqdm(prompts, desc="Reasoning Rollout"):
            # Tokenize
            input_ids = self.tokenizer.encode(
                prompt, return_tensors='pt').to(self.device)

            # System 2 Reasoning
            # ReasoningEngine.think_and_solve ã¯æ¤œè¨¼æ¸ˆã¿ã®ãƒ™ã‚¹ãƒˆãªå›ç­”ã‚’è¿”ã™
            result = reasoning_engine.think_and_solve(
                input_ids, task_type="general")

            if result['final_output'] is not None:
                # æ€è€ƒéç¨‹ã¨æ€è€ƒçµæœã‚’çµåˆã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ
                # thought_trace ã¯ãƒªã‚¹ãƒˆãªã®ã§çµåˆ

                # ReasoningEngineã®å®Ÿè£…æ¬¡ç¬¬ã ãŒã€final_outputã¯IDåˆ—
                answer_ids = result['final_output']

                # ã“ã“ã§ã¯ç°¡æ˜“çš„ã«ã€TeacherãŒå‡ºåŠ›ã—ãŸIDåˆ—ã‚’ãã®ã¾ã¾æ­£è§£ãƒ©ãƒ™ãƒ«ã¨ã™ã‚‹
                # (Prompt + Generated IDs)
                full_ids = torch.cat([input_ids, answer_ids], dim=1)

                cot_data.append({
                    'input_ids': full_ids.squeeze(0),  # (SeqLen)
                    # Causal LM training (next token prediction)
                    'labels': full_ids.squeeze(0)
                })

        if not cot_data:
            logger.warning("âš ï¸ No valid CoT data generated. Aborting.")
            return

        dataset = DistillationDataset(cot_data)
        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=True,
            collate_fn=collate_distillation_batch
        )

        # 2. è’¸ç•™ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° (Fine-tuning)
        optimizer = torch.optim.AdamW(self.student_model.parameters(), lr=lr)
        self.student_model.train()

        for epoch in range(epochs):
            total_loss = 0.0
            pbar = tqdm(
                dataloader, desc=f"CoT Distill Epoch {epoch+1}/{epochs}")

            for batch in pbar:
                input_ids = batch['input_ids'].to(self.device)
                labels = batch['labels'].to(self.device)

                # ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’
                # ç”Ÿå¾’ã¯SFormerç­‰ã®BaseModelã‚’æƒ³å®š (Auto-regressive losså†…è”µã¾ãŸã¯è‡ªå‰è¨ˆç®—)
                out = self.student_model(input_ids)
                logits = out[0] if isinstance(out, tuple) else out

                # Shift for causal LM loss
                shift_logits = logits[..., :-1, :].contiguous()
                shift_labels = labels[..., 1:].contiguous()

                loss = F.cross_entropy(
                    shift_logits.view(-1, shift_logits.size(-1)),
                    shift_labels.view(-1),
                    ignore_index=self.tokenizer.pad_token_id if self.tokenizer.pad_token_id else -100
                )

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                total_loss += loss.item()
                pbar.set_postfix({'loss': total_loss / (pbar.n + 1)})

        logger.info(
            "âœ… CoT Distillation complete. System 1 has absorbed System 2's reasoning patterns.")

    def _compute_distillation_loss(self, student_logits, teacher_logits, temperature):
        """
        Soft Targetã«å¯¾ã™ã‚‹KL Divergenceæå¤±ã‚’è¨ˆç®—ã™ã‚‹ã€‚
        """
        soft_targets = F.softmax(teacher_logits / temperature, dim=-1)
        soft_prob = F.log_softmax(student_logits / temperature, dim=-1)

        # KLDivLoss: reduction='batchmean' is recommended
        distill_loss = F.kl_div(soft_prob, soft_targets,
                                reduction='batchmean') * (temperature ** 2)
        return distill_loss

    def evaluate_student(self, test_dataset: Dataset) -> Dict[str, float]:
        """ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½è©•ä¾¡"""
        self.student_model.eval()
        # ç°¡æ˜“è©•ä¾¡ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆå®Ÿéš›ã«ã¯Accuracyãªã©ã‚’è¨ˆç®—ï¼‰
        return {"accuracy": 0.0}  # Dummy
