# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/distillation/system_distiller.py
# æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: System Distiller v1.2 - Async Fix
# ç›®çš„: asyncioã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ¼ã‚Œä¿®æ­£ã¨ã€AstrocyteNetworkã®å‹ã‚¨ãƒ©ãƒ¼å›é¿ã€‚

import asyncio
import torch
import torch.nn.functional as F
import logging
from typing import Dict, Any, List

from snn_research.core.snn_core import SNNCore
from snn_research.cognitive_architecture.reasoning_engine import ReasoningEngine
from snn_research.cognitive_architecture.astrocyte_network import AstrocyteNetwork

logger = logging.getLogger(__name__)

class SystemDistiller:
    """
    System 2 (ç†Ÿæ…®) ã®æ¨è«–ãƒ—ãƒ­ã‚»ã‚¹ã‚’ System 1 (ç›´æ„Ÿ) ã«è»¢ç§»ã•ã›ã‚‹è’¸ç•™å™¨ã€‚
    """

    def __init__(
        self,
        system1: SNNCore,
        system2: ReasoningEngine,
        astrocyte: AstrocyteNetwork,
        config: Dict[str, Any]
    ):
        self.system1 = system1
        self.system2 = system2
        # å‹ãƒ’ãƒ³ãƒˆã‚’æ˜ç¤º
        self.astrocyte: AstrocyteNetwork = astrocyte
        self.config = config
        
        self.temperature = float(config.get("distill_temperature", 2.0))
        
        self.optimizer = torch.optim.Adam(
            self.system1.parameters(), 
            lr=float(config.get("distill_lr", 1e-4))
        )

    async def distill_step(self, sensory_input: torch.Tensor) -> Dict[str, Any]:
        cost = 30.0
        # [Fix] æ˜ç¤ºçš„ãªãƒ¡ã‚½ãƒƒãƒ‰å‘¼ã³å‡ºã—
        if not self.astrocyte.request_resource("distillation_process", cost):
            return {"status": "skipped", "reason": "low_energy"}

        teacher_results = self.system2.process(sensory_input)
        teacher_output = teacher_results.get("final_output")
        
        if teacher_output is None:
            return {"status": "error", "reason": "teacher_no_output"}

        self.system1.train()
        self.optimizer.zero_grad()
        
        student_output = self.system1.forward(sensory_input)
        
        if isinstance(student_output, torch.Tensor) and isinstance(teacher_output, torch.Tensor):
            loss = self._calculate_distill_loss(student_output, teacher_output)
            loss.backward()
            self.optimizer.step()
            self.system1.reset_state()
            
            return {
                "status": "success",
                "loss": loss.item(),
                "verifier_score": teacher_results.get("verifier_score", 0.0)
            }
            
        return {"status": "skipped", "reason": "type_mismatch"}

    def _calculate_distill_loss(
        self, 
        student_logits: torch.Tensor, 
        teacher_logits: torch.Tensor
    ) -> torch.Tensor:
        if student_logits.shape != teacher_logits.shape:
             min_dim = min(student_logits.size(-1), teacher_logits.size(-1))
             student_logits = student_logits[..., :min_dim]
             teacher_logits = teacher_logits[..., :min_dim]

        soft_teacher = F.softmax(teacher_logits / self.temperature, dim=-1)
        soft_student = F.log_softmax(student_logits / self.temperature, dim=-1)
        
        return F.kl_div(soft_student, soft_teacher, reduction='batchmean') * (self.temperature ** 2)

    async def run_consolidation_phase(self, buffer: List[torch.Tensor]):
        logger.info(f"ğŸŒ™ Consolidation Phase: Distilling {len(buffer)} experiences...")
        results = []
        for experience in buffer:
            res = await self.distill_step(experience)
            results.append(res)
            await asyncio.sleep(0.01)
            
        # [Fix] æ˜ç¤ºçš„ãªå‹ã‚­ãƒ£ã‚¹ãƒˆã¾ãŸã¯ãƒ¡ã‚½ãƒƒãƒ‰åˆ©ç”¨
        # AstrocyteNetworkã§å®šç¾©ã•ã‚ŒãŸãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã™
        rate = float(len(buffer) * 0.5)
        self.astrocyte.monitor_neural_activity(firing_rate=rate)
        return results