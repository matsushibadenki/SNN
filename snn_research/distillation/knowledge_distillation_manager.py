# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/distillation/knowledge_distillation_manager.py
# æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: Knowledge Distillation Manager v2.1 (Async & Non-blocking)
# ç›®çš„ãƒ»å†…å®¹:
#   çŸ¥è­˜è’¸ç•™ãƒ—ãƒ­ã‚»ã‚¹ã‚’ç®¡ç†ã™ã‚‹ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã®éåŒæœŸå¯¾å¿œå¼·åŒ–ç‰ˆã€‚
#   - æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã‚„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®äº‹å‰è¨ˆç®—ï¼ˆãƒ­ã‚¸ãƒƒãƒˆæŠ½å‡ºï¼‰ã¨ã„ã£ãŸé‡ã„å‡¦ç†ã‚’
#     ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ã«ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ã—ã€Brain Kernelã®ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã—ãªã„ã‚ˆã†ã«ä¿®æ­£ã€‚
#   - AsyncArtificialBrain ã‹ã‚‰ã®å‘¼ã³å‡ºã—ã«å®Œå…¨å¯¾å¿œã€‚

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, PreTrainedTokenizerBase
from typing import Dict, Any, Optional, List, Callable, Tuple, cast, TypeAlias, Sized
import os
import logging
import asyncio
from concurrent.futures import ThreadPoolExecutor
from functools import partial
from omegaconf import DictConfig

# å¾ªç’°å‚ç…§å›é¿ã®ãŸã‚ã€datasetã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¯try-exceptã§ãƒ©ãƒƒãƒ—
try:
    from snn_research.data.datasets import SimpleTextDataset
except ImportError:
    class SimpleTextDataset(Dataset): # type: ignore
        def __init__(self, *args, **kwargs): pass
        def __len__(self): return 0
        def __getitem__(self, idx): return {}

import torchvision.models as models # type: ignore[import-untyped]

from snn_research.distillation.model_registry import ModelRegistry
from snn_research.training.trainers import DistillationTrainer

logger = logging.getLogger(__name__)

TextCollateFnDef: TypeAlias = Callable[[PreTrainedTokenizerBase, bool], Callable[[List[Any]], Any]]

try:
    from app.utils import collate_fn as text_collate_fn
    collate_fn_orig_factory: TextCollateFnDef = cast(TextCollateFnDef, text_collate_fn)
except ImportError:
    logger.warning("Warning: Could not import collate_fn from app.utils.py.")
    def _fallback_collate(batch: List[Any]) -> Any:
        raise NotImplementedError("Fallback collate_fn called.")
    def fallback_collate_fn_def(tokenizer: PreTrainedTokenizerBase, is_distillation: bool) -> Callable[[List[Any]], Any]:
        return _fallback_collate
    collate_fn_orig_factory = fallback_collate_fn_def


class _PrecomputedDistillationDataset(Dataset):
    """
    åˆæœŸåŒ–æ™‚ã«æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’ä¸€æ‹¬è¨ˆç®—ã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹Datasetã€‚
    """
    def __init__(
        self,
        original_dataset: Dataset,
        teacher_model: nn.Module,
        device: str,
        collate_fn_orig: Callable[[List[Any]], Any],
        batch_size: int = 16
    ):
        self.original_dataset = original_dataset
        self.cached_logits: List[torch.Tensor] = []
        
        logger.info(f"ğŸ”„ Pre-computing teacher logits for {len(cast(Sized, original_dataset))} samples...")
        
        teacher_model.eval()
        
        # æ¨è«–ç”¨ã®DataLoader (num_workers=0ã§ç¾åœ¨ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œ)
        temp_loader = DataLoader(
            original_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=0,
            collate_fn=collate_fn_orig
        )
        
        # æ¨è«–ãƒ«ãƒ¼ãƒ—
        with torch.no_grad():
            for batch in temp_loader:
                teacher_logits: torch.Tensor
                
                if isinstance(batch, dict):
                    if 'input_ids' in batch:
                        input_ids = batch['input_ids'].to(device)
                        attention_mask = batch.get('attention_mask')
                        if attention_mask is not None:
                            attention_mask = attention_mask.to(device)
                        outputs = teacher_model(input_ids=input_ids, attention_mask=attention_mask)
                        teacher_logits = outputs.logits
                        
                    elif 'input_images' in batch:
                        input_images = batch['input_images'].to(device)
                        teacher_logits = teacher_model(input_images)
                        if teacher_logits.dim() == 2:
                            teacher_logits = teacher_logits.unsqueeze(1)
                    else:
                        continue
                elif isinstance(batch, (list, tuple)):
                     inputs = batch[0].to(device)
                     if inputs.dtype == torch.long: # text
                         outputs = teacher_model(input_ids=inputs)
                         teacher_logits = outputs.logits
                     else: # image
                         teacher_logits = teacher_model(inputs)
                else:
                    continue

                # ãƒ¡ãƒ¢ãƒªç¯€ç´„ã®ãŸã‚CPUã¸ç§»å‹•ã—FP16åŒ–
                teacher_logits_cpu = teacher_logits.cpu().to(torch.float16)
                
                for i in range(teacher_logits_cpu.size(0)):
                    self.cached_logits.append(teacher_logits_cpu[i])
        
        logger.info("âœ… Teacher logits pre-computation complete.")

    def __len__(self) -> int:
        return len(self.cached_logits)

    def __getitem__(self, idx: int) -> Tuple[Any, torch.Tensor]:
        return self.original_dataset[idx], self.cached_logits[idx]


class KnowledgeDistillationManager:
    def __init__(
        self,
        student_model: nn.Module,
        trainer: DistillationTrainer,
        model_registry: ModelRegistry,
        device: str,
        config: DictConfig,
        teacher_model: Optional[nn.Module] = None,
        teacher_model_name: Optional[str] = None,
        tokenizer_name: Optional[str] = None
    ):
        self.student_model = student_model
        self.teacher_model = teacher_model
        self.teacher_model_name = teacher_model_name
        self.trainer = trainer
        self.model_registry = model_registry
        self.device = device
        self.config = config 
        
        # é‡ã„å‡¦ç†ç”¨ã«Executorã‚’ç”¨æ„
        self.executor = ThreadPoolExecutor(max_workers=1, thread_name_prefix="DistillationWorker")

        if not teacher_model and not teacher_model_name:
            raise ValueError("Either teacher_model (instance) or teacher_model_name (str) must be provided.")
            
        self.tokenizer_name = tokenizer_name if tokenizer_name else cast(str, teacher_model_name)
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
        except Exception:
            logger.warning(f"Could not load tokenizer '{self.tokenizer_name}'. Using gpt2 fallback.")
            self.tokenizer = AutoTokenizer.from_pretrained("gpt2")
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token

    def _load_teacher_model_sync(self) -> nn.Module:
        """åŒæœŸçš„ã«æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ï¼ˆåˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œç”¨ï¼‰"""
        if self.teacher_model:
            return self.teacher_model.to(self.device).eval()

        if not self.teacher_model_name:
             raise ValueError("teacher_model_name is not set.")

        print(f"ğŸ§  Loading teacher model '{self.teacher_model_name}' on {self.device}...")
        try:
            if self.teacher_model_name == "resnet18":
                model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)
                model.fc = torch.nn.Linear(model.fc.in_features, 10)
                teacher_weights_path = f"models/{self.teacher_model_name}_cifar10.pth"
                if os.path.exists(teacher_weights_path):
                    state_dict = torch.load(teacher_weights_path, map_location=self.device)
                    model.load_state_dict(state_dict)
            else:
                model = AutoModelForCausalLM.from_pretrained(self.teacher_model_name)
            
            model = model.to(self.device).eval()
            return model
        except Exception as e:
            logger.error(f"âŒ Failed to load teacher model: {e}")
            raise

    async def _get_or_load_teacher_model(self) -> nn.Module:
        """éåŒæœŸãƒ©ãƒƒãƒ‘ãƒ¼: æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ã¾ãŸã¯ãƒ­ãƒ¼ãƒ‰"""
        if self.teacher_model:
            return self.teacher_model

        loop = asyncio.get_running_loop()
        # é‡ã„ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚’åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œ
        self.teacher_model = await loop.run_in_executor(
            self.executor, self._load_teacher_model_sync
        )
        return cast(nn.Module, self.teacher_model)

    async def run_on_demand_pipeline(
        self,
        task_description: str,
        unlabeled_data_path: str,
        force_retrain: bool = False,
        student_config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        print(f"--- On-Demand Learning Pipeline Initiated: {task_description} ---")

        # 1. æ—¢å­˜ãƒ¢ãƒ‡ãƒ«æ¤œç´¢
        if not force_retrain:
            existing_experts = await self.model_registry.find_models_for_task(task_description, top_k=1)
            if existing_experts:
                best_expert = existing_experts[0]
                best_expert['model_id'] = task_description
                print(f"âœ… Found existing expert: {best_expert.get('model_path')}")
                return best_expert

        if not os.path.exists(unlabeled_data_path):
            return {"error": f"Data file not found: {unlabeled_data_path}"}
        
        # 2. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™ã¨ãƒ­ã‚¸ãƒƒãƒˆè¨ˆç®—
        try:
            # DatasetåˆæœŸåŒ–ã¯è»½ã„ã®ã§ã“ã“ã§å®Ÿè¡Œ
            train_dataset_raw: Dataset = SimpleTextDataset(
                file_path=unlabeled_data_path,
                tokenizer=self.tokenizer,
                max_seq_len=self.config.model.time_steps 
            )
            
            if len(cast(Sized, train_dataset_raw)) < 10:
                 if len(cast(Sized, train_dataset_raw)) == 0:
                     return {"error": "No data found."}
                 train_dataset_raw = torch.utils.data.ConcatDataset([train_dataset_raw] * (10 // len(cast(Sized, train_dataset_raw)) + 1))

            print("Preparing distillation dataset (This may take a while, but Brain is active)...")
            
            # ã“ã“ãŒé‡ã„å‡¦ç†ï¼šéåŒæœŸã§å®Ÿè¡Œ
            train_loader, val_loader = await self.prepare_dataset( 
                train_dataset_raw,
                None, 
                batch_size=self.config.training.batch_size, 
                collate_fn=None 
            )

        except Exception as e:
            logger.error(f"âŒ Dataset preparation failed: {e}", exc_info=True)
            return {"error": str(e)}

        print(f"Starting distillation training for {self.config.training.epochs} epochs...")
        
        # 3. å­¦ç¿’å®Ÿè¡Œ
        # Trainerå†…éƒ¨ã‚‚åŒæœŸçš„ãªã®ã§ã€å…¨ä½“ã‚’ãƒ©ãƒƒãƒ—ã™ã‚‹ã‹ã€Trainerã‚’éåŒæœŸåŒ–ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
        # ç¾çŠ¶ã¯ç°¡æ˜“çš„ã«ã“ã“ã‚‚Executorã§å›ã™ï¼ˆãŸã ã—GPUæ“ä½œã‚’å«ã‚€ãŸã‚æ³¨æ„ãŒå¿…è¦ï¼‰
        # PyTorchã®GPUæ“ä½œã¯GILã‚’è§£æ”¾ã™ã‚‹ãŸã‚ã€æ¨™æº–ã®ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ã§ã‚‚ä¸¦è¡Œæ€§ã¯å‡ºã‚‹
        loop = asyncio.get_running_loop()
        final_metrics = await loop.run_in_executor(
            self.executor,
            partial(
                self._run_distillation_sync,
                train_loader=train_loader,
                val_loader=val_loader,
                epochs=self.config.training.epochs,
                model_id=task_description,
                task_description=task_description,
                student_config=student_config
            )
        )

        print("âœ… On-demand learning finished.")
        return final_metrics

    def _run_distillation_sync(
        self,
        train_loader: DataLoader,
        val_loader: DataLoader,
        epochs: int,
        model_id: str,
        task_description: str,
        student_config: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """åŒæœŸå®Ÿè¡Œç”¨ã®å­¦ç¿’ãƒ«ãƒ¼ãƒ—ï¼ˆrun_in_executorã‹ã‚‰å‘¼ã°ã‚Œã‚‹ï¼‰"""
        best_metric = float('inf') 
        best_model_path = ""
        log_dir = self.config.training.log_dir 
        os.makedirs(log_dir, exist_ok=True)

        for epoch in range(epochs):
            # å­¦ç¿’
            _ = self.trainer.train_epoch(train_loader, epoch)
            
            # è©•ä¾¡
            if val_loader:
                val_metrics = self.trainer.evaluate(val_loader, epoch)
                metric_name = self.config.training.get("metric_to_optimize", "total")
                current_metric = val_metrics.get(metric_name, float('inf'))

                if current_metric < best_metric:
                    best_metric = current_metric
                    safe_model_id = model_id.replace(" ", "_").replace("/", "_")
                    best_model_path = os.path.join(log_dir, f"{safe_model_id}_best.pth")
                    config_to_save = student_config if student_config is not None else {} 
                    
                    self.trainer.save_checkpoint(
                        path=best_model_path,
                        epoch=epoch,
                        metric_value=best_metric,
                        config=config_to_save, 
                        tokenizer_name=self.tokenizer_name
                    )

        # æœ€çµ‚çµæœä½œæˆ
        final_metrics: Dict[str, Any] = {"accuracy": 0.0}
        if val_loader and os.path.exists(best_model_path):
            self.trainer.load_checkpoint(best_model_path)
            eval_res = self.trainer.evaluate(val_loader, epochs)
            final_metrics['accuracy'] = eval_res.get('accuracy', 0.0)

        # ãƒ¬ã‚¸ã‚¹ãƒˆãƒªç™»éŒ²ã¯éåŒæœŸãƒ¡ã‚½ãƒƒãƒ‰ãªã®ã§ã€ã“ã“ã§ã¯è¡Œã‚ãšãƒ‘ã‚¹ã‚’è¿”ã™ã®ãŒå®‰å…¨ã ãŒã€
        # RegistryãŒasyncio.runã‚’å†…éƒ¨ã§ä½¿ã†ã‹ã€ã“ã“ã ã‘åŒæœŸç‰ˆãƒ¡ã‚½ãƒƒãƒ‰ãŒã‚ã‚Œã°å‘¼ã¶ã€‚
        # ã“ã“ã§ã¯ç°¡æ˜“çš„ã«æƒ…å ±ã‚’è¿”ã™ã«ã¨ã©ã‚ã‚‹ã€‚
        
        return {
            "metrics": final_metrics,
            "path": best_model_path,
            "status": "training_complete"
        }

    def _create_dataset_sync(
        self,
        dataset: Dataset,
        teacher_model: nn.Module,
        collate_fn: Callable,
        batch_size: int
    ) -> Dataset:
        """ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆã®åŒæœŸå‡¦ç†"""
        return _PrecomputedDistillationDataset(
            original_dataset=dataset,
            teacher_model=teacher_model,
            device=self.device,
            collate_fn_orig=collate_fn,
            batch_size=batch_size
        )

    async def prepare_dataset(
        self,
        train_dataset: Dataset,
        val_dataset: Optional[Dataset] = None,
        batch_size: int = 16,
        num_workers: int = 0,
        collate_fn: Optional[Callable] = None
    ) -> Tuple[DataLoader, DataLoader]:
        
        # Collateæº–å‚™
        collate_fn_to_use: Callable[[List[Any]], Any]
        if collate_fn is None:
            collate_fn_factory = cast(TextCollateFnDef, collate_fn_orig_factory)
            collate_fn_to_use = collate_fn_factory(self.tokenizer, False)
        else:
            collate_fn_to_use = collate_fn

        # æ•™å¸«ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ï¼ˆéåŒæœŸï¼‰
        teacher_model_instance = await self._get_or_load_teacher_model()

        loop = asyncio.get_running_loop()
        
        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ã‚¸ãƒƒãƒˆè¨ˆç®—ï¼ˆåˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ï¼‰
        print("   -> Computing logits for Training set...")
        distill_train_dataset = await loop.run_in_executor(
            self.executor,
            partial(
                self._create_dataset_sync,
                dataset=train_dataset,
                teacher_model=teacher_model_instance,
                collate_fn=collate_fn_to_use,
                batch_size=batch_size
            )
        )
        
        # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ã‚¸ãƒƒãƒˆè¨ˆç®—ï¼ˆåˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ï¼‰
        distill_val_dataset: Dataset
        if val_dataset:
            print("   -> Computing logits for Validation set...")
            distill_val_dataset = await loop.run_in_executor(
                self.executor,
                partial(
                    self._create_dataset_sync,
                    dataset=val_dataset,
                    teacher_model=teacher_model_instance,
                    collate_fn=collate_fn_to_use,
                    batch_size=batch_size
                )
            )
        else:
            # ç°¡æ˜“åˆ†å‰²
            train_len = len(cast(Sized, distill_train_dataset))
            train_size = int(0.9 * train_len)
            val_size = train_len - train_size
            if train_size > 0 and val_size > 0:
                distill_train_dataset, distill_val_dataset = torch.utils.data.random_split(distill_train_dataset, [train_size, val_size])
            else:
                distill_val_dataset = distill_train_dataset

        # Collateé–¢æ•°ä½œæˆ
        distillation_collate_fn = self._create_distillation_collate_fn(collate_fn_to_use)

        # DataLoaderä½œæˆ
        train_loader = DataLoader(
            distill_train_dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=num_workers,
            collate_fn=distillation_collate_fn
        )
        val_loader = DataLoader(
            distill_val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            collate_fn=distillation_collate_fn
        )

        return train_loader, val_loader

    def _create_distillation_collate_fn(self, collate_fn_orig: Callable[[List[Any]], Any]) -> Callable:
        """è’¸ç•™ç”¨Collateé–¢æ•°ï¼ˆå¤‰æ›´ãªã—ï¼‰"""
        def distillation_collate(batch: List[Tuple[Dict[str, Any], torch.Tensor]]) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:
            original_batch_items = [item[0] for item in batch]
            teacher_logits_list = [item[1] for item in batch]

            collated_batch = collate_fn_orig(original_batch_items)
            
            if 'input_ids' in collated_batch:
                student_input = collated_batch['input_ids']
                attention_mask = collated_batch['attention_mask']
                student_target = collated_batch['labels']
            elif 'input_images' in collated_batch:
                student_input = collated_batch['input_images'] 
                student_target = collated_batch['labels']      
                attention_mask = torch.ones_like(student_target, dtype=torch.long) 
            else:
                raise KeyError("Unknown batch format")

            padded_teacher_logits = torch.nn.utils.rnn.pad_sequence(
                teacher_logits_list, batch_first=True, padding_value=0.0
            )

            # å½¢çŠ¶èª¿æ•´
            if student_input.dim() > 2: # Image
                if padded_teacher_logits.dim() == 3 and padded_teacher_logits.shape[1] == 1:
                    padded_teacher_logits = padded_teacher_logits.squeeze(1)
                return student_input, attention_mask, student_target, padded_teacher_logits

            # Text Length Matching
            max_len_student = student_input.shape[1]
            max_len_teacher = padded_teacher_logits.shape[1]
            
            if max_len_student > max_len_teacher:
                pad_size = max_len_student - max_len_teacher
                padding = torch.zeros(
                    (padded_teacher_logits.shape[0], pad_size, padded_teacher_logits.shape[2]),
                    dtype=padded_teacher_logits.dtype, device=padded_teacher_logits.device
                )
                padded_teacher_logits = torch.cat([padded_teacher_logits, padding], dim=1)
            elif max_len_teacher > max_len_student:
                pad_size = max_len_teacher - max_len_student
                pad_val = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else 0
                padding_input = torch.full((student_input.shape[0], pad_size), pad_val, dtype=student_input.dtype, device=student_input.device)
                student_input = torch.cat([student_input, padding_input], dim=1)
                padding_mask = torch.zeros((attention_mask.shape[0], pad_size), dtype=attention_mask.dtype, device=attention_mask.device)
                attention_mask = torch.cat([attention_mask, padding_mask], dim=1)
                padding_target = torch.full((student_target.shape[0], pad_size), -100, dtype=student_target.dtype, device=student_target.device)
                student_target = torch.cat([student_target, padding_target], dim=1)
            
            return student_input, attention_mask, student_target, padded_teacher_logits

        return distillation_collate