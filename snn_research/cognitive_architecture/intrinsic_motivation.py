# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/cognitive_architecture/intrinsic_motivation.py
# æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: Intrinsic Motivation System v2.5 (Phase 2: Intrinsic Reward)
# ç›®çš„ãƒ»å†…å®¹:
#   ROADMAP Phase 2 "Autonomy" ã«å¯¾å¿œã€‚
#   å¼·åŒ–å­¦ç¿’ã®ãŸã‚ã®å†…ç™ºçš„å ±é…¬(Intrinsic Reward)è¨ˆç®—ãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ ã€‚
#   å¥½å¥‡å¿ƒ(Curiosity)ã¨æœ‰èƒ½æ„Ÿ(Competence)ã®ãƒãƒ©ãƒ³ã‚¹ã«åŸºã¥ãè‡ªå¾‹çš„ãªå ±é…¬ã‚·ã‚°ãƒŠãƒ«ã‚’ç”Ÿæˆã™ã‚‹ã€‚

import torch.nn as nn
import logging
import numpy as np
from typing import Dict, Any, Optional, Callable, List

logger = logging.getLogger(__name__)

# çŸ¥è­˜ç²å¾—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®å‹å®šç¾©
KnowledgeCallback = Callable[[str, str, float, str], None]


class IntrinsicMotivationSystem(nn.Module):
    """
    AIã®å†…ç™ºçš„å‹•æ©Ÿï¼ˆæ„Ÿæƒ…ãƒ»æ¬²æ±‚ï¼‰ã‚’ç”Ÿæˆã™ã‚‹ã‚¨ãƒ³ã‚¸ãƒ³ã€‚
    AsyncBrainKernel (v2.x) ã¨ ArtificialBrain (v16.x) ã®ä¸¡æ–¹ã«å¯¾å¿œã€‚

    Phase 2 Update:
    - calculate_intrinsic_reward(): RLã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç”¨ã®å ±é…¬ã‚¹ã‚«ãƒ©ãƒ¼å€¤ã‚’è¨ˆç®—
    """

    def __init__(
        self,
        curiosity_weight: float = 1.0,
        boredom_decay: float = 0.995,
        boredom_threshold: float = 0.8,
        novelty_bonus: float = 1.0,  # æ–°å¥‡æ€§ã«å¯¾ã™ã‚‹å ±é…¬ä¿‚æ•°
        competence_bonus: float = 0.5  # èª²é¡Œé”æˆ(æœ‰èƒ½æ„Ÿ)ã«å¯¾ã™ã‚‹å ±é…¬ä¿‚æ•°
    ):
        super().__init__()
        self.curiosity_weight = curiosity_weight
        self.boredom_decay = boredom_decay
        self.boredom_threshold = boredom_threshold
        self.novelty_bonus = novelty_bonus
        self.competence_bonus = competence_bonus

        # çŠ¶æ…‹å±¥æ­´ï¼ˆé€€å±ˆåˆ¤å®šç”¨ï¼‰
        self.last_input_hash: Optional[int] = None
        self.repetition_count = 0

        # ç¾åœ¨ã®å‹•æ©ŸçŠ¶æ…‹ (0.0 - 1.0)
        self.drives: Dict[str, float] = {
            "curiosity": 0.5,    # çŸ¥çš„å¥½å¥‡å¿ƒ (Surpriseã«åŸºã¥ã)
            "boredom": 0.0,      # é€€å±ˆ (åå¾©ã«åŸºã¥ã)
            "survival": 1.0,     # ç”Ÿå­˜æœ¬èƒ½ (ã‚¨ãƒãƒ«ã‚®ãƒ¼æ®‹é‡ç­‰)
            "comfort": 0.5,      # å¿«é©ã•
            "competence": 0.3    # æœ‰èƒ½æ„Ÿ (äºˆæ¸¬æˆåŠŸã‚„ã‚¿ã‚¹ã‚¯é”æˆã«åŸºã¥ã)
        }

        # [Phase 2.1] çŸ¥è­˜ç²å¾—æ™‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ãƒªã‚¹ãƒˆ
        self._knowledge_callbacks: List[KnowledgeCallback] = []

        logger.info(
            "ğŸ”¥ Intrinsic Motivation System v2.5 (Intrinsic Reward Enabled) initialized.")

    def register_knowledge_callback(self, callback: KnowledgeCallback) -> None:
        """
        çŸ¥è­˜ç²å¾—æ™‚ã«å‘¼ã³å‡ºã•ã‚Œã‚‹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ç™»éŒ²ã™ã‚‹ã€‚
        CuriosityKnowledgeIntegrator.on_knowledge_acquired ã‚’ç™»éŒ²ã™ã‚‹ã“ã¨ã§ã€
        ç²å¾—ã—ãŸçŸ¥è­˜ãŒè‡ªå‹•çš„ã«çŸ¥è­˜ã‚°ãƒ©ãƒ•ã¸çµ±åˆã•ã‚Œã‚‹ã€‚
        """
        self._knowledge_callbacks.append(callback)
        logger.debug(
            f"ğŸ“ Knowledge callback registered. Total: {len(self._knowledge_callbacks)}")

    def notify_knowledge_acquired(
        self,
        query: str,
        content: str,
        surprise: float,
        source: str = "curiosity_search"
    ) -> None:
        """
        æ–°ã—ã„çŸ¥è­˜ã‚’ç²å¾—ã—ãŸã“ã¨ã‚’å…¨ã¦ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã«é€šçŸ¥ã™ã‚‹ã€‚
        """
        for callback in self._knowledge_callbacks:
            try:
                callback(query, content, surprise, source)
            except Exception as e:
                logger.warning(f"âš ï¸ Knowledge callback error: {e}")

    def process(self, input_payload: Any, prediction_error: Optional[float] = None) -> Dict[str, float]:
        """
        å…¥åŠ›ã«åŸºã¥ã„ã¦é©šã(Surprise)ã‚’è¨ˆç®—ã—ã€å‹•æ©ŸçŠ¶æ…‹ã‚’æ›´æ–°ã™ã‚‹ã€‚
        """
        surprise = 0.0

        # 1. äºˆæ¸¬èª¤å·®ã«åŸºã¥ãSurpriseè¨ˆç®—
        if prediction_error is not None:
            surprise = min(1.0, prediction_error)

            # äºˆæ¸¬èª¤å·®ã«ã‚ˆã‚‹é€€å±ˆãƒ»æœ‰èƒ½æ„Ÿã®æ›´æ–°
            if surprise < 0.1:
                # äºˆæ¸¬é€šã‚Šï¼ˆç°¡å˜ã™ãã‚‹ï¼‰ -> é€€å±ˆä¸Šæ˜‡ã€æœ‰èƒ½æ„Ÿå¾®å¢—
                self.repetition_count += 1
                boredom_delta = 0.05 * self.repetition_count
                self._update_drive("competence", 0.05)
            else:
                # é©šããŒã‚ã‚‹ï¼ˆæœªçŸ¥ï¼‰ -> é€€å±ˆè§£æ¶ˆã€å¥½å¥‡å¿ƒå……è¶³
                self.repetition_count = 0
                boredom_delta = -0.2
                # äºˆæ¸¬ãŒå¤–ã‚ŒãŸç›´å¾Œã¯ä¸€æ™‚çš„ã«æœ‰èƒ½æ„ŸãŒä¸‹ãŒã‚‹ãŒã€å­¦ç¿’ã®ãƒãƒ£ãƒ³ã‚¹
                self._update_drive("competence", -0.02)

        # 2. ãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ã®ç°¡æ˜“åˆ¤å®š (äºˆæ¸¬èª¤å·®ãŒãªã„å ´åˆ)
        elif isinstance(input_payload, (str, int, float)):
            input_hash = hash(input_payload)
            if input_hash == self.last_input_hash:
                self.repetition_count += 1
                surprise = 0.0
                boredom_delta = 0.1 * self.repetition_count
            else:
                self.repetition_count = 0
                surprise = 1.0
                boredom_delta = -0.5
            self.last_input_hash = input_hash
        else:
            boredom_delta = 0.01

        # å€¤ã®æ›´æ–°
        self._update_drive("curiosity", surprise * 0.2 - 0.01)  # è‡ªç„¶æ¸›è¡°ã‚ã‚Š
        self.drives["boredom"] = float(
            np.clip(self.drives["boredom"] + boredom_delta, 0.0, 1.0))

        # ãƒ­ã‚°å‡ºåŠ›
        if self.drives["boredom"] > 0.8:
            logger.debug(
                f"ğŸ¥± Boredom Level Critical: {self.drives['boredom']:.2f}")
        elif surprise > 0.8:
            logger.debug(
                f"âœ¨ High Surprise ({surprise:.2f})! Curiosity: {self.drives['curiosity']:.2f}")

        return self.get_internal_state()

    def calculate_intrinsic_reward(self, surprise: float, external_reward: float = 0.0) -> float:
        """
        å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆç”¨ã®ã€Œå†…ç™ºçš„å ±é…¬ã€ã‚’è¨ˆç®—ã™ã‚‹ã€‚
        Reward = å¤–çš„å ±é…¬ + (å¥½å¥‡å¿ƒä¿‚æ•° * æ–°å¥‡æ€§) + (æœ‰èƒ½æ„Ÿä¿‚æ•° * æœ‰èƒ½æ„Ÿ) - (é€€å±ˆãƒšãƒŠãƒ«ãƒ†ã‚£)

        Args:
            surprise (float): è¦³æ¸¬ã«ãŠã‘ã‚‹äºˆæ¸¬èª¤å·® (0.0 - 1.0)
            external_reward (float): ç’°å¢ƒã‹ã‚‰å¾—ã‚‰ã‚ŒãŸå¤–çš„å ±é…¬

        Returns:
            float: çµ±åˆã•ã‚ŒãŸå ±é…¬å€¤
        """
        # æ–°å¥‡æ€§ãƒœãƒ¼ãƒŠã‚¹ (Curiosity Driven)
        # å®Œå…¨ã«ãƒ©ãƒ³ãƒ€ãƒ ãªãƒã‚¤ã‚º(å¸¸ã«surprise=1)ã«ãƒãƒã‚‰ãªã„ã‚ˆã†ã€ã‚ã‚‹ç¨‹åº¦ã®äºˆæ¸¬å¯èƒ½æ€§ã‚‚é‡è¦–ã™ã‚‹ã€ŒICM (Intrinsic Curiosity Module)ã€çš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
        # ã“ã“ã§ã¯ç°¡æ˜“çš„ã«ã€ç¾åœ¨ã®Curiosityãƒ‰ãƒ©ã‚¤ãƒ–ãŒé«˜ã„ã»ã©ã€æ–°ã—ã„æƒ…å ±(surprise)ã«ä¾¡å€¤ã‚’æ„Ÿã˜ã‚‹ã‚ˆã†ã«ã™ã‚‹
        novelty_reward = self.novelty_bonus * \
            surprise * self.drives["curiosity"]

        # é€€å±ˆãƒšãƒŠãƒ«ãƒ†ã‚£
        boredom_penalty = 0.5 * self.drives["boredom"]

        # æœ‰èƒ½æ„Ÿãƒœãƒ¼ãƒŠã‚¹ (Competence)
        # ã‚¿ã‚¹ã‚¯ãŒã†ã¾ãã„ã£ã¦ã„ã‚‹(CompetenceãŒé«˜ã„)ã“ã¨è‡ªä½“ã‚’å ±é…¬ã¨ã™ã‚‹
        competence_reward = self.competence_bonus * self.drives["competence"]

        total_reward = external_reward + novelty_reward + \
            competence_reward - boredom_penalty

        return float(total_reward)

    def _update_drive(self, key: str, delta: float):
        """ãƒ‰ãƒ©ã‚¤ãƒ–å€¤ã‚’0.0-1.0ã®ç¯„å›²ã§å®‰å…¨ã«æ›´æ–°"""
        if key in self.drives:
            self.drives[key] = float(
                np.clip(self.drives[key] + delta, 0.0, 1.0))

    def update_drives(self, surprise: float, energy_level: float, fatigue_level: float, task_success: bool = False) -> Dict[str, float]:
        """ArtificialBrainäº’æ›: ç’°å¢ƒçŠ¶æ…‹ã«åŸºã¥ã„ã¦å…¨å‹•æ©Ÿã‚’æ›´æ–°"""
        # Curiosity
        if surprise > 0.1:
            self._update_drive("curiosity", 0.1)
        else:
            self._update_drive("curiosity", -0.01)  # è‡ªç„¶æ¸›è¡°

        # Survival (Energy based)
        self.drives["survival"] = max(0.0, 1.0 - (energy_level / 1000.0))

        # Competence
        if task_success:
            self._update_drive("competence", 0.1)
        else:
            self._update_drive("competence", -0.005)  # å¤±æ•—ã¾ãŸã¯ä½•ã‚‚ã—ãªã„ã¨è‡ªä¿¡å–ªå¤±

        return self.drives

    def get_internal_state(self) -> Dict[str, float]:
        """çŠ¶æ…‹å–å¾— (mypyå¯¾å¿œ: å€¤ã¯å…¨ã¦floatã§ã‚ã‚‹ã“ã¨ã‚’ä¿è¨¼)"""
        return {k: float(v) for k, v in self.drives.items()}
