# snn_research/cognitive_architecture/memory_consolidation.py
# Title: Hierarchical Memory System v1.0
# Description: ç¡çœ ã‚µã‚¤ã‚¯ãƒ«ã¨è¨˜æ†¶ã®å›ºå®šåŒ–ï¼ˆConsolidationï¼‰ã‚’ç®¡ç†ã™ã‚‹çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã€‚

from typing import Dict, List, Optional
import numpy as np
import time
import json
import logging
from dataclasses import dataclass, field

# æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ã«åŸºã¥ãï¼‰
from snn_research.cognitive_architecture.cortex import Cortex
from snn_research.cognitive_architecture.hippocampus import Hippocampus

logger = logging.getLogger(__name__)

@dataclass
class MemoryTrace:
    """è¨˜æ†¶ç—•è·¡ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    content: str
    spike_pattern: np.ndarray
    importance: float
    timestamp: float
    access_count: int = 0
    decay_rate: float = 0.05
    metadata: Dict = field(default_factory=dict)

class HierarchicalMemorySystem:
    """éšå±¤çš„è¨˜æ†¶ã‚·ã‚¹ãƒ†ãƒ  - äººé–“ã®è„³ã‚’æ¨¡å€£ã—ãŸè¨˜æ†¶ç®¡ç†"""
    
    def __init__(self, hippocampus: Optional[Hippocampus] = None, cortex: Optional[Cortex] = None):
        # ãƒ¬ã‚¤ãƒ¤ãƒ¼1: ä½œæ¥­è¨˜æ†¶ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ï¼‰
        self.working_memory: List[str] = []
        
        # ãƒ¬ã‚¤ãƒ¤ãƒ¼2: çŸ­æœŸè¨˜æ†¶ï¼ˆæµ·é¦¬ï¼‰- æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨
        self.hippocampus_module = hippocampus if hippocampus else Hippocampus()
        self.hippocampus_db: Dict[str, MemoryTrace] = {}
        
        # ãƒ¬ã‚¤ãƒ¤ãƒ¼3: ä¸­æœŸè¨˜æ†¶ - ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶
        self.episodic_memory: List[Dict] = []
        
        # ãƒ¬ã‚¤ãƒ¤ãƒ¼4: é•·æœŸè¨˜æ†¶ï¼ˆçš®è³ªï¼‰- SNNã®é‡ã¿ãŠã‚ˆã³RAG
        self.cortex_module = cortex if cortex else Cortex()
        self.cortical_weights: Optional[np.ndarray] = None
        
        # ãƒ¬ã‚¤ãƒ¤ãƒ¼5: çŸ¥è­˜ã‚°ãƒ©ãƒ•ï¼ˆCortexå†…ã®RAGSystemã¨çµ±åˆï¼‰
        self.knowledge_graph = self.cortex_module.rag_system
        
        logger.info("ğŸ§  HierarchicalMemorySystem initialized.")

    def store_experience(self, experience: Dict, spike_activity: np.ndarray):
        """çµŒé¨“ã‚’ä¿å­˜ - é‡è¦åº¦ã«å¿œã˜ã¦é©åˆ‡ãªéšå±¤ã¸æŒ¯ã‚Šåˆ†ã‘"""
        importance = self._calculate_importance(experience, spike_activity)
        
        # çµŒé¨“ã‚’ä¸€æ„ã®ã‚­ãƒ¼ã«å¤‰æ›
        exp_json = json.dumps(experience, ensure_ascii=False)
        key = self._generate_key(exp_json)
        
        trace = MemoryTrace(
            content=exp_json,
            spike_pattern=spike_activity,
            importance=importance,
            timestamp=time.time(),
            metadata=experience
        )
        
        # é‡è¦åº¦ã«å¿œã˜ãŸä¿å­˜å…ˆæ±ºå®š
        if importance > 0.8:
            # å³åº§ã«é•·æœŸè¨˜æ†¶ã¸ï¼ˆé«˜é‡è¦åº¦ï¼‰
            logger.info(f"âš¡ Instant consolidation for high importance memory: {key[:8]}...")
            self._consolidate_to_cortex(trace)
        elif importance > 0.4:
            # æµ·é¦¬ï¼ˆDBï¼‰ã§ä¿æŒï¼ˆä¸­é‡è¦åº¦ï¼‰
            self.hippocampus_db[key] = trace
            # æ—¢å­˜ã®Hippocampusãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã‚‚åŒæœŸï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            self.hippocampus_module.store_episode(experience)
        else:
            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶ã¸ï¼ˆä½é‡è¦åº¦ã€ä¸€æ™‚çš„ï¼‰
            self.episodic_memory.append({
                'experience': experience,
                'spike_pattern': spike_activity,
                'timestamp': time.time()
            })

    def _generate_key(self, content: str) -> str:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰ãƒãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ"""
        return str(hash(content))

    def _calculate_importance(self, experience: Dict, spikes: np.ndarray) -> float:
        """é‡è¦åº¦è¨ˆç®— - ã‚¹ãƒ‘ã‚¤ã‚¯åŒæœŸæ€§ã¨æ–°è¦æ€§ã‹ã‚‰åˆ¤æ–­"""
        # ã‚¹ãƒ‘ã‚¤ã‚¯åŒæœŸæ€§ï¼ˆé«˜ã„ã»ã©é‡è¦ï¼‰
        synchrony = self._spike_synchrony(spikes)
        
        # æ–°è¦æ€§ï¼ˆæ—¢å­˜è¨˜æ†¶ã¨ã®å·®ç•°ï¼‰- ç°¡æ˜“å®Ÿè£…ã¨ã—ã¦ãƒ©ãƒ³ãƒ€ãƒ ã¾ãŸã¯ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¾å­˜
        novelty = self._calculate_novelty(experience)
        
        # æ„Ÿæƒ…çš„ä¾¡å€¤ï¼ˆå ±é…¬ç³»ã®æ´»å‹•ï¼‰
        emotional_value = float(experience.get('reward', 0.0))
        
        # é‡ã¿ä»˜ã‘å¹³å‡
        score = 0.4 * synchrony + 0.4 * novelty + 0.2 * emotional_value
        return min(1.0, max(0.0, score))
    
    def _spike_synchrony(self, spikes: np.ndarray) -> float:
        """ã‚¹ãƒ‘ã‚¤ã‚¯åŒæœŸæ€§ã®è¨ˆç®—"""
        if spikes.size == 0:
            return 0.0
        # æ™‚é–“è»¸æ–¹å‘ã®åˆè¨ˆï¼ˆå„ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—ã®ç™ºç«æ•°ï¼‰
        time_bins = spikes.sum(axis=0)  
        if time_bins.size == 0:
            return 0.0
        # ç™ºç«æ•°ã®åˆ†æ•£ãŒå¤§ãã„ï¼ç‰¹å®šã®ç¬é–“ã«åŒæœŸã—ã¦ç™ºç«ã—ã¦ã„ã‚‹
        variance = np.var(time_bins)
        # æ­£è¦åŒ–ï¼ˆãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ãªå€¤ï¼‰
        return min(1.0, variance / 5.0)

    def _calculate_novelty(self, experience: Dict) -> float:
        """æ–°è¦æ€§ã®è¨ˆç®—"""
        # å®Ÿéš›ã«ã¯RAGã®æ¤œç´¢ã‚¹ã‚³ã‚¢ã®é€†æ•°ãªã©ã‚’ä½¿ç”¨ã™ã‚‹ãŒã€ã“ã“ã§ã¯ç°¡æ˜“å®Ÿè£…
        # 'novelty'ã‚­ãƒ¼ãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        return float(experience.get('novelty', 0.5))

    def sleep_consolidation(self, duration_steps: int = 1000):
        """ç¡çœ ã«ã‚ˆã‚‹è¨˜æ†¶å›ºå®šåŒ– - æµ·é¦¬â†’çš®è³ªã¸ã®è»¢é€"""
        logger.info(f"ğŸ’¤ Sleep consolidation starting ({duration_steps} steps)...")
        
        # æµ·é¦¬ã®è¨˜æ†¶ã‚’é‡è¦åº¦é †ã«ã‚½ãƒ¼ãƒˆ
        sorted_memories = sorted(
            self.hippocampus_db.items(),
            key=lambda x: x[1].importance,
            reverse=True
        )
        
        # è»¢é€å‡¦ç†
        consolidation_threshold = 0.5
        transferred_count = 0
        
        for key, trace in sorted_memories:
            if trace.importance < consolidation_threshold:
                break
                
            # ã‚¹ãƒ‘ã‚¤ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’STDPå­¦ç¿’ã§é‡ã¿ã«å¤‰æ›ã—ã€çŸ¥è­˜ã‚’RAGã¸
            self._consolidate_to_cortex(trace)
            transferred_count += 1
            
            # è»¢é€æ¸ˆã¿ã®ãŸã‚æµ·é¦¬ã‹ã‚‰å‰Šé™¤
            if key in self.hippocampus_db:
                del self.hippocampus_db[key]
        
        # ä½é‡è¦åº¦ã®è¨˜æ†¶ã¯å¿˜å´
        self._forget_low_importance_memories()
        
        logger.info(f"âœ… Consolidation complete. Transferred: {transferred_count}")
    
    def _consolidate_to_cortex(self, trace: MemoryTrace):
        """STDPã«ã‚ˆã‚‹é‡ã¿æ›´æ–°ã¨æ¦‚å¿µã®æ°¸ç¶šåŒ–"""
        # 1. çŸ¥è­˜ã®ä¿å­˜ (Cortex RAG)
        try:
            content_dict = json.loads(trace.content)
            query = content_dict.get('query', '')
            response = content_dict.get('response', '')
            text_to_save = f"Q: {query}\nA: {response}"
            
            self.cortex_module.consolidate_episode(
                text_to_save, 
                source="sleep_consolidation"
            )
        except json.JSONDecodeError:
            self.cortex_module.consolidate_episode(trace.content)

        # 2. SNNé‡ã¿ã®æ›´æ–° (ç°¡æ˜“STDP)
        if self.cortical_weights is None:
            # ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°xãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°ã®è¡Œåˆ—ã‚’æƒ³å®š (ä¾‹: 256x256)
            dim = trace.spike_pattern.shape[0] if trace.spike_pattern.ndim > 0 else 256
            self.cortical_weights = np.random.randn(dim, dim) * 0.01
        
        delta_w = self._compute_stdp_update(trace.spike_pattern)
        
        # å­¦ç¿’ç‡ã¯é‡è¦åº¦ã«æ¯”ä¾‹
        learning_rate = 0.01 * trace.importance
        
        # ã‚µã‚¤ã‚ºãŒåˆã†å ´åˆã®ã¿æ›´æ–°
        if delta_w.shape == self.cortical_weights.shape:
            self.cortical_weights += learning_rate * delta_w
    
    def _compute_stdp_update(self, spike_pattern: np.ndarray) -> np.ndarray:
        """STDP (Spike-Timing Dependent Plasticity) ã®ç°¡æ˜“è¨ˆç®—"""
        # spike_pattern: [Neurons, TimeSteps]
        if spike_pattern.ndim != 2:
            return np.zeros_like(self.cortical_weights) if self.cortical_weights is not None else np.zeros((256, 256))

        n_neurons, n_steps = spike_pattern.shape
        weight_update = np.zeros((n_neurons, n_neurons))
        
        # ç°¡æ˜“çš„ãªHebbå‰‡: åŒæ™‚ç™ºç«ã—ãŸãƒšã‚¢ã®çµåˆã‚’å¼·åŒ–
        # æœ¬æ¥ã¯pre/postã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°å·®ã‚’è¦‹ã‚‹ãŒã€ã“ã“ã§ã¯ç›¸é–¢è¡Œåˆ—ã§è¿‘ä¼¼
        firing_rates = np.mean(spike_pattern, axis=1) # [Neurons]
        weight_update = np.outer(firing_rates, firing_rates)
        
        return weight_update

    def _forget_low_importance_memories(self):
        """å¿˜å´æ›²ç·šã«åŸºã¥ãè¨˜æ†¶ã®å‰Šé™¤"""
        current_time = time.time()
        keys_to_delete = []
        
        for key, trace in self.hippocampus_db.items():
            # æ™‚é–“çµŒéã«ã‚ˆã‚‹æ¸›è¡°
            elapsed = current_time - trace.timestamp
            # é‡è¦åº¦ãŒæ¸›è¡°ã™ã‚‹
            decayed_importance = trace.importance * np.exp(-trace.decay_rate * (elapsed / 3600.0)) # 1æ™‚é–“å˜ä½
            
            # é–¾å€¤ã‚’ä¸‹å›ã£ãŸã‚‰å¿˜å´
            if decayed_importance < 0.2:
                keys_to_delete.append(key)
        
        for key in keys_to_delete:
            del self.hippocampus_db[key]
        
        if keys_to_delete:
            logger.info(f"ğŸ§¹ Forgotten {len(keys_to_delete)} low-importance memories.")