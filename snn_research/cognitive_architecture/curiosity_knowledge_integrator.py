# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/cognitive_architecture/curiosity_knowledge_integrator.py
# æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: Curiosity-Knowledge Graph Integrator v1.0
# ç›®çš„ãƒ»å†…å®¹:
#   ROADMAP Phase 2.1 ã€Œç²å¾—ã—ãŸçŸ¥è­˜ã®çŸ¥è­˜ã‚°ãƒ©ãƒ•ã¸ã®çµ±åˆã€ã‚’å®Ÿè£…ã€‚
#   å¥½å¥‡å¿ƒãƒ‰ãƒªãƒ–ãƒ³æ¤œç´¢(IntrinsicMotivation)ã§å¾—ãŸæƒ…å ±ã‚’ã€
#   çŸ¥è­˜ã‚°ãƒ©ãƒ•(RAGSystem)ãŠã‚ˆã³NeuroSymbolicBridgeã¸è‡ªå‹•çµ±åˆã™ã‚‹ã€‚

import logging
import re
from typing import Any, Dict, List, Optional, Callable
from dataclasses import dataclass, field

import torch

logger = logging.getLogger(__name__)


@dataclass
class AcquiredKnowledge:
    """å¥½å¥‡å¿ƒã§ç²å¾—ã—ãŸçŸ¥è­˜ã‚¨ãƒ³ãƒˆãƒª"""
    query: str                          # æ¤œç´¢ã‚¯ã‚¨ãƒª
    content: str                        # å–å¾—ã—ãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„
    source: str = "web_search"          # æƒ…å ±æº
    surprise_score: float = 0.0         # å¥½å¥‡å¿ƒã‚¹ã‚³ã‚¢
    entities: List[str] = field(default_factory=list)      # æŠ½å‡ºã•ã‚ŒãŸã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£
    relations: List[tuple] = field(default_factory=list)   # æŠ½å‡ºã•ã‚ŒãŸé–¢ä¿‚ (s, p, o)
    embedding: Optional[torch.Tensor] = None               # åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«


class CuriosityKnowledgeIntegrator:
    """
    å¥½å¥‡å¿ƒãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨çŸ¥è­˜ã‚°ãƒ©ãƒ•ã®ãƒ–ãƒªãƒƒã‚¸ã€‚
    æ–°ã—ã„çŸ¥è­˜ã‚’ç²å¾—ã—ãŸéš›ã«è‡ªå‹•çš„ã«ã‚°ãƒ©ãƒ•ã¸çµ±åˆã™ã‚‹ã€‚
    """

    def __init__(
        self,
        rag_system: Optional[Any] = None,
        neuro_symbolic_bridge: Optional[Any] = None,
        entity_extractor: Optional[Callable[[str], List[str]]] = None,
        relation_extractor: Optional[Callable[[str], List[tuple]]] = None,
        min_surprise_threshold: float = 0.3,  # ã“ã®é–¾å€¤ä»¥ä¸Šã®é©šããŒã‚ã‚‹çŸ¥è­˜ã®ã¿ç™»éŒ²
        max_pending_knowledge: int = 100      # ç¡çœ å‰ã«ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ã™ã‚‹æœ€å¤§æ•°
    ):
        self.rag = rag_system
        self.nsb = neuro_symbolic_bridge  # NeuroSymbolicBridge

        # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£/é–¢ä¿‚æŠ½å‡ºå™¨ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ç°¡æ˜“ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ï¼‰
        self.entity_extractor = entity_extractor or self._default_entity_extractor
        self.relation_extractor = relation_extractor or self._default_relation_extractor

        self.min_surprise_threshold = min_surprise_threshold
        self.max_pending_knowledge = max_pending_knowledge

        # æœªçµ±åˆã®çŸ¥è­˜ãƒãƒƒãƒ•ã‚¡ï¼ˆç¡çœ æ™‚ã«ã¾ã¨ã‚ã¦å‡¦ç†ï¼‰
        self.pending_knowledge: List[AcquiredKnowledge] = []

        # çµ±è¨ˆ
        self.stats = {
            "total_acquired": 0,
            "total_integrated": 0,
            "total_discarded": 0
        }

        logger.info("ğŸ”— CuriosityKnowledgeIntegrator initialized.")

    def on_knowledge_acquired(
        self,
        query: str,
        content: str,
        surprise_score: float,
        source: str = "web_search"
    ) -> Optional[AcquiredKnowledge]:
        """
        å¥½å¥‡å¿ƒãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒæ–°ã—ã„çŸ¥è­˜ã‚’ç²å¾—ã—ãŸéš›ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚

        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            content: å–å¾—ã—ãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            surprise_score: å¥½å¥‡å¿ƒã‚·ã‚¹ãƒ†ãƒ ãŒè¨ˆç®—ã—ãŸé©šãã‚¹ã‚³ã‚¢
            source: æƒ…å ±æºï¼ˆweb_search, dialogue, observationç­‰ï¼‰

        Returns:
            å‡¦ç†ã•ã‚ŒãŸAcquiredKnowledgeã€ã¾ãŸã¯é–¾å€¤æœªæº€ã§None
        """
        self.stats["total_acquired"] += 1

        # é–¾å€¤ãƒã‚§ãƒƒã‚¯
        if surprise_score < self.min_surprise_threshold:
            self.stats["total_discarded"] += 1
            logger.debug(
                f"ğŸ“‰ Knowledge discarded (surprise={surprise_score:.2f} < threshold={self.min_surprise_threshold})")
            return None

        # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¨é–¢ä¿‚ã‚’æŠ½å‡º
        entities = self.entity_extractor(content)
        relations = self.relation_extractor(content)

        knowledge = AcquiredKnowledge(
            query=query,
            content=content,
            source=source,
            surprise_score=surprise_score,
            entities=entities,
            relations=relations
        )

        # å³æ™‚çµ±åˆãƒ¢ãƒ¼ãƒ‰ï¼ˆRAGãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
        if self.rag is not None:
            self._integrate_to_rag(knowledge)

        # NeuroSymbolicBridgeã¸ã®ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã¯ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°
        if len(self.pending_knowledge) < self.max_pending_knowledge:
            self.pending_knowledge.append(knowledge)
        else:
            # ãƒãƒƒãƒ•ã‚¡ãŒã„ã£ã±ã„ãªã‚‰æœ€ã‚‚é©šãã®ä½ã„ã‚‚ã®ã‚’å‰Šé™¤
            self.pending_knowledge.sort(key=lambda k: k.surprise_score)
            if knowledge.surprise_score > self.pending_knowledge[0].surprise_score:
                self.pending_knowledge.pop(0)
                self.pending_knowledge.append(knowledge)
                logger.debug("ğŸ“¤ Replaced low-surprise knowledge in buffer.")

        logger.info(
            f"âœ¨ Knowledge acquired: '{query[:30]}...' "
            f"(entities={len(entities)}, relations={len(relations)}, surprise={surprise_score:.2f})")

        return knowledge

    def _integrate_to_rag(self, knowledge: AcquiredKnowledge):
        """çŸ¥è­˜ã‚’RAGã‚·ã‚¹ãƒ†ãƒ ã«çµ±åˆ"""
        if self.rag is None:
            return

        # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¿½åŠ 
        metadata = {
            "type": "curiosity_acquired",
            "query": knowledge.query,
            "source": knowledge.source,
            "surprise": knowledge.surprise_score,
            "entities": knowledge.entities
        }
        self.rag.add_knowledge(knowledge.content, metadata=metadata)

        # æŠ½å‡ºã•ã‚ŒãŸé–¢ä¿‚ã‚’ãƒˆãƒªãƒ—ãƒ«ã¨ã—ã¦è¿½åŠ 
        for rel in knowledge.relations:
            if len(rel) == 3:
                subj, pred, obj = rel
                self.rag.add_triple(subj, pred, obj, metadata={
                                    "source": "curiosity"})

        self.stats["total_integrated"] += 1

    def integrate_during_sleep(self) -> Dict[str, Any]:
        """
        ç¡çœ ã‚µã‚¤ã‚¯ãƒ«ä¸­ã«å‘¼ã³å‡ºã•ã‚Œã‚‹çµ±åˆå‡¦ç†ã€‚
        ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ã•ã‚ŒãŸçŸ¥è­˜ã‚’NeuroSymbolicBridgeã¨é€£æºã—ã¦å‡¦ç†ã€‚

        Returns:
            çµ±åˆãƒ¬ãƒãƒ¼ãƒˆ
        """
        if not self.pending_knowledge:
            return {"status": "no_pending_knowledge", "integrated": 0}

        integrated_count = 0
        grounded_concepts = []

        for knowledge in self.pending_knowledge:
            # 1. ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’SNNãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°
            if self.nsb is not None:
                for entity in knowledge.entities:
                    try:
                        self.nsb.ground_symbol(entity)  # ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°å®Ÿè¡Œ
                        grounded_concepts.append(entity)
                    except Exception as e:
                        logger.warning(
                            f"âš ï¸ Failed to ground entity '{entity}': {e}")

            # 2. é–¢ä¿‚ã‚’SNNçµåˆå¼·åŒ–ã¨ã—ã¦åæ˜ 
            if self.nsb is not None and hasattr(self.nsb, 'learn_from_dialogue'):
                # ç°¡æ˜“çš„ãªã‚¹ãƒ‘ã‚¤ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ
                import numpy as np
                dummy_pattern = np.random.randn(256)
                self.nsb.learn_from_dialogue(
                    knowledge.content[:500], dummy_pattern)

            integrated_count += 1

        # ãƒãƒƒãƒ•ã‚¡ã‚¯ãƒªã‚¢
        self.pending_knowledge.clear()

        report = {
            "status": "success",
            "integrated": integrated_count,
            "grounded_concepts": grounded_concepts,
            "total_stats": self.stats.copy()
        }

        logger.info(
            f"ğŸ›Œ Sleep integration complete: {integrated_count} knowledge entries processed, "
            f"{len(grounded_concepts)} concepts grounded.")

        return report

    def _default_entity_extractor(self, text: str) -> List[str]:
        """
        ç°¡æ˜“çš„ãªã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡ºï¼ˆå¤§æ–‡å­—ã§å§‹ã¾ã‚‹å˜èªï¼‰
        å®Ÿé‹ç”¨ã§ã¯ spaCy ã‚„ Transformerãƒ™ãƒ¼ã‚¹ã® NER ã‚’ä½¿ç”¨æ¨å¥¨
        """
        # å¤§æ–‡å­—ã§å§‹ã¾ã‚‹å˜èªã‚’æŠ½å‡º
        words = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', text)
        # é‡è¤‡é™¤å»ã—ã¤ã¤é †åºä¿æŒ
        seen = set()
        entities = []
        for w in words:
            if w not in seen and len(w) > 2:
                seen.add(w)
                entities.append(w)
        return entities[:20]  # æœ€å¤§20ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£

    def _default_relation_extractor(self, text: str) -> List[tuple]:
        """
        ç°¡æ˜“çš„ãªé–¢ä¿‚æŠ½å‡º (ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°)
        å®Ÿé‹ç”¨ã§ã¯ OpenIE ã‚„ KGæ§‹ç¯‰ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨æ¨å¥¨
        """
        relations = []

        # "X is Y" ãƒ‘ã‚¿ãƒ¼ãƒ³
        is_pattern = re.findall(
            r'(\b[A-Z][a-z]+\b)\s+is\s+(?:a|an|the)?\s*(\b[a-z]+\b)', text)
        for subj, obj in is_pattern:
            relations.append((subj, "is_a", obj))

        # "X causes Y" ãƒ‘ã‚¿ãƒ¼ãƒ³
        cause_pattern = re.findall(
            r'(\b[A-Z][a-z]+\b)\s+causes?\s+(\b[a-z]+\b)', text, re.IGNORECASE)
        for subj, obj in cause_pattern:
            relations.append((subj, "causes", obj))

        # "X has Y" ãƒ‘ã‚¿ãƒ¼ãƒ³
        has_pattern = re.findall(
            r'(\b[A-Z][a-z]+\b)\s+has\s+(?:a|an)?\s*(\b[a-z]+\b)', text)
        for subj, obj in has_pattern:
            relations.append((subj, "has", obj))

        return relations[:10]  # æœ€å¤§10é–¢ä¿‚

    def get_stats(self) -> Dict[str, Any]:
        """çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        return {
            **self.stats,
            "pending_count": len(self.pending_knowledge)
        }


# --- Factory Function for Brain Integration ---
def create_curiosity_integrator(
    rag_system: Optional[Any] = None,
    neuro_symbolic_bridge: Optional[Any] = None
) -> CuriosityKnowledgeIntegrator:
    """
    ãƒ–ãƒ¬ã‚¤ãƒ³ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰åˆ©ç”¨ã™ã‚‹ãŸã‚ã®ãƒ•ã‚¡ã‚¯ãƒˆãƒªé–¢æ•°ã€‚
    """
    return CuriosityKnowledgeIntegrator(
        rag_system=rag_system,
        neuro_symbolic_bridge=neuro_symbolic_bridge
    )
