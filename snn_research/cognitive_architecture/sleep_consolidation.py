# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/cognitive_architecture/sleep_consolidation.py
# æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: Sleep Consolidator v3.1 (VLM Compatible / Fix)
# ç›®çš„: Generative Replayã«ã‚ˆã‚‹è¨˜æ†¶ã®å®šç€ã€‚SpikingVLMã®å…¥å‡ºåŠ›ã«å¯¾å¿œã€‚

import logging
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Any, Optional, List

# ãƒ­ã‚¬ãƒ¼è¨­å®šã‚’å®‰å…¨ã«è¡Œã†
logger = logging.getLogger(__name__)


class SleepConsolidator(nn.Module):
    """
    ç¡çœ æ™‚ã®è¨˜æ†¶å›ºå®šåŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« (VLMå¯¾å¿œç‰ˆ)ã€‚
    """

    def __init__(self, memory_system: Any, target_brain_model: Optional[nn.Module] = None, **kwargs: Any):
        super().__init__()
        self.memory = memory_system
        self.brain_model = target_brain_model
        self.experience_buffer: List[Dict[str, Any]] = []
        self.dream_rate = kwargs.get('dream_rate', 0.1)
        logger.info("ğŸŒ™ Sleep Consolidator v3.1 (VLM Supported) initialized.")

    def perform_sleep_cycle(self, duration_cycles: int = 5) -> Dict[str, Any]:
        """ç¡çœ ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè¡Œ"""
        # å¼·åˆ¶çš„ã«printå‡ºåŠ›ï¼ˆãƒ­ã‚°ãŒå‡ºãªã„å ´åˆç”¨ï¼‰
        print(f"ğŸŒ™ Sleep cycle started for {duration_cycles} cycles.")
        logger.info(f"ğŸŒ™ Sleep cycle started for {duration_cycles} cycles.")

        loss_history = []
        dreams_replayed = 0

        # 1. ãƒãƒƒãƒ•ã‚¡ã®å†…å®¹ã‚’çµ±åˆ
        if self.experience_buffer:
            self._consolidate_buffer()

        # 2. ç”Ÿæˆçš„ãƒªãƒ—ãƒ¬ã‚¤ (å¤¢ã‚’è¦‹ã‚‹)
        if self.brain_model is not None:
            self.brain_model.eval()
            for i in range(duration_cycles):
                energy = self._dream_step()
                loss_history.append(energy)
                dreams_replayed += 1
                if i % 10 == 0:
                    logger.info(f"  ... Dream cycle {i}: Clarity={energy:.4f}")
        else:
            loss_history.extend([0.0 for _ in range(duration_cycles)])

        return {
            "consolidated": 0,
            "dreams_replayed": dreams_replayed,
            "loss_history": loss_history,
            "status": "COMPLETED"
        }

    def _consolidate_buffer(self) -> None:
        """ãƒãƒƒãƒ•ã‚¡å†…ã®çµŒé¨“ã‚’ã‚¯ãƒªã‚¢"""
        count = len(self.experience_buffer)
        logger.info(f"  Consolidating {count} episodic experiences...")
        self.experience_buffer.clear()

    def _dream_step(self) -> float:
        """
        Generative Replay: è¦–è¦šãƒã‚¤ã‚ºã‹ã‚‰æ„å‘³ã‚’è¦‹å‡ºã™
        """
        if self.brain_model is None:
            return 0.0

        try:
            device = next(self.brain_model.parameters()).device

            # 1. è¦–è¦šãƒã‚¤ã‚º (Random Visual Stimulation)
            noise_image = torch.randn(
                1, 3, 224, 224, device=device) * 0.5 + 0.5

            # 2. è¨€èªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ (Start Token)
            input_ids = torch.tensor([[101]], device=device, dtype=torch.long)

            # 3. å¤¢ã‚’è¦‹ã‚‹
            with torch.no_grad():
                outputs = self.brain_model(input_ids, input_images=noise_image)
                if isinstance(outputs, tuple):
                    logits = outputs[0]
                else:
                    logits = outputs

            # 4. å¤¢ã®é®®æ˜åº¦ã‚’è©•ä¾¡ (Confidence)
            probs = F.softmax(logits, dim=-1)
            max_prob, _ = probs.max(dim=-1)
            clarity = max_prob.mean().item()

            # 5. å¯å¡‘æ€§æ›´æ–° (ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³)
            if clarity > 0.3:
                self._apply_hebbian_reinforcement(clarity)

            return clarity

        except Exception as e:
            logger.warning(f"Dreaming failed: {e}")
            return 0.0

    def _apply_hebbian_reinforcement(self, strength: float):
        """
        å˜ç´”åŒ–ã•ã‚ŒãŸãƒ˜ãƒƒãƒ–å‰‡çš„å¼·åŒ–:
        å¤¢ï¼ˆGenerative Replayï¼‰ãŒé®®æ˜(strengthé«˜)ã§ã‚ã‚Œã°ã‚ã‚‹ã»ã©ã€
        ç¾åœ¨ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®çµåˆè·é‡ã‚’ã‚ãšã‹ã«å¼·åŒ–ï¼ˆçµ¶å¯¾å€¤ã‚’å¢—åŠ ï¼‰ã™ã‚‹ã€‚
        ã“ã‚Œã¯ã€Œé®®æ˜ã«æ€ã„å‡ºã›ã‚‹ï¼ˆç”Ÿæˆã§ãã‚‹ï¼‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯é‡è¦ãªçŸ¥è­˜ã§ã‚ã‚‹ã€ã¨ã„ã†ä»®å®šã«åŸºã¥ãã€‚
        """
        if self.brain_model is None:
            return

        reinforcement_factor = 0.0001 * strength  # éå¸¸ã«å°ã•ãªå­¦ç¿’ç‡

        with torch.no_grad():
            for name, param in self.brain_model.named_parameters():
                if param.requires_grad and "weight" in name:
                    # æ—¢å­˜ã®çµåˆã‚’å¼·åŒ–ï¼ˆç¬¦å·ã‚’ç¶­æŒã—ãŸã¾ã¾çµ¶å¯¾å€¤ã‚’å¤§ããã™ã‚‹ï¼‰
                    # w_new = w_old + factor * w_old
                    param.data += reinforcement_factor * param.data

        logger.debug(
            f"  ğŸ§  Hebbian reinforcement applied (Factor: {reinforcement_factor:.6f})")
