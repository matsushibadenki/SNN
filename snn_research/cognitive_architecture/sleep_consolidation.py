# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/cognitive_architecture/sleep_consolidation.py
# ã‚¿ã‚¤ãƒˆãƒ«: Sleep Consolidator (Type Safe)
# ä¿®æ­£å†…å®¹: mypyã‚¨ãƒ©ãƒ¼ (Incompatible types, union-attr, misc) ã‚’ä¿®æ­£ã€‚

import torch
import torch.nn as nn
import logging
import random
from typing import Dict, Any, Optional, List, Deque, cast
from collections import deque

logger = logging.getLogger(__name__)


class Episode:
    """
    ãƒ¬ã‚¬ã‚·ãƒ¼/ãƒ†ã‚¹ãƒˆäº’æ›ç”¨ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚³ãƒ³ãƒ†ãƒŠã€‚
    """

    def __init__(self, state: torch.Tensor, text: torch.Tensor, reward: float):
        self.state = state.cpu().detach()
        self.text = text.cpu().detach()
        self.reward = reward


class SleepConsolidator:
    """
    ç¡çœ ã«ã‚ˆã‚‹è¨˜æ†¶å›ºå®šåŒ–ã‚·ã‚¹ãƒ†ãƒ  (System 2 Consolidation)ã€‚
    æµ·é¦¬ã®çŸ­æœŸè¨˜æ†¶ã‚’ãƒªãƒ—ãƒ¬ã‚¤ã—ã€å¤§è„³çš®è³ªã®é•·æœŸè¨˜æ†¶(RAG/Weights)ã¸è»¢é€ãƒ»çµ±åˆã™ã‚‹ã€‚
    """

    def __init__(
        self,
        memory_system: Optional[Any] = None,
        hippocampus: Optional[Any] = None,
        cortex: Optional[Any] = None,
        target_brain_model: Optional[nn.Module] = None,
        agent: Optional[nn.Module] = None,
        optimizer: Optional[torch.optim.Optimizer] = None,
        config: Dict[str, Any] = {},
        device: Optional[str] = None 
    ):
        self.config = config
        self.device = device 
        self.hippocampus_buffer: Deque[Episode] = deque(maxlen=1000)
        
        # ä¾å­˜é–¢ä¿‚ã®è§£æ±º
        self.agent = target_brain_model if target_brain_model else agent
        self.cortex = cortex
        self.memory_system = memory_system
        
        self.batch_size = config.get("replay_batch_size", 32)
        self.learning_rate = config.get("sleep_learning_rate", 1e-4)
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®åˆæœŸåŒ–
        # [Mypy Fix] self.optimizer ã®å‹ãƒ’ãƒ³ãƒˆã‚’ Optional[torch.optim.Optimizer] ã¨ã—ã¦æ‰±ã†
        self.optimizer: Optional[torch.optim.Optimizer] = None

        if optimizer:
            self.optimizer = optimizer
        elif self.agent:
            # å‡çµã•ã‚Œã¦ã„ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿å¯¾è±¡
            params = [p for p in self.agent.parameters() if p.requires_grad]
            if params:
                self.optimizer = torch.optim.AdamW(params, lr=self.learning_rate)
            else:
                self.optimizer = None
        else:
            self.optimizer = None

        logger.info(f"ğŸ’¤ Sleep Consolidator v2.8 initialized.")

    @property
    def brain_model(self) -> Optional[nn.Module]:
        return self.agent

    @brain_model.setter
    def brain_model(self, model: nn.Module):
        self.agent = model

    def store_experience(self, image: torch.Tensor, text: torch.Tensor, reward: float):
        """è¦šé†’æ™‚ã®çµŒé¨“ã‚’æµ·é¦¬(ãƒãƒƒãƒ•ã‚¡)ã«ä¸€æ™‚ä¿å­˜"""
        episode = Episode(image, text, reward)
        self.hippocampus_buffer.append(episode)

    def perform_sleep_cycle(self, duration_cycles: int = 5, recent_memories: List[Any] = []) -> Dict[str, Any]:
        """ç¡çœ ã‚µã‚¤ã‚¯ãƒ«ã®å®Ÿè¡Œ (Replay & Consolidation)"""
        if not self.agent:
            return {"status": "skipped", "reason": "no_agent_model"}
        
        if len(self.hippocampus_buffer) < 2 and not recent_memories:
             return {"status": "skipped", "reason": "no_memories"}

        logger.info(f"ğŸŒ™ Starting Sleep Consolidation. Processing {len(self.hippocampus_buffer)} episodes over {duration_cycles} cycles.")
        
        self.agent.train()
        total_loss = 0.0
        consolidated_count = 0
        
        try:
            for cycle in range(duration_cycles):
                loss = self._train_step()
                total_loss += loss
                
                if self.hippocampus_buffer and random.random() < 0.3:
                    mem = self.hippocampus_buffer[0]
                    self._transfer_to_cortex(mem)
                    consolidated_count += 1
            
            return {
                "status": "success",
                "cycles": duration_cycles,
                "processed_episodes": len(self.hippocampus_buffer),
                "consolidated_to_cortex": consolidated_count,
                "avg_replay_loss": total_loss / duration_cycles,
                "knowledge_graph": {} 
            }
            
        except Exception as e:
            logger.error(f"Sleep cycle failed: {e}")
            return {"status": "failed", "error": str(e)}
        finally:
            self.agent.eval()

    def _train_step(self) -> float:
        """ãƒªãƒ—ãƒ¬ã‚¤ã«ã‚ˆã‚‹å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—"""
        if not self.optimizer or len(self.hippocampus_buffer) == 0:
            return 0.0
            
        batch_size = min(len(self.hippocampus_buffer), self.batch_size)
        batch = random.sample(self.hippocampus_buffer, batch_size)
        
        try:
            # [Mypy Fix] self.agent ãŒ None ã§ãªã„ã“ã¨ã‚’ä¿è¨¼
            if self.agent is None:
                return 0.0

            device = next(self.agent.parameters()).device
            
            states = torch.stack([e.state for e in batch]).to(device).squeeze(1).contiguous()
            
            self.optimizer.zero_grad()
            
            # Forward
            # [Mypy Fix] nn.Module ã¯ __call__ ã‚’æŒã¤ãŸã‚ã€é™çš„è§£æã‚¨ãƒ©ãƒ¼ã‚’æŠ‘åˆ¶ã¾ãŸã¯ã‚­ãƒ£ã‚¹ãƒˆ
            outputs = self.agent(states)
            
            if isinstance(outputs, tuple):
                logits = outputs[0]
            elif isinstance(outputs, dict):
                logits = outputs.get('logits', list(outputs.values())[0])
            else:
                logits = outputs
            
            # [Mypy Fix] vocab_size å±æ€§ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯ã‚’å®‰å…¨ã«è¡Œã†
            if hasattr(self.agent, "vocab_size") and logits.shape[-1] == getattr(self.agent, "vocab_size"):
                B, L, V = logits.shape
                targets = states
                if targets.dim() > 2:
                    targets = targets.squeeze(1)
                
                if targets.shape[1] > L:
                    targets = targets[:, :L]
                elif targets.shape[1] < L:
                    logits = logits[:, :targets.shape[1], :]
                    
                loss_fct = nn.CrossEntropyLoss()
                loss = loss_fct(logits.reshape(-1, V), targets.reshape(-1))
            else:
                loss = logits.mean() 
            
            loss.backward()
            self.optimizer.step()
            
            return float(loss.item())
                
        except Exception as e:
            logger.error(f"Replay training step failed: {e}")
            return 0.0
            
        return 0.0

    def _transfer_to_cortex(self, memory: Any):
        """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’é•·æœŸè¨˜æ†¶(Cortex/RAG)ã¸è»¢é€ãƒ»ä¿å­˜ã™ã‚‹"""
        pass 

    def _apply_hebbian_reinforcement(self, strength: float = 1.0):
        """
        [New Method] ç¡çœ ä¸­ã®ã‚·ãƒŠãƒ—ã‚¹å¼·åŒ–ï¼ˆHebbian Reinforcementï¼‰ã€‚
        """
        if not self.agent:
            return

        with torch.no_grad():
            for param in self.agent.parameters():
                if param.requires_grad:
                    reinforcement = param * (1e-7 * strength)
                    param.add_(reinforcement)
        
        logger.info(f"Hebbian reinforcement applied with strength {strength}")