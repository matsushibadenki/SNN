# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/cognitive_architecture/sleep_consolidation.py
# æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: Sleep Consolidator v3.5 (Phase 2: Autonomous Cycle)
# ç›®çš„: Hippocampusã‹ã‚‰Cortexã¸ã®è¨˜æ†¶è»¢é€ã€ãŠã‚ˆã³ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã«åŸºã¥ãGenerative Replayã‚’çµ±æ‹¬ã™ã‚‹ã€‚

import logging
import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Any, Optional, List, Union
import random

# å‹ãƒ’ãƒ³ãƒˆç”¨ (å¾ªç’°å‚ç…§å›é¿ã®ãŸã‚æ–‡å­—åˆ—ã§æŒ‡å®šã™ã‚‹å ´åˆã‚ã‚Š)
from snn_research.cognitive_architecture.hippocampus import Hippocampus
from snn_research.cognitive_architecture.cortex import Cortex

# ãƒ­ã‚¬ãƒ¼è¨­å®š
logger = logging.getLogger(__name__)


class SleepConsolidator(nn.Module):
    """
    ç¡çœ æ™‚ã®è¨˜æ†¶å›ºå®šåŒ–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚

    Functions:
    1. Memory Transfer: Hippocampus(STM) -> Cortex(LTM)
    2. Generative Replay (Dreaming): é‡è¦ãªè¨˜æ†¶ã‚’è„³ãƒ¢ãƒ‡ãƒ«(SNN/VLM)ã«å…¥åŠ›ã—ã€
       è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’ã‚’è¡Œã†ã“ã¨ã§é‡ã¿ã‚’èª¿æ•´ãƒ»å®šç€ã•ã›ã‚‹ã€‚
    """

    def __init__(
        self,
        memory_system: Any,  # Legacy support
        hippocampus: Optional['Hippocampus'] = None,
        cortex: Optional['Cortex'] = None,
        target_brain_model: Optional[nn.Module] = None,
        **kwargs: Any
    ):
        super().__init__()
        self.memory = memory_system  # Legacy
        self.hippocampus = hippocampus
        self.cortex = cortex
        self.brain_model = target_brain_model

        # å¤¢ã®ææ–™ã¨ãªã‚‹ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒãƒƒãƒ•ã‚¡ï¼ˆè»¢é€ä¸­ã«ä¸€æ™‚ä¿æŒï¼‰
        self.dream_seeds: List[str] = []

        self.dream_rate = kwargs.get('dream_rate', 0.1)
        logger.info(
            "ğŸŒ™ Sleep Consolidator v3.5 (Hippocampus-Cortex Link) initialized.")

    def perform_sleep_cycle(self, duration_cycles: int = 5) -> Dict[str, Any]:
        """
        å®Œå…¨ãªç¡çœ ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
        1. è¨˜æ†¶è»¢é€ (Transfer)
        2. å¤¢ã«ã‚ˆã‚‹ãƒªãƒ—ãƒ¬ã‚¤ (Replay/Dreaming)
        """
        logger.info(f"ğŸŒ™ Sleep cycle started for {duration_cycles} cycles.")
        print(f"ğŸŒ™ Sleep cycle started. Duration: {duration_cycles}")

        # 1. Hippocampus -> Cortex è»¢é€ (Memory Transfer)
        transferred_count = self._transfer_memories()

        loss_history = []
        dreams_replayed = 0

        # 2. Generative Replay (å¤¢ã‚’è¦‹ã‚‹)
        if self.brain_model is not None:
            self.brain_model.eval()  # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ï¼ˆãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç­‰ã‚’åˆ‡ã‚‹ï¼‰ã ãŒã€å¯å¡‘æ€§ã¯æœ‰åŠ¹ã«ã™ã‚‹å ´åˆãŒã‚ã‚‹

            for i in range(duration_cycles):
                # è»¢é€ã—ãŸè¨˜æ†¶(dream_seeds)ãŒã‚ã‚Œã°ã€ãã‚Œã‚’ç¨®ã«å¤¢ã‚’è¦‹ã‚‹
                seed_text = random.choice(
                    self.dream_seeds) if self.dream_seeds else None

                clarity = self._dream_step(seed_text=seed_text)
                loss_history.append(clarity)
                dreams_replayed += 1

                if i % 10 == 0:
                    logger.info(
                        f"  ... Dream cycle {i}: Clarity={clarity:.4f}")
        else:
            logger.warning(
                "  Running sleep cycle without brain_model. Skipping dreams.")
            loss_history.extend([0.0 for _ in range(duration_cycles)])

        # å¤¢ã®ç¨®ã‚’ã‚¯ãƒªã‚¢ï¼ˆå¿˜å´ï¼‰ã™ã‚‹ã‹ã€ä¸€éƒ¨æ®‹ã™ã‹ã¯ä»Šå¾Œã®èª²é¡Œã€‚ç¾åœ¨ã¯ã‚¯ãƒªã‚¢ã€‚
        self.dream_seeds.clear()

        return {
            "consolidated_items": transferred_count,
            "dreams_replayed": dreams_replayed,
            "loss_history": loss_history,
            "status": "COMPLETED"
        }

    def _transfer_memories(self) -> int:
        """
        Hippocampusã®ãƒãƒƒãƒ•ã‚¡ã‹ã‚‰è¨˜æ†¶ã‚’å–ã‚Šå‡ºã—ã€Cortexã¸ä¿å­˜ã™ã‚‹ã€‚
        åŒæ™‚ã«ã€å¤¢ã®ãƒªãƒ—ãƒ¬ã‚¤ç”¨ã«ãƒ­ãƒ¼ã‚«ãƒ«ãƒãƒƒãƒ•ã‚¡(dream_seeds)ã«ã‚‚ã‚³ãƒ”ãƒ¼ã™ã‚‹ã€‚
        """
        if self.hippocampus is None:
            logger.warning("  Hippocampus not connected. Skipping transfer.")
            return 0

        # STMã‹ã‚‰å–ã‚Šå‡ºã—ï¼ˆHippocampusã¯ç©ºã«ãªã‚‹ï¼‰
        memories = self.hippocampus.flush_memories()
        count = len(memories)

        if count == 0:
            logger.info("  No new memories to consolidate.")
            return 0

        logger.info(f"  Transferring {count} episodic memories to Cortex...")

        # Cortexã¸ä¿å­˜ & å¤¢ã®ç¨®ã¨ã—ã¦ä¿æŒ
        for mem in memories:
            mem_text = str(mem)
            self.dream_seeds.append(mem_text)

            if self.cortex:
                self.cortex.consolidate_episode(mem_text, source="sleep_cycle")

        return count

    def _dream_step(self, seed_text: Optional[str] = None) -> float:
        """
        Generative Replay: 
        ç›´è¿‘ã®è¨˜æ†¶(seed_text) ã¾ãŸã¯ ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¤ã‚º ã‹ã‚‰è„³æ´»å‹•ã‚’ç”Ÿæˆã—ã€
        Hebbianå­¦ç¿’å‰‡ã‚’é€šã˜ã¦ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®é‡ã¿ã‚’èª¿æ•´ã™ã‚‹ã€‚
        """
        if self.brain_model is None:
            return 0.0

        try:
            device = next(self.brain_model.parameters()).device

            # 1. å…¥åŠ›ç”Ÿæˆ
            if seed_text:
                # è¨˜æ†¶ãŒã‚ã‚‹å ´åˆ: è¨€èªå…¥åŠ›ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ (Token IDå¤‰æ›ã¯ç°¡æ˜“å®Ÿè£…)
                # æœ¬æ¥ã¯TokenizerãŒå¿…è¦ã ãŒã€ã“ã“ã§ã¯ãƒ€ãƒŸãƒ¼IDã¨ãƒã‚¤ã‚ºç”»åƒã§ä»£ç”¨
                # seed_textã®å†…å®¹ã«ã‚ˆã£ã¦embeddingã‚’å¤‰ãˆã‚‹ãªã©ã®å‡¦ç†ãŒç†æƒ³
                input_ids = torch.randint(
                    100, 1000, (1, 5), device=device)  # ä»®ã®ãƒˆãƒ¼ã‚¯ãƒ³åˆ—
            else:
                # è¨˜æ†¶ãŒãªã„å ´åˆ: ç´”ç²‹ãªãƒ©ãƒ³ãƒ€ãƒ å¤¢
                input_ids = torch.tensor(
                    [[101]], device=device, dtype=torch.long)

            # è¦–è¦šé‡ã¸ã®å…¥åŠ›: æŠ½è±¡çš„ãªå¤¢ (Gaussian Noise + Pattern)
            noise_image = torch.randn(
                1, 3, 224, 224, device=device) * 0.5 + 0.5

            # 2. å¤¢ã‚’è¦‹ã‚‹ (Forward Pass)
            with torch.no_grad():
                # Brain ModelãŒ (input_ids, input_images) ã‚’å—ã‘å–ã‚Œã‚‹ã¨ä»®å®š
                outputs = self.brain_model(input_ids, input_images=noise_image)

                # å‡ºåŠ›ã®å½¢å¼ã«å¯¾å¿œ (Tuple or Tensor)
                if isinstance(outputs, tuple):
                    logits = outputs[0]
                else:
                    logits = outputs

            # 3. å¤¢ã®é®®æ˜åº¦ (Clarity/Confidence)
            # å‡ºåŠ›ãŒç¢ºç‡åˆ†å¸ƒ(logits)ã§ã‚ã‚‹ã¨ä»®å®š
            if isinstance(logits, torch.Tensor):
                probs = F.softmax(logits, dim=-1)
                max_prob, _ = probs.max(dim=-1)
                clarity = max_prob.mean().item()
            else:
                clarity = 0.5  # Default

            # 4. å¯å¡‘æ€§æ›´æ–° (Consolidation Rule)
            # é®®æ˜ãªå¤¢(Clarityé«˜) ã¾ãŸã¯ è¨˜æ†¶ã«åŸºã¥ãå¤¢(seed_textã‚ã‚Š) ã®å ´åˆã€çµåˆã‚’å¼·åŒ–
            threshold = 0.2 if seed_text else 0.4

            if clarity > threshold:
                self._apply_hebbian_reinforcement(clarity)

            return clarity

        except Exception as e:
            logger.debug(f"Dreaming step skipped/failed: {e}")
            return 0.0

    def _apply_hebbian_reinforcement(self, strength: float):
        """
        å˜ç´”åŒ–ã•ã‚ŒãŸãƒ˜ãƒƒãƒ–å‰‡çš„å¼·åŒ–:
        ç™ºç«ã—ãŸçµŒè·¯ï¼ˆå¤¢ã®ä¸­ã§æ´»æ€§åŒ–ã—ãŸé‡ã¿ï¼‰ã‚’ã‚ãšã‹ã«å¼·åŒ–ã™ã‚‹ã€‚
        ã“ã‚Œã«ã‚ˆã‚Šã€é‡è¦ãªãƒ‘ã‚¿ãƒ¼ãƒ³ãŒã€Œç„¼ãä»˜ã‘ã€ã‚‰ã‚Œã‚‹ã€‚
        """
        if self.brain_model is None:
            return

        # å­¦ç¿’ç‡: å¤¢ã®ä¸­ã§ã¯éå¸¸ã«å°ã•ãè¨­å®š (æ—¢å­˜çŸ¥è­˜ã®ç ´å£Šã‚’é˜²ããŸã‚)
        reinforcement_factor = 1e-5 * strength

        with torch.no_grad():
            for name, param in self.brain_model.named_parameters():
                if param.requires_grad and "weight" in name:
                    # Hebbian Term: w += alpha * w (Self-reinforcement of active weights)
                    # å³å¯†ãªHebbå‰‡ (x * y) ã§ã¯ãªã„ãŒã€æ´»æ€§åŒ–ã—ã¦ã„ã‚‹ãƒ‘ã‚¹ã®é‡ã¿ã‚’å¢—å¼·ã™ã‚‹ç°¡æ˜“å®Ÿè£…
                    if param.grad is not None:
                        # BackwardãŒèµ°ã£ã¦ã„ã‚‹å ´åˆã¯å‹¾é…æ–¹å‘ã¸
                        param.data -= reinforcement_factor * param.grad
                    else:
                        # Forwardã®ã¿ã®å ´åˆã¯ã€ç¾åœ¨ã®é‡ã¿åˆ†å¸ƒã‚’å¼·åŒ–ï¼ˆWeight Decayã®é€†ï¼‰
                        # â€»æ³¨æ„: ç™ºæ•£ã‚’é˜²ããŸã‚æ­£è¦åŒ–é …ãŒå¿…è¦ã ãŒã€ã“ã“ã§ã¯çŸ­æœŸçš„å¼·åŒ–ã®ã¿ã¨ã™ã‚‹
                        param.data += reinforcement_factor * param.data * 0.01

        logger.debug(
            f"  ğŸ§  Synaptic weights adjusted (Factor: {reinforcement_factor:.2e})")
