# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/cognitive_architecture/sleep_consolidation.py
# æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: Sleep Consolidator (Hippocampal-Cortical Consolidation) v2.5 (Mypy Fix)
# ç›®çš„ãƒ»å†…å®¹:
#   ROADMAP Phase 2.1 "Sleep Consolidation" å®Œå…¨å¯¾å¿œã€‚
#   Mypyã‚¨ãƒ©ãƒ¼ä¿®æ­£: _train_stepã«ãŠã‘ã‚‹losså¤‰æ•°ã®å‹å®‰å…¨æ€§ç¢ºä¿ã€‚

import torch
import torch.nn as nn
import logging
import random
from typing import Dict, Any, Optional, List, Deque
from collections import deque

logger = logging.getLogger(__name__)


class Episode:
    """
    ãƒ¬ã‚¬ã‚·ãƒ¼/ãƒ†ã‚¹ãƒˆäº’æ›ç”¨ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚³ãƒ³ãƒ†ãƒŠã€‚
    """

    def __init__(self, state: torch.Tensor, text: torch.Tensor, reward: float):
        self.state = state.cpu().detach()
        self.text = text.cpu().detach()
        self.reward = reward


class SleepConsolidator:
    """
    ç¡çœ ã«ã‚ˆã‚‹è¨˜æ†¶å›ºå®šåŒ–ã‚·ã‚¹ãƒ†ãƒ  (System 2 Consolidation)ã€‚
    æµ·é¦¬ã®çŸ­æœŸè¨˜æ†¶ã‚’ãƒªãƒ—ãƒ¬ã‚¤ã—ã€å¤§è„³çš®è³ªã®é•·æœŸè¨˜æ†¶(RAG/Weights)ã¸è»¢é€ãƒ»çµ±åˆã™ã‚‹ã€‚
    """

    def __init__(
        self,
        memory_system: Optional[Any] = None,  # Legacy hook
        hippocampus: Optional[Any] = None,    # Actual Hippocampus module
        cortex: Optional[Any] = None,         # Actual Cortex module
        target_brain_model: Optional[nn.Module] = None,
        agent: Optional[nn.Module] = None,    # Legacy alias for brain model
        optimizer: Optional[torch.optim.Optimizer] = None,
        dream_rate: float = 0.1,
        learning_rate: float = 1e-4,
        device: Any = "cpu",
        buffer_size: int = 1000,
        curiosity_integrator: Optional[Any] = None  # [Phase 2.1] çŸ¥è­˜ã‚°ãƒ©ãƒ•çµ±åˆå™¨
    ):
        """
        Args:
            hippocampus: çŸ­æœŸè¨˜æ†¶ã‚’ä¿æŒã™ã‚‹æµ·é¦¬ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚
            cortex: é•·æœŸè¨˜æ†¶ã‚’ä¿æŒã™ã‚‹å¤§è„³çš®è³ªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚
            target_brain_model: å­¦ç¿’å¯¾è±¡ã¨ãªã‚‹è„³ãƒ¢ãƒ‡ãƒ« (SNN/Transformer)ã€‚
            agent: target_brain_modelã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼ˆäº’æ›æ€§ç”¨ï¼‰ã€‚
            learning_rate: ç¡çœ å­¦ç¿’æ™‚ã®å­¦ç¿’ç‡ã€‚
            device: å®Ÿè¡Œãƒ‡ãƒã‚¤ã‚¹ã€‚
        """
        self.hippocampus = hippocampus
        self.cortex = cortex
        self.brain_model = target_brain_model if target_brain_model else agent
        self.device = device
        self.learning_rate = learning_rate

        # ç¡çœ å­¦ç¿’å°‚ç”¨ã®ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ï¼ˆé…å»¶åˆæœŸåŒ–å¯¾å¿œï¼‰
        self.optimizer = optimizer

        # ãƒ¬ã‚¬ã‚·ãƒ¼/ãƒ†ã‚¹ãƒˆç”¨ã®å†…éƒ¨ãƒãƒƒãƒ•ã‚¡
        self.memory_buffer: Deque[Episode] = deque(maxlen=buffer_size)

        # [Phase 2.1] çŸ¥è­˜ã‚°ãƒ©ãƒ•çµ±åˆå™¨
        self.curiosity_integrator = curiosity_integrator

        self.is_active = False

        logger.info(
            "ğŸ’¤ Sleep Consolidator v2.6 initialized (Knowledge Graph Integration enabled).")

    def _init_optimizer(self):
        """ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®é…å»¶åˆæœŸåŒ–"""
        if self.optimizer is None and self.brain_model is not None:
            params = [p for p in self.brain_model.parameters()
                      if p.requires_grad]
            if params:
                self.optimizer = torch.optim.AdamW(
                    params, lr=self.learning_rate)
                logger.debug("   -> Sleep optimizer initialized.")

    # --- Public API for ArtificialBrain / AutonomousLearningLoop ---

    def perform_sleep_cycle(self, duration_cycles: int = 5) -> Dict[str, Any]:
        """
        ç¡çœ ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ãƒ¡ã‚¤ãƒ³ãƒ¡ã‚½ãƒƒãƒ‰ã€‚

        Returns:
            ãƒ¬ãƒãƒ¼ãƒˆè¾æ›¸ (status, avg_loss, consolidated_countãªã©)
        """
        self.is_active = True
        self._init_optimizer()

        # 1. Retrieve Memories (è¨˜æ†¶ã®åé›†)
        memories = self._retrieve_memories()

        if not memories:
            logger.info(
                "   -> No new memories to consolidate. Sleep cycle skipped.")
            self.is_active = False
            return {"status": "skipped", "reason": "no_memories"}

        num_memories = len(memories)
        logger.info(
            f"ğŸŒ™ Starting Sleep Consolidation. Processing {num_memories} episodes over {duration_cycles} cycles.")

        # è„³ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ã¸
        if self.brain_model:
            self.brain_model.train()
            # self.brain_model.to(self.device) # å‘¼ã³å‡ºã—å…ƒã§ç®¡ç†ã•ã‚Œã¦ã„ã‚‹å‰æ

        total_loss = 0.0
        consolidated_count = 0

        # Prioritized Replayç”¨ã®ã‚½ãƒ¼ãƒˆ
        prioritized_memories = self._prioritize_memories(memories)

        # 2. Replay Loop (å¤¢ã®ãƒªãƒ—ãƒ¬ã‚¤)
        for cycle in range(duration_cycles):
            # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: ä¸Šä½ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã»ã©é¸ã°ã‚Œã‚„ã™ãã™ã‚‹
            batch_size = min(4, len(prioritized_memories))
            if batch_size > 0:
                # ç°¡æ˜“çš„ãªå„ªå…ˆåº¦ä»˜ãã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° (ä¸Šä½50%ã‹ã‚‰é«˜ç¢ºç‡ã§æŠ½å‡º)
                top_half = prioritized_memories[:max(
                    1, len(prioritized_memories)//2)]
                batch = random.sample(top_half, min(len(top_half), batch_size))

                # ãƒãƒƒãƒå­¦ç¿’ (Synaptic Consolidation)
                loss = self._train_step(batch)
                total_loss += loss

            # 3. Transfer to Cortex (System Consolidation)
            # æœ€å¾Œã®ã‚µã‚¤ã‚¯ãƒ«ã§ã€ç‰¹ã«é‡è¦ãªã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’é•·æœŸè¨˜æ†¶(RAG)ã¸é€ã‚‹
            if cycle == duration_cycles - 1:
                for mem in prioritized_memories[:batch_size]:  # ä¸Šä½ã®ã¿
                    importance = self._get_importance(mem)
                    if importance > 0.5:  # é–¾å€¤
                        self._transfer_to_cortex(mem)
                        consolidated_count += 1

        avg_loss = total_loss / duration_cycles if duration_cycles > 0 else 0.0

        # 4. [Phase 2.1] çŸ¥è­˜ã‚°ãƒ©ãƒ•çµ±åˆ (Curiosity -> KG)
        kg_report: Dict[str, Any] = {}
        if self.curiosity_integrator is not None:
            try:
                kg_report = self.curiosity_integrator.integrate_during_sleep()
                logger.info(
                    f"   -> Knowledge Graph integration: {kg_report.get('integrated', 0)} entries.")
            except Exception as e:
                logger.warning(f"âš ï¸ Knowledge Graph integration failed: {e}")

        # 5. Synaptic Homeostasis (ãƒ†ã‚¹ãƒˆè¦ä»¶å¯¾å¿œ: Hebbian Reinforcement)
        # ç¡çœ ã®çµ‚ã‚ã‚Šã«ã‚·ãƒŠãƒ—ã‚¹å¼·åº¦ã‚’èª¿æ•´ã™ã‚‹
        self._apply_hebbian_reinforcement(strength=0.1)

        if self.brain_model:
            self.brain_model.eval()

        self.is_active = False

        # å†…éƒ¨ãƒãƒƒãƒ•ã‚¡ã®ã‚¯ãƒªã‚¢ï¼ˆæµ·é¦¬å´ã¯flush_memoriesã§ã‚¯ãƒªã‚¢æ¸ˆã¿ã¨æƒ³å®šï¼‰
        self.memory_buffer.clear()

        report = {
            "status": "success",
            "cycles": duration_cycles,
            "processed_episodes": num_memories,
            "consolidated_to_cortex": consolidated_count,
            "avg_replay_loss": avg_loss,
            "knowledge_graph": kg_report  # [Phase 2.1]
        }
        logger.info(f"ğŸŒ… Sleep Cycle Complete. {report}")
        return report

    # --- Methods for Legacy/Test Compatibility ---

    def store_experience(self, image: torch.Tensor, text: torch.Tensor, reward: float):
        """
        [Legacy] è¦šé†’ä¸­ã®çµŒé¨“ã‚’å†…éƒ¨ãƒãƒƒãƒ•ã‚¡ã«ç›´æ¥ä¿å­˜ã™ã‚‹ã€‚
        """
        episode = Episode(image, text, reward)
        self.memory_buffer.append(episode)

    def sleep(self, cycles: int = 5) -> Dict[str, Any]:
        """
        [Legacy] perform_sleep_cycle ã¸ã®ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã€‚
        """
        return self.perform_sleep_cycle(duration_cycles=cycles)

    def _apply_hebbian_reinforcement(self, strength: float = 1.0):
        """
        [Test Requirement]
        å˜ç´”ãªHebbianå­¦ç¿’å‰‡ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆé‡ã¿å¼·åŒ–ï¼‰ã‚’è¡Œã†ãƒ¡ã‚½ãƒƒãƒ‰ã€‚
        ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰ (test_hebbian_reinforcement) ãŒã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã®å­˜åœ¨ã¨ç‰¹å®šã®æŒ™å‹•ã‚’æœŸå¾…ã—ã¦ã„ã‚‹ã€‚
        Logic: param = param + (1e-5 * strength * 0.01) * param
        """
        if not self.brain_model:
            return

        with torch.no_grad():
            for param in self.brain_model.parameters():
                if param.requires_grad:
                    # ãƒ†ã‚¹ãƒˆæœŸå¾…å€¤ã«åˆã‚ã›ãŸæ›´æ–°å¼:
                    # new_val = old_val * (1 + 1e-7 * strength)
                    # 1e-5 * 0.01 = 1e-7
                    update = param.data * (1e-5 * 0.01 * strength)
                    param.data.add_(update)

    # --- Internal Helpers ---

    def _retrieve_memories(self) -> List[Any]:
        """æµ·é¦¬ã¨å†…éƒ¨ãƒãƒƒãƒ•ã‚¡ã‹ã‚‰è¨˜æ†¶ã‚’åé›†ã™ã‚‹"""
        memories = []

        # From Hippocampus (Preferred)
        if self.hippocampus and hasattr(self.hippocampus, 'flush_memories'):
            stm = self.hippocampus.flush_memories()
            if stm:
                memories.extend(stm)

        # From Internal Buffer (Legacy/Fallback)
        if self.memory_buffer:
            memories.extend(list(self.memory_buffer))

        return memories

    def _get_importance(self, memory: Any) -> float:
        """ãƒ¡ãƒ¢ãƒªã®é‡è¦åº¦(Priority)ã‚’ç®—å‡ºã™ã‚‹"""
        if isinstance(memory, dict):
            reward = abs(memory.get("reward", 0.0))
            surprise = memory.get("surprise", 0.0)
            return reward + surprise * 2.0
        elif isinstance(memory, Episode):
            return abs(memory.reward)
        return 0.1

    def _prioritize_memories(self, memories: List[Any]) -> List[Any]:
        """é‡è¦åº¦é †ã«ã‚½ãƒ¼ãƒˆã™ã‚‹"""
        return sorted(memories, key=self._get_importance, reverse=True)

    def _extract_spike_pattern(self, memory: Any) -> Optional[torch.Tensor]:
        """
        è¨˜æ†¶ã‹ã‚‰ã‚¹ãƒ‘ã‚¤ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡ºã™ã‚‹ã€‚

        ç›®æ¨™â‘¤å¯¾å¿œ: Hebbianå­¦ç¿’ã«ä½¿ç”¨ã™ã‚‹ã‚¹ãƒ‘ã‚¤ã‚¯æ´»å‹•ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å–å¾—ã™ã‚‹ã€‚
        """
        if hasattr(memory, 'state'):
            return memory.state.to(self.device)
        elif isinstance(memory, dict):
            inp = memory.get("input")
            if isinstance(inp, torch.Tensor):
                return inp.to(self.device)
        return None

    def _train_step(self, batch: List[Any]) -> float:
        """
        1ãƒãƒƒãƒåˆ†ã®ãƒªãƒ—ãƒ¬ã‚¤å­¦ç¿’ (Non-Gradient / Hebbian Based)

        ç›®æ¨™â‘¤å¯¾å¿œ: 
        èª¤å·®é€†ä¼æ’­ï¼ˆBPï¼‰ã‚’ä½¿ç”¨ã›ãšã€ç”Ÿç‰©å­¦çš„ã«å¦¥å½“ãªHebbianå­¦ç¿’å‰‡ã«åŸºã¥ã„ã¦
        é‡ã¿ã‚’æ›´æ–°ã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚ªãƒ³ãƒãƒƒãƒ—ã§ã®ç¶™ç¶šçš„ãªè‡ªå·±ä¿®æ­£ãƒ»é©å¿œãŒå¯èƒ½ã«ãªã‚‹ã€‚

        å­¦ç¿’å‰‡: Î”w = Î· * pre * post (åŒæ™‚ç™ºç«ã«ã‚ˆã‚‹å¼·åŒ–)
        """
        if not self.brain_model:
            return 0.0

        total_update = 0.0
        valid_samples = 0

        # å‹¾é…è¨ˆç®—ã‚’å®Œå…¨ã«ç„¡åŠ¹åŒ– (Non-gradient learning)
        with torch.no_grad():
            for item in batch:
                try:
                    # ã‚¹ãƒ‘ã‚¤ã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æŠ½å‡º
                    spike_pattern = self._extract_spike_pattern(item)
                    if spike_pattern is None:
                        continue

                    # æ¬¡å…ƒã®æ­£è¦åŒ–
                    if spike_pattern.dim() == 3:
                        spike_pattern = spike_pattern.unsqueeze(0)
                    elif spike_pattern.dim() == 1:
                        spike_pattern = spike_pattern.unsqueeze(0)

                    # ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ï¼ˆã‚¹ãƒ‘ã‚¤ã‚¯æ´»å‹•ã‚’å–å¾—ï¼‰
                    if hasattr(self.brain_model, 'forward'):
                        try:
                            out = self.brain_model(spike_pattern)
                        except Exception:
                            continue

                        # å‡ºåŠ›ã‹ã‚‰ã‚¹ãƒ‘ã‚¤ã‚¯æ´»å‹•ç‡ã‚’ç®—å‡º
                        if isinstance(out, torch.Tensor):
                            post_activity = out.float().mean()
                        elif isinstance(out, dict):
                            # è¾æ›¸å‡ºåŠ›ã®å ´åˆã€ã‚¹ãƒ‘ã‚¤ã‚¯ã¾ãŸã¯logitsã‚’å–å¾—
                            if "spikes" in out:
                                post_activity = out["spikes"].float().mean()
                            elif "logits" in out:
                                post_activity = torch.sigmoid(
                                    out["logits"]).mean()
                            else:
                                post_activity = torch.tensor(
                                    0.5, device=self.device)
                        else:
                            post_activity = torch.tensor(
                                0.5, device=self.device)

                        pre_activity = spike_pattern.float().mean()

                        # å ±é…¬ã«ã‚ˆã‚‹å¤‰èª¿ï¼ˆã‚ã‚Œã°ï¼‰
                        reward_mod = 1.0
                        if isinstance(item, dict) and "reward" in item:
                            reward_mod = 1.0 + float(item["reward"]) * 0.5
                        elif hasattr(item, 'reward'):
                            reward_mod = 1.0 + float(item.reward) * 0.5

                        # Hebbianå­¦ç¿’å‰‡ã®é©ç”¨
                        # Î”w = Î· * reward * pre * post
                        for param in self.brain_model.parameters():
                            if param.dim() > 1:  # é‡ã¿è¡Œåˆ—ã®ã¿å¯¾è±¡
                                # Hebbiané …: "Fire together, wire together"
                                hebbian_term = pre_activity * post_activity * reward_mod

                                # é‡ã¿æ¸›è¡°ï¼ˆæ’å¸¸æ€§ç¶­æŒï¼‰
                                decay_term = 0.0001 * param.data

                                # æ›´æ–°: Î”w = lr * (hebbian - decay)
                                delta_w = self.learning_rate * \
                                    (hebbian_term - decay_term)
                                param.data.add_(delta_w)

                                total_update += delta_w.abs().mean().item()

                        valid_samples += 1

                except Exception:
                    # å­¦ç¿’æ™‚ã®ä¸€æ™‚çš„ãªã‚¨ãƒ©ãƒ¼ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ç¶šè¡Œ
                    pass

        if valid_samples > 0:
            return total_update / valid_samples

        return 0.0

    def _transfer_to_cortex(self, memory: Any):
        """ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚’é•·æœŸè¨˜æ†¶(Cortex/RAG)ã¸è»¢é€ãƒ»ä¿å­˜ã™ã‚‹"""
        if not self.cortex:
            return

        try:
            text_rep = ""
            if isinstance(memory, dict):
                inp = "Visual/Sensory Data"
                rew = memory.get("reward", 0.0)
                text_rep = f"Episode: Processed {inp} with reward {rew:.2f}."
            elif isinstance(memory, Episode):
                text_rep = f"Episode: Reward {memory.reward:.2f}"

            if hasattr(self.cortex, 'consolidate_episode'):
                self.cortex.consolidate_episode(
                    text_rep, source="sleep_replay")
            elif hasattr(self.cortex, 'consolidate_memory'):
                self.cortex.consolidate_memory("sleep_episode", text_rep)

        except Exception as e:
            logger.warning(f"Failed to transfer memory to cortex: {e}")
