# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/cognitive_architecture/synesthetic_sleep.py
# æ—¥æœ¬èªžã‚¿ã‚¤ãƒˆãƒ«: Synesthetic Sleep Manager (Dream Consolidation) - Type Fixed
# ä¿®æ­£å†…å®¹: seed_idx ã®åž‹ã‚’ int ã«æ˜Žç¤ºçš„ã«ã‚­ãƒ£ã‚¹ãƒˆã—ã€mypyã‚¨ãƒ©ãƒ¼ã‚’è§£æ¶ˆã€‚

import torch
import torch.nn as nn
import logging
from typing import Dict, List

from snn_research.agent.synesthetic_agent import SynestheticAgent

logger = logging.getLogger(__name__)


class SynestheticSleepManager:
    """
    è‡ªå¾‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ç¡çœ ãƒ»è¨˜æ†¶å®šç€ã‚’ç®¡ç†ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚

    Process:
    1. REM Sleep (Dreaming): ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ãŒè‡ªå¾‹çš„ã«ã‚·ãƒŠãƒªã‚ª(å¤¢)ã‚’ç”Ÿæˆã€‚
    2. Consolidation: ç”Ÿæˆã•ã‚ŒãŸå¤¢ã‚’ã€Œç–‘ä¼¼ä½“é¨“ã€ã¨ã—ã¦Brainã«å…¥åŠ›ã—ã€é‡ã¿ã‚’æ›´æ–°ã€‚
    3. Evaluation: å¤¢ã®å†…å®¹ã¨Brainã®äºˆæ¸¬ã®æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯ã€‚
    """

    def __init__(self, agent: SynestheticAgent, learning_rate: float = 1e-4):
        self.agent = agent
        self.device = agent.device

        # ç¡çœ å­¦ç¿’ç”¨ã®ã‚ªãƒ—ãƒ†ã‚£ãƒžã‚¤ã‚¶ (Brainã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿æ›´æ–°)
        # â€»WorldModelã¯æ—¥ä¸­ã«å­¦ç¿’æ¸ˆã¿ã¨ã™ã‚‹ã€ã‚ã‚‹ã„ã¯ç¡çœ ä¸­ã«æ•´åˆæ€§ã‚’å–ã‚‹
        self.optimizer = torch.optim.AdamW(
            self.agent.brain.parameters(), lr=learning_rate)
        self.criterion = nn.CrossEntropyLoss()

        self.sleep_history: List[Dict[str, float]] = []

    def enter_sleep_cycle(self, initial_memories: List[Dict[str, torch.Tensor]], num_cycles: int = 5):
        """
        ç¡çœ ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ã€‚

        Args:
            initial_memories: æ—¥ä¸­ã«å¾—ãŸã€Œè¨˜æ†¶ã®ç¨®ã€(è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ã®æ–­ç‰‡)ã®ãƒªã‚¹ãƒˆã€‚
            num_cycles: ç¡çœ ã®æ·±ã•ï¼ˆåå¾©å›žæ•°ï¼‰ã€‚
        """
        logger.info("ðŸŒ™ Entering Sleep Mode... (Consolidating Memories)")
        self.agent.brain.train()
        self.agent.world_model.eval()  # å¤¢ã‚’è¦‹ã‚‹å´ã¯å›ºå®š

        total_consolidation_loss = 0.0

        for cycle in range(num_cycles):
            # 1. REM Sleep: Generate Dreams from seeds
            # è¨˜æ†¶ã®ç¨®ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã«1ã¤é¸ã‚“ã§å¤¢ã‚’è¦‹å§‹ã‚ã‚‹
            # mypy fix: item() returns int|float, explicit cast to int required for indexing
            seed_idx = int(torch.randint(
                0, len(initial_memories), (1,)).item())
            seed_obs = initial_memories[seed_idx]

            # å¤¢ã®ç”Ÿæˆ (Dreaming)
            # horizon: ã©ã‚Œãã‚‰ã„å…ˆã¾ã§å¤¢ã‚’è¦‹ã‚‹ã‹
            dream_horizon = 10
            dream_trajectory = self.agent.dream(
                seed_obs, horizon=dream_horizon)

            # 2. Consolidation: Learn from Dreams
            # å¤¢ã®ä¸­ã§èµ·ããŸå‡ºæ¥äº‹ï¼ˆæ„Ÿè¦šå…¥åŠ›ï¼‰ã«å¯¾ã—ã¦ã€BrainãŒã©ã†æ€è€ƒã™ã‚‹ã‹ã‚’å­¦ç¿’
            # (è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’: å¤¢ã®ä¸­ã®æ–‡è„ˆæ•´åˆæ€§ã‚’é«˜ã‚ã‚‹)

            loss = self._train_on_dream(dream_trajectory)
            total_consolidation_loss += loss

            if (cycle + 1) % 2 == 0:
                logger.info(
                    f"   ðŸ’¤ REM Cycle {cycle+1}/{num_cycles}: Consolidation Loss = {loss:.4f}")

        avg_loss = total_consolidation_loss / num_cycles
        self.sleep_history.append({'avg_loss': avg_loss, 'cycles': num_cycles})

        logger.info(
            f"ðŸŒ… Waking Up... Memory Consolidated (Avg Loss: {avg_loss:.4f})")
        return avg_loss

    def _train_on_dream(self, trajectory: List[Dict[str, torch.Tensor]]) -> float:
        """
        ç”Ÿæˆã•ã‚ŒãŸå¤¢ã®è»Œé“ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦Brainã‚’å­¦ç¿’ã•ã›ã‚‹ã€‚
        """
        self.optimizer.zero_grad()

        # è»Œé“ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒãƒå½¢å¼ã«å¤‰æ› (List[Dict] -> Dict[Tensor])
        # trajectory[t]['vision'] is (B, 1, D)
        # -> combined['vision'] is (B, T, D)
        combined_inputs = {}
        sample_keys = trajectory[0].keys()

        for key in sample_keys:
            # å„ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒ†ãƒ³ã‚½ãƒ«ã‚’çµåˆ
            # (B, 1, D) -> list -> (B, T, D)
            tensors = [step[key] for step in trajectory]
            combined_inputs[key] = torch.cat(tensors, dim=1)

        # Brainã«å…¥åŠ›
        # ã“ã“ã§ã¯ã€Œå¤¢ã®ä¸­ã®æ„Ÿè¦šå…¥åŠ›ã€ã‚’è¦‹ã¦ã€ã€Œæ¬¡ã®æ€è€ƒ(ã¾ãŸã¯è¡Œå‹•)ã€ã‚’äºˆæ¸¬ã•ã›ã‚‹ã‚¿ã‚¹ã‚¯ãªã©ãŒè€ƒãˆã‚‰ã‚Œã‚‹ãŒã€
        # ç°¡æ˜“çš„ã«ã€ŒBrainã®å†…éƒ¨çŠ¶æ…‹ã®å®‰å®šåŒ–ï¼ˆAuto-Associative Learningï¼‰ã€ã‚’è¡Œã†ã€‚
        # å…·ä½“çš„ã«ã¯ã€Brainã«é€šã—ã¦å‡ºåŠ›ã•ã‚Œã‚‹LogitsãŒã€ä½•ã‚‰ã‹ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆã“ã“ã§ã¯è‡ªå·±å›žå¸°çš„ãªæ¬¡ã‚¹ãƒ†ãƒƒãƒ—äºˆæ¸¬ãªã©ï¼‰
        # ã«åˆã†ã‚ˆã†ã«ã™ã‚‹ãŒã€æ•™å¸«ãƒ‡ãƒ¼ã‚¿ãŒãªã„ãŸã‚ã€
        # ã€Œä¸–ç•Œãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã€ã¨ã€ŒBrainã®äºˆæ¸¬ã€ã®ä¸€è‡´åº¦ã‚’é«˜ã‚ã‚‹å­¦ç¿’ã¨ã™ã‚‹ã€‚

        # ç°¡æ˜“å®Ÿè£…: Brainã«å¤¢ã‚’è¦‹ã›ã€ãã®å‡ºåŠ›ãŒç™ºæ•£ã—ãªã„ã‚ˆã†ã«æ­£å‰‡åŒ–ã€
        # ã¾ãŸã¯WorldModelãŒæŒã¤ã€Œæ½œåœ¨çš„ãªæ„å‘³ã€ã‚’BrainãŒè¨€èªžåŒ–(ãƒˆãƒ¼ã‚¯ãƒ³åŒ–)ã§ãã‚‹ã‹è©¦ã™ã€‚

        # ã“ã“ã§ã¯ã€Œè¡Œå‹•ç”Ÿæˆã®å®‰å®šåŒ–ã€ã‚’ç›®çš„ã¨ã—ã€BrainãŒå‡ºåŠ›ã™ã‚‹ActionãŒæ¥µç«¯ãªå€¤ã«ãªã‚‰ãªã„ã‚ˆã†å­¦ç¿’
        # (æœ¬æ¥ã¯ã‚‚ã£ã¨è¤‡é›‘ãªç›®çš„é–¢æ•°ãŒå¿…è¦)

        logits = self.agent.brain(
            text_input=None,  # è¨€èªžãªã—ã®ç´”ç²‹ãªå¤¢
            image_input=combined_inputs.get('vision'),
            audio_input=combined_inputs.get('audio'),
            tactile_input=combined_inputs.get('tactile'),
            olfactory_input=combined_inputs.get('olfactory')
        )

        # Loss:
        # 1. æ´»æ€§ã®ã‚¹ãƒ‘ãƒ¼ã‚¹åŒ– (ã‚¨ãƒãƒ«ã‚®ãƒ¼åŠ¹çŽ‡)
        loss_sparsity = torch.mean(logits ** 2) * 0.01

        # 2. äºˆæ¸¬ã®ç¢ºä¿¡åº¦ (ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æœ€å°åŒ–) - å¤¢ã®ä¸­ã§è¿·ã‚ãªã„ã‚ˆã†ã«
        # Logits -> Probabilities
        probs = torch.softmax(logits, dim=-1)
        loss_entropy = - \
            torch.sum(probs * torch.log(probs + 1e-6), dim=-1).mean()

        total_loss = loss_sparsity + (loss_entropy * 0.1)

        total_loss.backward()
        self.optimizer.step()

        return total_loss.item()
