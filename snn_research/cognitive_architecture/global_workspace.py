# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/cognitive_architecture/global_workspace.py
# æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: Global Workspace (Consciousness Hub) v1.0
# ç›®çš„ãƒ»å†…å®¹:
#   ROADMAP Phase 4 "Consciousness" ã®ä¸­æ ¸ã€‚
#   è¤‡æ•°ã®ç„¡æ„è­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆSpecialist Modulesï¼‰ã‹ã‚‰ã®å…¥åŠ›ã‚’é›†ç´„ã—ã€
#   æ³¨æ„æ©Ÿæ§‹ï¼ˆAttentionï¼‰ã«ã‚ˆã£ã¦æœ€ã‚‚é‡è¦ãªæƒ…å ±ã‚’é¸æŠï¼ˆç€ç«ï¼‰ã—ã€
#   ãã‚Œã‚’è„³å…¨ä½“ã«ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆã™ã‚‹ã“ã¨ã§ã€Œæ„è­˜ã€ã‚’å½¢æˆã™ã‚‹ã€‚

import torch
import torch.nn as nn
import torch.nn.functional as F
import logging
from typing import Dict, Any, List, Optional

logger = logging.getLogger(__name__)


class GlobalWorkspace(nn.Module):
    """
    ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ»ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ï¼ˆGWTï¼‰ã€‚
    è„³å†…ã®æƒ…å ±ã®ã€Œç«¶åˆã€ã¨ã€Œæ”¾é€ã€ã‚’ç®¡ç†ã™ã‚‹ã€‚
    """

    def __init__(
        self,
        dim: int = 64,
        num_slots: int = 1,  # ä¸€åº¦ã«æ„è­˜ã§ãã‚‹äº‹è±¡ã®æ•°ï¼ˆé€šå¸¸ã¯1ã€œæ•°å€‹ï¼‰
        decay: float = 0.9  # æ„è­˜ã®æŒç¶šæ€§ï¼ˆãƒ¯ãƒ¼ã‚­ãƒ³ã‚°ãƒ¡ãƒ¢ãƒªçš„æ€§è³ªï¼‰
    ):
        super().__init__()
        self.dim = dim
        self.num_slots = num_slots
        self.decay = decay

        # æ„è­˜ã®å†…å®¹ï¼ˆGlobal Working Memoryï¼‰
        self.register_buffer("workspace_state", torch.zeros(1, dim))

        # Attention Mechanism (Selector)
        # å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰ã®å…¥åŠ›ã®ã€Œé‡è¦åº¦ã€ã‚’è©•ä¾¡ã™ã‚‹
        self.selector = nn.Sequential(
            nn.Linear(dim, dim),
            nn.Tanh(),
            nn.Linear(dim, 1)  # Importance Score
        )

        logger.info("ğŸ‘ï¸ Global Workspace (Consciousness) initialized.")

    def forward(self, inputs: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        """
        Args:
            inputs: å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰ã®å…¥åŠ›è¾æ›¸
                    {"vision": tensor_v, "pain": tensor_p, "thought": tensor_t, ...}
                    å„Tensorã¯ [Batch, Dim] ã¾ãŸã¯ [Batch, Seq, Dim]

        Returns:
            broadcast: æ„è­˜ã«é¸ã°ã‚ŒãŸæƒ…å ±ï¼ˆå…¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¸é€ä¿¡ã•ã‚Œã‚‹ï¼‰
            winner: é¸ã°ã‚ŒãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®åå‰
        """
        # 1. å…¥åŠ›ã®å‰å‡¦ç†ï¼ˆæ¬¡å…ƒçµ±ä¸€ï¼‰
        candidates = []
        names = []

        for name, tensor in inputs.items():
            # å¹³å‡ãƒ—ãƒ¼ãƒªãƒ³ã‚°ç­‰ã§ [Batch, Dim] ã«æƒãˆã‚‹
            if tensor.dim() > 2:
                flat_tensor = tensor.mean(dim=1)
            else:
                flat_tensor = tensor

            # æ¬¡å…ƒãƒã‚§ãƒƒã‚¯ï¼ˆç•°ãªã‚‹å ´åˆã¯å°„å½±ãŒå¿…è¦ã ãŒã€ã“ã“ã§ã¯åŒä¸€ã¨ä»®å®šã™ã‚‹ã‹ã‚¼ãƒ­åŸ‹ã‚ï¼‰
            if flat_tensor.shape[-1] != self.dim:
                # ç°¡æ˜“ãƒªã‚µã‚¤ã‚ºï¼ˆå®Ÿé‹ç”¨ã§ã¯å°‚ç”¨ã®AdapterãŒå¿…è¦ï¼‰
                if flat_tensor.shape[-1] < self.dim:
                    pad = self.dim - flat_tensor.shape[-1]
                    flat_tensor = F.pad(flat_tensor, (0, pad))
                else:
                    flat_tensor = flat_tensor[:, :self.dim]

            candidates.append(flat_tensor)
            names.append(name)

        if not candidates:
            return {"broadcast": self.workspace_state, "winner": None}

        # ã‚¹ã‚¿ãƒƒã‚¯: [Num_Modules, Batch, Dim]
        # Batch=1å‰æã§ç°¡æ˜“åŒ–: [Num_Modules, Dim]
        stack = torch.cat(candidates, dim=0)

        # 2. ç«¶åˆ (Competition) - Bottom-up Attention
        # å„æƒ…å ±ã®ã€ŒSalienceï¼ˆé¡•è‘—æ€§ï¼‰ã€ã‚’è¨ˆç®—
        scores = self.selector(stack).squeeze(-1)  # [Num_Modules]

        # Softmaxã§ç¢ºç‡åˆ†å¸ƒã«ã™ã‚‹ï¼ˆç¢ºç‡çš„é¸æŠã‚‚å¯èƒ½ã ãŒã€ã“ã“ã§ã¯Winner-Take-Allï¼‰
        # ãƒã‚¤ã‚ºã‚’åŠ ãˆã¦æºã‚‰ãã‚’æŒãŸã›ã‚‹ï¼ˆã‚«ã‚ªã‚¹çš„éæ­´ï¼‰
        noise = torch.randn_like(scores) * 0.1
        probs = F.softmax(scores + noise, dim=0)

        # æœ€ã‚‚å¼·ã„ä¿¡å·ã‚’é¸æŠ
        winner_idx = torch.argmax(probs).item()
        winner_name = names[winner_idx]
        winner_content = candidates[winner_idx]

        # 3. æ”¾é€ (Broadcast) - Update Global State
        # å‰å›ã®æ„è­˜çŠ¶æ…‹ã¨ãƒ–ãƒ¬ãƒ³ãƒ‰ï¼ˆæ€è€ƒã®æµã‚Œï¼‰
        new_state = (1 - self.decay) * winner_content + \
            self.decay * self.workspace_state

        # Update buffer
        # Detach to prevent infinite graph history
        self.workspace_state = new_state.detach()

        return {
            "broadcast": new_state,     # å…¨è„³ã¸é€ä¿¡ã•ã‚Œã‚‹ä¿¡å·
            "winner": winner_name,      # æ„è­˜ã«ä¸ŠãŒã£ãŸãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å
            "salience": probs.detach()  # å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®æ³¨ç›®åº¦
        }

    def get_current_thought(self):
        return self.workspace_state
