# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/cognitive_architecture/global_workspace.py
# æ—¥æœ¬èªžã‚¿ã‚¤ãƒˆãƒ«: Global Workspace (Consciousness Hub) v1.2
# ä¿®æ­£å†…å®¹: DIã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰ã® model_registry æ³¨å…¥ã«å¯¾å¿œã—ã€ãƒ†ã‚¹ãƒˆç”¨ã® get_information ã‚’è¿½åŠ ã€‚

import torch
import torch.nn as nn
import torch.nn.functional as F
import logging
from typing import Dict, Any, Optional, List, Callable

logger = logging.getLogger(__name__)


class GlobalWorkspace(nn.Module):
    """
    ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ»ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ï¼ˆGWTï¼‰ã€‚
    è„³å†…ã®æƒ…å ±ã®ã€Œç«¶åˆã€ã¨ã€Œæ”¾é€ã€ã‚’ç®¡ç†ã™ã‚‹ã€‚
    """
    workspace_state: torch.Tensor

    def __init__(
        self,
        dim: int = 64,
        num_slots: int = 1,
        decay: float = 0.9,
        model_registry: Optional[Any] = None  # Added for DI compatibility
    ):
        super().__init__()
        self.dim = dim
        self.num_slots = num_slots
        self.decay = decay
        self.model_registry = model_registry

        # æ„è­˜ã®å†…å®¹ï¼ˆGlobal Working Memoryï¼‰
        self.register_buffer("workspace_state", torch.zeros(1, dim))

        # Attention Mechanism (Selector)
        self.selector = nn.Sequential(
            nn.Linear(dim, dim),
            nn.Tanh(),
            nn.Linear(dim, 1)
        )

        # Subscribers
        self.subscribers: List[Callable[[str, Any], None]] = []
        self.current_content: Dict[str, Any] = {}

        logger.info("ðŸ‘ï¸ Global Workspace (Consciousness) initialized.")

    def subscribe(self, callback: Callable[[str, Any], None]):
        """ä»–ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒæ„è­˜ã®æ”¾é€ã‚’å—ä¿¡ã™ã‚‹ãŸã‚ã«ç™»éŒ²ã™ã‚‹"""
        self.subscribers.append(callback)

    def upload_to_workspace(self, source: str, data: Any, salience: float = 0.5):
        """ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‹ã‚‰ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã¸ã®æƒ…å ±æä¾›"""
        if salience > 0.7:
            if isinstance(data, dict) and "vector_state" in data:
                vec = data["vector_state"]
                if isinstance(vec, torch.Tensor):
                    if vec.shape[-1] == self.dim:
                        self.workspace_state = vec.detach()

            self._broadcast_to_subscribers(source, data)

    def broadcast(self, inputs: List[Any], context: Optional[str] = None) -> Any:
        """Legacy Interface"""
        tensor_inputs = {}
        for i, item in enumerate(inputs):
            if isinstance(item, torch.Tensor):
                tensor_inputs[f"input_{i}"] = item
            elif isinstance(item, dict) and "features" in item:
                tensor_inputs[f"module_{i}"] = item["features"]

        if tensor_inputs:
            result = self.forward(tensor_inputs)
            self._broadcast_to_subscribers(
                str(result["winner"]), result["broadcast"])
            return result["broadcast"]

        return self.workspace_state

    def get_current_thought(self) -> torch.Tensor:
        return self.workspace_state

    def get_information(self) -> torch.Tensor:
        """Alias for test compatibility (test_cognitive_components.py)"""
        return self.get_current_thought()

    def get_current_content(self) -> Dict[str, Any]:
        """
        [Phase 3.1] ç¾åœ¨ã®æ„è­˜å†…å®¹ï¼ˆè¾žæ›¸å½¢å¼ï¼‰ã‚’å–å¾—ã™ã‚‹ã€‚
        ExplainabilityEngineç­‰ã§ä½¿ç”¨ã€‚
        """
        return self.current_content

    def _broadcast_to_subscribers(self, source: str, content: Any):
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ä¿æŒ
        if isinstance(content, dict):
            self.current_content = content
        else:
            self.current_content = {"type": "raw",
                                    "data": content, "source": source}

        for callback in self.subscribers:
            try:
                callback(source, content)
            except Exception as e:
                logger.warning(f"Broadcast error: {e}")

    def forward(self, inputs: Dict[str, torch.Tensor]) -> Dict[str, Any]:
        candidates = []
        names = []

        for name, tensor in inputs.items():
            if tensor.dim() > 2:
                flat_tensor = tensor.mean(dim=1)
            else:
                flat_tensor = tensor

            if flat_tensor.shape[-1] != self.dim:
                if flat_tensor.shape[-1] < self.dim:
                    pad = self.dim - flat_tensor.shape[-1]
                    flat_tensor = F.pad(flat_tensor, (0, pad))
                else:
                    flat_tensor = flat_tensor[:, :self.dim]

            candidates.append(flat_tensor)
            names.append(name)

        if not candidates:
            return {"broadcast": self.workspace_state, "winner": None}

        stack = torch.cat(candidates, dim=0)
        scores = self.selector(stack).squeeze(-1)
        noise = torch.randn_like(scores) * 0.1
        probs = F.softmax(scores + noise, dim=0)

        winner_idx = int(torch.argmax(probs).item())
        winner_name = names[winner_idx]
        winner_content = candidates[winner_idx]

        new_state = (1 - self.decay) * winner_content + \
            self.decay * self.workspace_state
        self.workspace_state = new_state.detach()

        return {
            "broadcast": new_state,
            "winner": winner_name,
            "salience": probs.detach()
        }
