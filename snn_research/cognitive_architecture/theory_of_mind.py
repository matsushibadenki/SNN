# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/cognitive_architecture/theory_of_mind.py
# æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: å¿ƒã®ç†è«– (Theory of Mind) ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
# ç›®çš„ãƒ»å†…å®¹:
#   ROADMAP Phase 3.1 å¯¾å¿œã€‚
#   ä»–è€…ã®ä¿¡å¿µã€æ„å›³ã€æ¬²æ±‚ã‚’æ¨è«–ãƒ»ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚
#   å†å¸°çš„æ¨è«– (I believe that you believe...) ã‚’ã‚µãƒãƒ¼ãƒˆã€‚

from typing import Dict, Any, List, Optional
import logging
from dataclasses import dataclass, field

from .global_workspace import GlobalWorkspace
from .rag_snn import RAGSystem

logger = logging.getLogger(__name__)


@dataclass
class AgentModel:
    """ä»–è€…ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ¡ãƒ³ã‚¿ãƒ«ãƒ¢ãƒ‡ãƒ«"""
    agent_id: str
    beliefs: Dict[str, float] = field(
        default_factory=dict)  # Fact -> Confidence
    goals: List[str] = field(default_factory=list)
    last_action: Optional[str] = None
    trust_level: float = 0.5


class TheoryOfMind:
    """
    å¿ƒã®ç†è«– (ToM) ã‚¨ãƒ³ã‚¸ãƒ³ã€‚
    ä»–è€…ã®è¦–ç‚¹ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã—ã€è¡Œå‹•ã‚’äºˆæ¸¬ã™ã‚‹ã€‚
    """

    def __init__(
        self,
        workspace: GlobalWorkspace,
        rag_system: RAGSystem,
        simulation_depth: int = 2
    ):
        self.workspace = workspace
        self.rag_system = rag_system
        self.simulation_depth = simulation_depth

        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
        self.agent_models: Dict[str, AgentModel] = {}

        # ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã®è³¼èª­
        self.workspace.subscribe(self.handle_broadcast)

        logger.info(
            f"ğŸ§  Theory of Mind module initialized (Depth: {simulation_depth})")

    def get_or_create_agent(self, agent_id: str) -> AgentModel:
        if agent_id not in self.agent_models:
            self.agent_models[agent_id] = AgentModel(agent_id=agent_id)
            logger.info(f"ğŸ‘¤ New agent model created: {agent_id}")
        return self.agent_models[agent_id]

    def update_agent_belief(self, agent_id: str, fact: str, confidence: float):
        """ä»–è€…ã®ä¿¡å¿µã‚’æ›´æ–°ã™ã‚‹"""
        agent = self.get_or_create_agent(agent_id)
        agent.beliefs[fact] = confidence
        logger.debug(
            f"ğŸ§  Updated belief for {agent_id}: {fact} (conf={confidence:.2f})")

    def infer_intent(self, agent_id: str, action: str, context: str) -> str:
        """
        ä»–è€…ã®è¡Œå‹•ã‹ã‚‰æ„å›³ã‚’æ¨è«–ã™ã‚‹ã€‚
        ç°¡æ˜“çš„ãªå®Ÿè£…: ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã‚´ãƒ¼ãƒ«ã‚’æ¨æ¸¬ã€‚
        """
        agent = self.get_or_create_agent(agent_id)
        agent.last_action = action

        # æœ¬æ¥ã¯LLMã‚„å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§æ¨è«–ã™ã‚‹ç®‡æ‰€
        # ã“ã“ã§ã¯ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ç°¡æ˜“æ¨è«–
        intent = "unknown"
        if "asking" in action or "question" in action:
            intent = "information_seeking"
        elif "attacking" in action or "threat" in action:
            intent = "hostile"
        elif "helping" in action or "sharing" in action:
            intent = "cooperative"

        logger.info(
            f"ğŸ¤” Inferred intent of {agent_id} for action '{action}': {intent}")

        # æ„å›³ã‚’ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã«æŠ•ç¨¿
        self.workspace.upload_to_workspace(
            source="theory_of_mind",
            data={
                "type": "intent_inference",
                "target_agent": agent_id,
                "action": action,
                "inferred_intent": intent
            },
            salience=0.7
        )
        return intent

    def simulate_other(self, agent_id: str, context: str) -> str:
        """
        [Simulation] ä»–è€…ã®è¦–ç‚¹ã«ç«‹ã£ã¦ã€ãã®åå¿œã‚’äºˆæ¸¬ã™ã‚‹ã€‚
        """
        agent = self.get_or_create_agent(agent_id)

        # ä»–è€…ã®ä¿¡å¿µã«åŸºã¥ã„ãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ
        simulated_context = f"Context: {context}\n"
        simulated_context += f"Agent {agent_id} believes:\n"
        for fact, conf in agent.beliefs.items():
            if conf > 0.5:
                simulated_context += f"- {fact}\n"

        # æœ¬æ¥ã¯ã“ã“ã§LLMç­‰ã‚’å‘¼ã³å‡ºã—ã€simulated_contextã«å¯¾ã™ã‚‹åå¿œã‚’ç”Ÿæˆ
        # ç°¡æ˜“ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        predicted_reaction = f"Agent {agent_id} acts based on beliefs: {list(agent.beliefs.keys())}"

        logger.info(
            f"ğŸ­ Simulated perspective of {agent_id}: {predicted_reaction}")
        return predicted_reaction

    def handle_broadcast(self, source: str, data: Any):
        """
        æ„è­˜ã«ä¸Šã£ãŸæƒ…å ±ã‹ã‚‰ã€ç¤¾ä¼šçš„ã‚·ã‚°ãƒŠãƒ«ã‚’æ¤œå‡ºã™ã‚‹ã€‚
        """
        if not isinstance(data, dict):
            return

        # ä¼šè©±ã‚„ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã®æ¤œå‡º
        if data.get("type") == "interaction" or source == "auditory_cortex":
            agent_id = data.get("agent_id", "unknown_user")
            content = data.get("content", "")

            # ä¿¡å¿µã®æ›´æ–° (ç™ºè©±å†…å®¹ã¯ç›¸æ‰‹ãŒãã†ä¿¡ã˜ã¦ã„ã‚‹ã¨ä»®å®š)
            if content:
                self.update_agent_belief(agent_id, f"said: {content}", 0.8)
                self.infer_intent(agent_id, content, "conversation")
