# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/hybrid/multimodal_projector.py
# æ—¥æœ¬èªžã‚¿ã‚¤ãƒˆãƒ«: Spiking Multimodal Projector (Vision-Language Bridge) v1.2
# ç›®çš„ãƒ»å†…å®¹:
#   ROADMAP Phase 2 "Multi-modal Integration" å¯¾å¿œã€‚
#   æ—¢å­˜ã®UnifiedSensoryProjectorå‘¼ã³å‡ºã—ï¼ˆmodality_configs, language_dimç­‰ï¼‰ã«å¯¾å¿œã—ã€
#   ä»»æ„ã®æ•°ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’å‹•çš„ã«æ‰±ãˆã‚‹ã‚ˆã†ã«æ‹¡å¼µã€‚

import torch
import torch.nn as nn
import torch.nn.functional as F
import logging
from typing import Optional, Dict, Any, Union
import numpy as np

logger = logging.getLogger(__name__)

# BitNetå¯¾å¿œ: ã‚¤ãƒ³ãƒãƒ¼ãƒˆã§ããªã‘ã‚Œã°é€šå¸¸ã®Linearã‚’ä½¿ç”¨
try:
    from snn_research.core.layers.bit_spike_layer import BitSpikeLinear
except ImportError:
    BitSpikeLinear = nn.Linear  # type: ignore

# DSA (Dynamic Sparse Attention) ã®åˆ©ç”¨
try:
    from snn_research.core.layers.dsa import DSALayer
except ImportError:
    DSALayer = None  # type: ignore


class CrossModalAttentionBlock(nn.Module):
    """
    ç•°ãªã‚‹ãƒ¢ãƒ€ãƒªãƒ†ã‚£é–“ã§ã®æƒ…å ±ã®ã‚„ã‚Šå–ã‚Šã‚’è¡Œã†ãƒ–ãƒ­ãƒƒã‚¯ã€‚
    """

    def __init__(
        self,
        d_model: int,
        num_heads: int,
        dropout: float = 0.1,
        use_bitnet: bool = True
    ):
        super().__init__()
        self.d_model = d_model

        linear_cls = BitSpikeLinear if use_bitnet else nn.Linear

        self.q_proj = linear_cls(d_model, d_model)
        self.k_proj = linear_cls(d_model, d_model)
        self.v_proj = linear_cls(d_model, d_model)
        self.out_proj = linear_cls(d_model, d_model)

        self.norm = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        self.scale = self.head_dim ** -0.5

    def forward(self, x: torch.Tensor, context: torch.Tensor) -> torch.Tensor:
        B, T, C = x.shape
        _, T_ctx, _ = context.shape

        residual = x
        x = self.norm(x)

        q = self.q_proj(x).view(B, T, self.num_heads,
                                self.head_dim).transpose(1, 2)
        k = self.k_proj(context).view(B, T_ctx, self.num_heads,
                                      self.head_dim).transpose(1, 2)
        v = self.v_proj(context).view(B, T_ctx, self.num_heads,
                                      self.head_dim).transpose(1, 2)

        attn_scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale
        attn_probs = F.softmax(attn_scores, dim=-1)
        attn_probs = self.dropout(attn_probs)

        out = torch.matmul(attn_probs, v)
        out = out.transpose(1, 2).contiguous().view(B, T, C)

        return residual + self.out_proj(out)


class MultimodalProjector(nn.Module):
    """
    è¤‡æ•°ã®æ„Ÿè¦šå…¥åŠ›ï¼ˆVision, Audio, Tactileç­‰ï¼‰ã¨è¨€èªžè¡¨ç¾ã‚’çµ±åˆã™ã‚‹ãŸã‚ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚¿ãƒ¼ã€‚

    Arguments:
        vision_dim (int, optional): ãƒ¬ã‚¬ã‚·ãƒ¼å¼•æ•° (Visionå°‚ç”¨)
        text_dim (int, optional): ãƒ¬ã‚¬ã‚·ãƒ¼å¼•æ•° (Textå°‚ç”¨)
        language_dim (int, optional): Text/Languageå…±é€šåŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ (Brain v4äº’æ›)
        modality_configs (Dict[str, int], optional): å„ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®åå‰ã¨å…¥åŠ›æ¬¡å…ƒã®ãƒžãƒƒãƒ—
        embed_dim (int): å…±é€šæ½œåœ¨ç©ºé–“ã®æ¬¡å…ƒæ•°
    """

    def __init__(
        self,
        vision_dim: Optional[int] = None,
        text_dim: Optional[int] = None,
        embed_dim: int = 512,
        use_bitnet: bool = True,
        # Legacy / Universal compatibility args
        language_dim: Optional[int] = None,
        modality_configs: Optional[Dict[str, int]] = None
    ):
        super().__init__()

        # å…±é€šæ¬¡å…ƒã®æ±ºå®š (embed_dimå„ªå…ˆ, æ¬¡ã«language_dim)
        self.embed_dim = embed_dim if language_dim is None else language_dim
        linear_cls = BitSpikeLinear if use_bitnet else nn.Linear

        self.projections = nn.ModuleDict()

        # 1. ãƒ¬ã‚¬ã‚·ãƒ¼è¨­å®š (Vision / Text)
        if vision_dim is not None:
            self.projections['vision'] = self._build_proj(
                vision_dim, self.embed_dim, linear_cls)

        if text_dim is not None:
            self.projections['text'] = self._build_proj(
                text_dim, self.embed_dim, linear_cls)

        # 2. æ±Žç”¨è¨­å®š (Modality Configs)
        # Brain v4: {'vision': 784, 'tactile': 64, ...}
        if modality_configs is not None:
            for mod_name, input_dim in modality_configs.items():
                # æ—¢ã«ç™»éŒ²æ¸ˆã¿ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ã€ã¾ãŸã¯ä¸Šæ›¸ã
                self.projections[mod_name] = self._build_proj(
                    input_dim, self.embed_dim, linear_cls)

        # 3. Fusion Layers (Cross Attention)
        # å…¨ã¦ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£æƒ…å ±ã‚’çµ±åˆã™ã‚‹ãŸã‚ã®Attention
        # ã“ã“ã§ã¯ã‚·ãƒ³ãƒ—ãƒ«ã« Self-Attention ã¾ãŸã¯ Cross-Attention ã‚’åˆ©ç”¨
        self.fusion_attn = CrossModalAttentionBlock(
            d_model=self.embed_dim,
            num_heads=8,
            use_bitnet=use_bitnet
        )

        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))
        logger.info(
            f"ðŸŒ‰ Multimodal Projector initialized. Embed: {self.embed_dim}, Modalities: {list(self.projections.keys())}")

    def _build_proj(self, in_dim: int, out_dim: int, linear_cls: Any) -> nn.Sequential:
        return nn.Sequential(
            linear_cls(in_dim, out_dim),
            nn.LayerNorm(out_dim),
            nn.GELU(),
            linear_cls(out_dim, out_dim)
        )

    def forward(self,
                inputs: Union[Dict[str, torch.Tensor], torch.Tensor],
                text_features: Optional[torch.Tensor] = None) -> Union[Dict[str, torch.Tensor], torch.Tensor]:
        """
        Flexible forward method.

        Mode A: forward(vision_features, text_features) -> Compatible with old API
        Mode B: forward({'vision': ..., 'tactile': ...}) -> Compatible with Brain v4 (returns fused context)
        """

        # Mode A: Legacy (Vision, Text)
        if isinstance(inputs, torch.Tensor) and text_features is not None:
            vision_in = inputs
            v_emb = self.projections['vision'](vision_in)
            t_emb = self.projections['text'](text_features)

            # Simple Fusion
            fused = self.fusion_attn(v_emb, t_emb)

            # Pooling for Loss
            v_pool = v_emb.mean(dim=1)
            t_pool = t_emb.mean(dim=1)
            v_pool = v_pool / v_pool.norm(dim=-1, keepdim=True)
            t_pool = t_pool / t_pool.norm(dim=-1, keepdim=True)

            return {
                "vision_projected": v_emb,
                "text_projected": t_emb,
                "vision_pooled": v_pool,
                "text_pooled": t_pool,
                "fused_representation": fused
            }

        # Mode B: Universal (Dict input) -> Returns Fused Context Tensor [B, T_total, D]
        elif isinstance(inputs, dict):
            projected_features = []

            for mod_name, tensor in inputs.items():
                if mod_name in self.projections:
                    # [B, T, In_Dim] -> [B, T, Embed_Dim]
                    proj = self.projections[mod_name](tensor)
                    projected_features.append(proj)
                else:
                    logger.warning(
                        f"Unknown modality '{mod_name}' passed to Projector. Skipping.")

            if not projected_features:
                # Fallback for empty input (prevent crash)
                device = list(inputs.values())[
                    0].device if inputs else torch.device('cpu')
                return torch.zeros(1, 1, self.embed_dim, device=device)

            # Concatenate along time dimension [B, T_v + T_a + ..., D]
            concat_features = torch.cat(projected_features, dim=1)

            # Apply Self-Attention over all modalities (Fusion)
            # context is same as input for self-attention
            fused_context = self.fusion_attn(concat_features, concat_features)

            return fused_context

        else:
            raise ValueError("Invalid input format for MultimodalProjector")

    def compute_alignment_loss(self, output: Dict[str, torch.Tensor]) -> torch.Tensor:
        v_feat = output["vision_pooled"]
        t_feat = output["text_pooled"]

        logit_scale = self.logit_scale.exp()
        logits_per_image = logit_scale * v_feat @ t_feat.t()
        logits_per_text = logits_per_image.t()

        batch_size = v_feat.shape[0]
        labels = torch.arange(batch_size, device=v_feat.device)

        loss_i = F.cross_entropy(logits_per_image, labels)
        loss_t = F.cross_entropy(logits_per_text, labels)

        return (loss_i + loss_t) / 2


# Backward compatibility alias
UnifiedSensoryProjector = MultimodalProjector
