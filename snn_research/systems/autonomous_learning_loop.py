# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/systems/autonomous_learning_loop.py
# æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: Autonomous Learning Loop v2.0 (Phase 2 Integration)
# ç›®çš„ãƒ»å†…å®¹:
#   ROADMAP Phase 2 "Autonomy" ã®ä¸­æ ¸å®Ÿè£…ã€‚
#   è¦šé†’(Wake)ã¨ç¡çœ (Sleep)ã®ã‚µã‚¤ã‚¯ãƒ«ã‚’ç®¡ç†ã—ã€å†…ç™ºçš„å‹•æ©Ÿã«åŸºã¥ãè‡ªå¾‹å­¦ç¿’ã‚’è¡Œã†ã€‚
#   EmbodiedVLMAgent, IntrinsicMotivationSystem, SleepConsolidator ã‚’çµ±åˆã€‚

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from typing import Dict, Any, Tuple, Optional
import logging

from snn_research.systems.embodied_vlm_agent import EmbodiedVLMAgent
from snn_research.cognitive_architecture.intrinsic_motivation import IntrinsicMotivationSystem
from snn_research.cognitive_architecture.sleep_consolidation import SleepConsolidator

logger = logging.getLogger(__name__)


class AutonomousLearningLoop:
    """
    è‡ªå¾‹å­¦ç¿’ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ  (v2.0)

    ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’æŒã¤:
    1. Active Inference: äºˆæ¸¬èª¤å·®ã‚’æœ€å°åŒ–ã™ã‚‹è¡Œå‹•ã€ã¾ãŸã¯å¥½å¥‡å¿ƒã‚’æœ€å¤§åŒ–ã™ã‚‹è¡Œå‹•ã®é¸æŠã€‚
    2. Intrinsic Reward: å¤–éƒ¨å ±é…¬ãŒãªã„ç’°å¢ƒã§ã‚‚ã€Œé©šãã€ã‚’å ±é…¬ã¨ã—ã¦å­¦ç¿’ã€‚
    3. Homeostasis: ã‚¨ãƒãƒ«ã‚®ãƒ¼æ¶ˆè²»ã¨ç–²åŠ´ã‚’ç®¡ç†ã—ã€é©åˆ‡ãªã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§ç¡çœ ã‚’ãƒˆãƒªã‚¬ãƒ¼ã€‚
    4. Sleep Consolidation: ç¡çœ ä¸­ã«çŸ­æœŸè¨˜æ†¶ã‚’ãƒªãƒ—ãƒ¬ã‚¤ã—ã€é•·æœŸè¨˜æ†¶ã¸å›ºå®šã€‚
    """

    def __init__(
        self,
        agent: EmbodiedVLMAgent,
        optimizer: torch.optim.Optimizer,
        device: str = "cpu",
        energy_capacity: float = 1000.0,
        fatigue_threshold: float = 800.0
    ):
        self.device = device
        self.agent = agent.to(device)
        self.optimizer = optimizer

        # Phase 2 Components
        self.motivator = IntrinsicMotivationSystem().to(device)
        self.sleep_system = SleepConsolidator(agent, optimizer, device=device)

        # World Predictor (äºˆæ¸¬ç¬¦å·åŒ–ç”¨ãƒ˜ãƒƒãƒ‰)
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ½œåœ¨çŠ¶æ…‹ã¨è¡Œå‹•ã‹ã‚‰ã€æ¬¡ã®æ½œåœ¨çŠ¶æ…‹ã‚’äºˆæ¸¬ã™ã‚‹
        # ã“ã‚Œã«ã‚ˆã‚Šã€Œäºˆæ¸¬èª¤å·®(Surprise)ã€ã‚’è¨ˆç®—å¯èƒ½ã«ã™ã‚‹
        fusion_dim = getattr(agent, "fusion_dim", 512)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¾ãŸã¯agentã‹ã‚‰å–å¾—
        action_dim = getattr(agent, "action_dim", 64)

        self.world_predictor = nn.Sequential(
            nn.Linear(fusion_dim + action_dim, 512),
            nn.GELU(),
            nn.Linear(512, fusion_dim)
        ).to(device)

        self.predictor_optimizer = optim.AdamW(
            self.world_predictor.parameters(), lr=1e-3)

        # æ’å¸¸æ€§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (Homeostasis)
        self.energy = energy_capacity
        self.max_energy = energy_capacity
        self.fatigue = 0.0
        self.fatigue_threshold = fatigue_threshold

        logger.info("ğŸ”„ Autonomous Learning Loop v2.0 (Phase 2) initialized.")

    def step(self,
             current_image: torch.Tensor,
             current_text: torch.Tensor,
             next_image: Optional[torch.Tensor] = None
             ) -> Dict[str, Any]:
        """
        ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã®1ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®Ÿè¡Œã™ã‚‹ã€‚

        Returns:
            Dict: å®Ÿè¡Œçµæœã¨ç¾åœ¨ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
        """
        # 1. çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯ (ç¡çœ ãŒå¿…è¦ã‹ï¼Ÿ)
        if self._should_sleep():
            return self._perform_sleep_cycle()

        # è¦šé†’ãƒ¢ãƒ¼ãƒ‰ (Wake Phase)
        self.agent.train()
        self.world_predictor.train()

        # 2. Agent Perception & Action (SNN Forward)
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ç¾åœ¨ã®è¦³æ¸¬ã‹ã‚‰è¡Œå‹•ã‚’æ±ºå®š
        agent_out = self.agent(current_image, current_text)

        z_t = agent_out.get("fused_context")  # ç¾åœ¨ã®æ½œåœ¨è¡¨ç¾ [B, D]
        action = agent_out.get("action_pred")  # è¡Œå‹•ãƒ™ã‚¯ãƒˆãƒ« [B, A]

        # 3. World Prediction (Next State Prediction)
        # ã€Œè‡ªåˆ†ã®è¡Œå‹•ã«ã‚ˆã£ã¦ä¸–ç•ŒãŒã©ã†å¤‰ã‚ã‚‹ã‹ã€ã‚’äºˆæ¸¬
        # z_{t+1}_pred = P(z_t, action)
        if z_t is not None and action is not None:
            # æ¬¡å…ƒèª¿æ•´ (Batchæ¬¡å…ƒã®ã¿ã«ã™ã‚‹)
            if z_t.dim() > 2:
                z_t = z_t.mean(dim=1)

            pred_input = torch.cat([z_t, action], dim=-1)
            z_next_pred = self.world_predictor(pred_input)
        else:
            # åˆå›ãªã©ãƒ‡ãƒ¼ã‚¿ä¸è¶³æ™‚
            z_next_pred = torch.zeros(1, 512).to(self.device)

        # 4. Reality Check (Compute Surprise)
        # æ¬¡ã®æ™‚åˆ»ã®ç”»åƒãŒå¾—ã‚‰ã‚Œã¦ã„ã‚‹å ´åˆï¼ˆå­¦ç¿’æ™‚ï¼‰ã€äºˆæ¸¬èª¤å·®ã‚’è¨ˆç®—
        surprise = 0.0
        prediction_loss = torch.tensor(0.0).to(self.device)

        if next_image is not None:
            with torch.no_grad():
                # VLMã‚’ä½¿ã£ã¦ã€Œå®Ÿéš›ã®ã€æ¬¡ã®æ½œåœ¨è¡¨ç¾ã‚’å–å¾—
                next_out = self.agent.vlm(next_image, current_text)
                z_next_actual = next_out.get("fused_representation")

                if z_next_actual is not None:
                    if z_next_actual.dim() > 2:
                        z_next_actual = z_next_actual.mean(dim=1)

                    # äºˆæ¸¬èª¤å·® (MSE) -> ã“ã‚ŒãŒã€Œé©šãã€ã¨ãªã‚‹
                    prediction_loss = F.mse_loss(z_next_pred, z_next_actual)
                    surprise = torch.clamp(prediction_loss, 0.0, 1.0).item()

        # 5. Intrinsic Motivation & Reward Calculation
        # å†…ç™ºçš„å‹•æ©Ÿã‚·ã‚¹ãƒ†ãƒ ã‚’æ›´æ–°ã—ã€å ±é…¬ã‚’è¨ˆç®—
        motivation_state = self.motivator.process(
            input_payload=z_t, prediction_error=surprise)
        intrinsic_reward = self.motivator.calculate_intrinsic_reward(
            surprise=surprise)

        # 6. Memory Storage (Hippocampus)
        # çµŒé¨“ã‚’ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¨ã—ã¦ä¿å­˜ (ç¡çœ æ™‚ã®å­¦ç¿’ç”¨)
        # å ±é…¬ãŒé«˜ã‹ã£ãŸ(=é©šããŒå¤§ãã‹ã£ãŸ)çµŒé¨“ã»ã©é‡è¦
        self.sleep_system.store_experience(
            current_image, current_text, reward=intrinsic_reward)

        # 7. Online Learning (Backprop)
        # äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ›´æ–°
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ã€Œæ•´åˆæ€§(Alignment)ã€ã¨ã€Œäºˆæ¸¬èƒ½åŠ›ã€ã‚’é«˜ã‚ã‚‹ã‚ˆã†ã«å­¦ç¿’
        total_loss = prediction_loss + agent_out.get("alignment_loss", 0) * 0.1

        self.optimizer.zero_grad()
        self.predictor_optimizer.zero_grad()

        total_loss.backward()

        self.optimizer.step()
        self.predictor_optimizer.step()

        # 8. Homeostasis Update
        # ã‚¨ãƒãƒ«ã‚®ãƒ¼æ¶ˆè²»ã¨ç–²åŠ´ã®è“„ç©
        self.energy -= 1.0  # æ´»å‹•ã‚³ã‚¹ãƒˆ
        self.fatigue += (0.5 + surprise * 2.0)  # é©šããŒå¤§ãã„ã»ã©ç–²ã‚Œã‚‹

        # å‹•æ©ŸçŠ¶æ…‹ã®æ›´æ–° (UIè¡¨ç¤ºç”¨)
        drives = self.motivator.update_drives(
            surprise=surprise,
            energy_level=self.energy,
            fatigue_level=self.fatigue,
            task_success=True  # ã“ã“ã§ã¯å¸¸ã«ç”Ÿå­˜ä¸­
        )

        return {
            "mode": "wake",
            "step_loss": total_loss.item(),
            "surprise": surprise,
            "intrinsic_reward": intrinsic_reward,
            "energy": self.energy,
            "fatigue": self.fatigue,
            "drives": drives
        }

    def _should_sleep(self) -> bool:
        """ç¡çœ ã«å…¥ã‚‹ã¹ãã‹åˆ¤å®š"""
        if self.fatigue >= self.fatigue_threshold:
            return True
        if self.energy <= 0:
            return True
        return False

    def _perform_sleep_cycle(self) -> Dict[str, Any]:
        """ç¡çœ ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè¡Œ"""
        # ç¡çœ å®Ÿè¡Œ (Sleep Consolidation)
        sleep_stats = self.sleep_system.sleep(cycles=5)

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å›å¾©
        self.fatigue = 0.0
        self.energy = self.max_energy * 0.9  # å®Œå…¨å›å¾©ã§ã¯ãªã„(ä»£è¬ã‚³ã‚¹ãƒˆ)

        # å¤¢ãƒ­ã‚°
        logger.info(
            f"ğŸ’¤ Slept. Fatigue reset. Loss: {sleep_stats.get('sleep_loss', 0):.4f}")

        return {
            "mode": "sleep",
            "sleep_loss": sleep_stats.get("sleep_loss", 0.0),
            "energy": self.energy,
            "fatigue": self.fatigue,
            "info": "Memory Consolidated"
        }
