# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/systems/autonomous_learning_loop.py
# æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: Autonomous Learning Loop (Self-Supervised System) v1.1
# ç›®çš„ãƒ»å†…å®¹:
#   EmbodiedVLMAgent (Actor) ã¨ IntrinsicMotivator (Critic/Teacher) ã‚’çµåˆã€‚
#   [Fix] NameError: name 'F' is not defined ã‚’è§£æ¶ˆã™ã‚‹ãŸã‚ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆè¿½åŠ ã€‚

import torch
import torch.nn as nn
import torch.nn.functional as F  # Added
import torch.optim as optim
from typing import Dict, Any, List, Optional
import logging

from snn_research.systems.embodied_vlm_agent import EmbodiedVLMAgent
from snn_research.adaptive.intrinsic_motivator import IntrinsicMotivator

logger = logging.getLogger(__name__)

class AutonomousLearningLoop:
    """
    è‡ªå¾‹å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã€‚
    Observation -> Action -> Observation' -> Reward(Internal) -> Update
    ã®ã‚µã‚¤ã‚¯ãƒ«ã‚’å›ã™ã€‚
    """
    
    def __init__(
        self,
        agent: EmbodiedVLMAgent,
        optimizer: torch.optim.Optimizer,
        device: str = "cpu"
    ):
        self.agent = agent.to(device)
        self.motivator = IntrinsicMotivator().to(device)
        self.optimizer = optimizer
        self.device = device
        
        # äºˆæ¸¬ãƒ˜ãƒƒãƒ‰ (Latent Predictor)
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã€Œç¾åœ¨ã®çŠ¶æ…‹+è¡Œå‹•ã€ã‹ã‚‰ã€Œæ¬¡ã®çŠ¶æ…‹ã€ã‚’äºˆæ¸¬ã™ã‚‹ç°¡æ˜“ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
        fusion_dim = self.agent.motor_decoder.input_dim
        action_dim = self.agent.motor_decoder.action_dim
        
        self.world_predictor = nn.Sequential(
            nn.Linear(fusion_dim + action_dim, 256),
            nn.GELU(),
            nn.Linear(256, fusion_dim) # Predict next latent
        ).to(device)
        
        self.predictor_optimizer = optim.AdamW(self.world_predictor.parameters(), lr=1e-3)
        
        logger.info("ğŸ”„ Autonomous Learning Loop initialized.")

    def step(self, 
             current_image: torch.Tensor, 
             current_text: torch.Tensor,
             next_image: torch.Tensor # ç’°å¢ƒã‹ã‚‰ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
             ) -> Dict[str, float]:
        """
        1ã‚¹ãƒ†ãƒƒãƒ—ã®è‡ªå¾‹å­¦ç¿’ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè¡Œã€‚
        """
        self.agent.train()
        self.world_predictor.train()
        
        # 1. Agent Perception & Action
        # Forward pass to get current latent and action
        agent_out = self.agent(current_image, current_text)
        
        z_t = agent_out["fused_context"] # [B, T, D] or [B, 1, D]
        action = agent_out["action_pred"]
        
        # ç°¡æ˜“åŒ–: æ™‚é–“æ–¹å‘ã®å¹³å‡ã¾ãŸã¯æœ€å¾Œã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’ä½¿ç”¨
        if z_t.dim() == 3:
            z_t_flat = z_t.mean(dim=1)
        else:
            z_t_flat = z_t
            
        # 2. World Prediction (What happens next?)
        # Predict z_{t+1} from z_t and action
        pred_input = torch.cat([z_t_flat, action], dim=-1)
        z_next_pred = self.world_predictor(pred_input)
        
        # 3. Observe Reality (Encode next image to get z_{t+1})
        # æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã®ã€Œæœªæ¥ã®è‡ªåˆ†ã€
        with torch.no_grad():
            # VLMã‚’ä½¿ã£ã¦æ¬¡ã®ç”»åƒã®æ½œåœ¨è¡¨ç¾ã‚’å–å¾— (Textã¯åŒã˜ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä»®å®š)
            next_out = self.agent.vlm(next_image, current_text)
            z_next_actual = next_out["fused_representation"]
            
            # Fallback if None
            if z_next_actual is None:
                if "vision_latents" in next_out and len(next_out["vision_latents"]) > 0:
                    z_next_actual = next_out["vision_latents"].unsqueeze(1)
                else:
                    z_next_actual = torch.zeros_like(z_t) # dummy

            if z_next_actual.dim() == 3:
                z_next_actual_flat = z_next_actual.mean(dim=1)
            else:
                z_next_actual_flat = z_next_actual

        # 4. Compute Intrinsic Reward (Surprise)
        reward = self.motivator.compute_reward(z_next_pred, z_next_actual_flat)
        
        # 5. Losses
        # World Model Loss: äºˆæ¸¬ã‚’ç¾å®Ÿã«è¿‘ã¥ã‘ã‚‹
        wm_loss = F.mse_loss(z_next_pred, z_next_actual_flat)
        
        # Agent Loss: 
        # æœ¬æ¥ã¯å ±é…¬æœ€å¤§åŒ–ã ãŒã€ãƒ‡ãƒ¢ç”¨ã¨ã—ã¦wm_lossã‚’é€†ä¼æ’­ã•ã›ã‚‹
        # ã€Œäºˆæ¸¬ã—ã‚„ã™ã„æ½œåœ¨è¡¨ç¾ã‚’ç²å¾—ã™ã‚‹ã€ã‚ˆã†ã«Encoderã‚’æ›´æ–°ã™ã‚‹ã€‚
        total_loss = wm_loss + agent_out["alignment_loss"] * 0.1
        
        # 6. Update
        self.optimizer.zero_grad()
        self.predictor_optimizer.zero_grad()
        
        total_loss.backward()
        
        self.optimizer.step()
        self.predictor_optimizer.step()
        
        return {
            "loss": total_loss.item(),
            "prediction_error": wm_loss.item(),
            "intrinsic_reward": reward.item(),
            "baseline": self.motivator.running_error_mean.item()
        }