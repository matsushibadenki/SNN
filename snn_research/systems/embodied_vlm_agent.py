# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/systems/embodied_vlm_agent.py
# æ—¥æœ¬èªžã‚¿ã‚¤ãƒˆãƒ«: Embodied VLM Agent (Robust Version)
# ç›®çš„ãƒ»å†…å®¹:
#   ROADMAP Phase 2 "Multimodal Integration" ã®å®Œæˆå½¢ã€‚
#   [Fix] VLMã‹ã‚‰ã®å‡ºåŠ›ã‚­ãƒ¼æ¬ è½ã«å¯¾ã™ã‚‹å …ç‰¢ãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚’è¿½åŠ ã€‚

import torch
import torch.nn as nn
from typing import Dict, Any

from snn_research.models.transformer.spiking_vlm import SpikingVLM
from snn_research.models.motor.spiking_motor_decoder import SpikingMotorDecoder
import logging

logger = logging.getLogger(__name__)

class EmbodiedVLMAgent(nn.Module):
    """
    èº«ä½“æ€§ã‚’æŒã¤è¦–è¦šè¨€èªžã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚
    """
    
    def __init__(
        self,
        vlm_model: SpikingVLM,
        motor_config: Dict[str, Any]
    ):
        super().__init__()
        self.vlm = vlm_model
        
        # VLMã®èžåˆè¡¨ç¾ã®æ¬¡å…ƒã‚’å–å¾— (Projectorã®å‡ºåŠ›æ¬¡å…ƒ)
        fusion_dim = self.vlm.projector.embed_dim
        
        self.motor_decoder = SpikingMotorDecoder(
            input_dim=fusion_dim,
            action_dim=motor_config.get("action_dim", 6),
            action_type=motor_config.get("action_type", "continuous"),
            hidden_dim=motor_config.get("hidden_dim", 128)
        )
        
        logger.info("ðŸ¤– Embodied VLM Agent initialized (Vision+Language+Motor).")

    def forward(
        self, 
        image_input: torch.Tensor, 
        text_input: torch.Tensor
    ) -> Dict[str, Any]:
        """
        å­¦ç¿’ãƒ»æŽ¨è«–ãƒ‘ã‚¹
        """
        # 1. Perception & Cognition (VLM Forward)
        vlm_out = self.vlm(image_input, text_input)
        
        # 2. Extract Fused Representation
        fused_context = vlm_out.get("fused_representation")
        
        if fused_context is None:
             # Fallback: æ¬¡å…ƒä¸æ•´åˆã‚’é˜²ããŸã‚ vision_latents ã‚’ä½¿ç”¨
             # vision_latents: [B, D] -> unsqueeze -> [B, 1, D]
             if "vision_latents" in vlm_out and len(vlm_out["vision_latents"]) > 0:
                 fused_context = vlm_out["vision_latents"].unsqueeze(1)
             else:
                 # æœ€æ‚ªã‚±ãƒ¼ã‚¹: ã‚¼ãƒ­åŸ‹ã‚
                 device = vlm_out["logits"].device
                 B = vlm_out["logits"].shape[0]
                 fused_context = torch.zeros(B, 1, self.motor_decoder.input_dim, device=device)

        # 3. Action Generation (Motor Decoder)
        action_output = self.motor_decoder(fused_context)
        
        return {
            "logits": vlm_out["logits"],         # For Language Loss
            "action_pred": action_output,        # For Action Loss
            "alignment_loss": vlm_out["alignment_loss"],
            "fused_context": fused_context
        }

    @torch.no_grad()
    def act_and_speak(
        self, 
        image_input: torch.Tensor, 
        prompt_input: torch.Tensor,
        max_len: int = 20
    ) -> Dict[str, Any]:
        """
        æŽ¨è«–ãƒ¢ãƒ¼ãƒ‰: ç”»åƒã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ã€è¡Œå‹•ã¨ç™ºè©±ã‚’ç”Ÿæˆã™ã‚‹ã€‚
        """
        self.eval()
        
        # 1. Generate Caption / Response
        generated_ids = self.vlm.generate_caption(image_input, max_len=max_len)
        
        # 2. Generate Action based on context
        vlm_out = self.vlm(image_input, prompt_input)
        fused_context = vlm_out.get("fused_representation")
        
        # Fallback Logic (Same as forward)
        if fused_context is None:
             if "vision_latents" in vlm_out and len(vlm_out["vision_latents"]) > 0:
                 fused_context = vlm_out["vision_latents"].unsqueeze(1)
             else:
                 device = image_input.device
                 B = image_input.shape[0]
                 fused_context = torch.zeros(B, 1, self.motor_decoder.input_dim, device=device)
        
        action = self.motor_decoder(fused_context)
        
        return {
            "generated_tokens": generated_ids,
            "action": action
        }