# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/hardware/event_driven_simulator.py
# Title: ã‚¤ãƒ™ãƒ³ãƒˆé§†å‹•å‹SNNã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ (Trace Saturation Fix)
# Description:
#   ROADMAP Phase 6 "Hardware Native Transition" å®Ÿè£…ã€‚
#   ä¿®æ­£: STDPãƒˆãƒ¬ãƒ¼ã‚¹ã®é£½å’Œ(Saturation)ã‚’å°å…¥ã—ã€éå‰°ãªLTDã«ã‚ˆã‚‹å­¦ç¿’å´©å£Šã‚’é˜²ãã€‚

import torch
import torch.nn as nn
import heapq
import logging
import math
from typing import List, Dict, Any
from dataclasses import dataclass, field

# æ—¢å­˜ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³å®šç¾©ã‚’åˆ©ç”¨
from snn_research.core.neurons import AdaptiveLIFNeuron, IzhikevichNeuron

logger = logging.getLogger(__name__)


@dataclass(order=True)
class SpikeEvent:
    timestamp: float
    neuron_id: int = field(compare=False)
    layer_index: int = field(compare=False)
    source_index: int = field(compare=False)
    payload: float = field(compare=False, default=1.0)


class EventDrivenNeuronState:
    """
    ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³çŠ¶æ…‹ä¿æŒã‚¯ãƒ©ã‚¹ã€‚STDPç”¨ã®ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’è¿½åŠ ã€‚
    """

    def __init__(self, v_threshold: float, tau_mem: float, v_reset: float, tau_trace: float = 20.0):
        self.v = 0.0
        self.last_update_time = 0.0

        self.trace = 0.0
        self.last_spike_time = -1.0
        self.tau_trace = tau_trace

        self.v_threshold = v_threshold
        self.tau_mem = tau_mem
        self.v_reset = v_reset

    def decay_and_integrate(self, current_time: float, input_weight: float) -> None:
        """
        æ™‚é–“çµŒéã«ã‚ˆã‚‹æ¸›è¡°ã¨å…¥åŠ›ã®çµ±åˆã‚’è¡Œã†ï¼ˆç™ºç«åˆ¤å®šã¯ã—ãªã„ï¼‰ã€‚
        """
        dt = current_time - self.last_update_time

        # 1. è†œé›»ä½ã®æ¸›è¡°
        decay_factor = math.exp(-dt / self.tau_mem)
        self.v = self.v * decay_factor

        # 2. ãƒˆãƒ¬ãƒ¼ã‚¹ã®æ¸›è¡°
        trace_decay = math.exp(-dt / self.tau_trace)
        self.trace *= trace_decay

        # 3. å…¥åŠ›ã®çµ±åˆ
        self.v += input_weight

        self.last_update_time = current_time

    def check_fire_and_update_trace(self, current_time: float) -> bool:
        """
        ç™ºç«åˆ¤å®šã‚’è¡Œã„ã€ç™ºç«ã—ãŸå ´åˆã¯ãƒªã‚»ãƒƒãƒˆã¨ãƒˆãƒ¬ãƒ¼ã‚¹æ›´æ–°ã‚’è¡Œã†ã€‚
        """
        if self.v >= self.v_threshold:
            self.v = self.v_reset

            # --- ä¿®æ­£: ãƒˆãƒ¬ãƒ¼ã‚¹ã®æ›´æ–°ãƒ­ã‚¸ãƒƒã‚¯ ---
            # åŠ ç®—ã—ç¶šã‘ã‚‹ã¨ãƒãƒ¼ã‚¹ãƒˆæ™‚ã«å€¤ãŒçˆ†ç™ºã™ã‚‹ãŸã‚ã€ä¸Šé™ã‚’è¨­ã‘ã‚‹ã‹ãƒªã‚»ãƒƒãƒˆã™ã‚‹ã€‚
            # ã“ã“ã§ã¯ã€Œç™ºç«ç›´å¾Œã¯ãƒˆãƒ¬ãƒ¼ã‚¹ãŒæœ€å¤§(1.0)ã«ãªã‚‹ã€ãƒ¢ãƒ‡ãƒ«ã‚’æ¡ç”¨ã€‚
            self.trace = 1.0

            self.last_spike_time = current_time
            return True
        return False


class EventDrivenSimulator:
    """
    ã‚¤ãƒ™ãƒ³ãƒˆé§†å‹•å‹SNNã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ (with On-Chip Plasticity).
    """

    def __init__(
        self,
        model: nn.Module,
        enable_learning: bool = False,
        learning_rate: float = 0.001,
        stdp_window: float = 20.0
    ):
        self.model = model
        self.event_queue: List[SpikeEvent] = []
        self.current_time = 0.0
        self.total_ops = 0
        self.enable_learning = enable_learning
        self.learning_rate = learning_rate
        self.stdp_window = stdp_window

        self.layers: List[List[EventDrivenNeuronState]] = []
        self.weights: List[torch.Tensor] = []

        self._parse_model(model)

        logger.info(
            f"âš™ï¸ Event-Driven Simulator initialized. Learning: {self.enable_learning}")

    def _parse_model(self, model: nn.Module):
        current_weights = None
        for name, mod in model.named_modules():
            if isinstance(mod, nn.Linear):
                current_weights = mod.weight.detach().cpu()
                self.weights.append(current_weights)
            elif isinstance(mod, (AdaptiveLIFNeuron, IzhikevichNeuron)):
                if hasattr(mod, 'base_threshold'):
                    v_th = mod.base_threshold
                    if isinstance(v_th, torch.Tensor):
                        v_th = v_th.mean().item()
                else:
                    v_th = 1.0

                if hasattr(mod, 'tau_mem'):
                    tau = float(mod.tau_mem)
                elif hasattr(mod, 'log_tau_mem'):
                    tau = (torch.exp(mod.log_tau_mem) + 1.1).mean().item()
                else:
                    tau = 20.0

                v_reset = getattr(mod, 'v_reset', 0.0)

                if hasattr(mod, 'features'):
                    n_neurons = mod.features
                elif current_weights is not None:
                    n_neurons = current_weights.shape[0]
                else:
                    continue

                layer_states = [
                    EventDrivenNeuronState(
                        v_th, tau, v_reset, self.stdp_window)
                    for _ in range(n_neurons)
                ]
                self.layers.append(layer_states)

    def set_input_spikes(self, input_spikes: torch.Tensor):
        spike_indices = torch.nonzero(input_spikes)
        count = 0
        for t, n_idx in spike_indices:
            event = SpikeEvent(
                timestamp=float(t),
                neuron_id=int(n_idx),
                layer_index=-1,
                source_index=int(n_idx)
            )
            heapq.heappush(self.event_queue, event)
            count += 1
        logger.info(f"ğŸ“¥ Registered {count} input events.")

    def run(self, max_time: float = 100.0) -> Dict[str, Any]:
        logger.info(
            f"ğŸš€ Running simulation (Learning={self.enable_learning})...")
        processed_events = 0
        output_spikes_count = 0
        weight_updates = 0

        last_spike_times: Dict[int, Dict[int, float]] = {-1: {}}

        while self.event_queue:
            event = heapq.heappop(self.event_queue)
            if event.timestamp > max_time:
                break

            self.current_time = event.timestamp
            processed_events += 1

            src_layer_idx = event.layer_index
            src_neuron_idx = event.source_index

            # ç™ºç«æ™‚åˆ»ã®è¨˜éŒ²
            if src_layer_idx not in last_spike_times:
                last_spike_times[src_layer_idx] = {}
            last_spike_times[src_layer_idx][src_neuron_idx] = self.current_time

            target_layer_idx = src_layer_idx + 1
            if target_layer_idx >= len(self.layers):
                output_spikes_count += 1
                continue
            if target_layer_idx >= len(self.weights):
                continue

            W = self.weights[target_layer_idx]

            relevant_weights = W[:, src_neuron_idx]
            active_indices = torch.nonzero(
                torch.abs(relevant_weights) > 0.001).flatten()

            for tgt_neuron_idx in active_indices:
                w = relevant_weights[tgt_neuron_idx].item()
                target_neuron = self.layers[target_layer_idx][tgt_neuron_idx]

                # 1. æ¸›è¡°ã¨çµ±åˆ
                self.total_ops += 1
                target_neuron.decay_and_integrate(self.current_time, w)

                # --- LTD (Long-Term Depression) ---
                if self.enable_learning:
                    if target_neuron.trace > 0.05:
                        # LTDã®å¼·ã•ã‚’å°‘ã—æ§ãˆã‚ã«è¨­å®š
                        dw = -self.learning_rate * target_neuron.trace * 0.25
                        W[tgt_neuron_idx, src_neuron_idx] += dw
                        weight_updates += 1

                # 2. ç™ºç«åˆ¤å®š
                fired = target_neuron.check_fire_and_update_trace(
                    self.current_time)

                if fired:
                    delay = 1.0
                    new_event = SpikeEvent(
                        timestamp=self.current_time + delay,
                        neuron_id=int(tgt_neuron_idx),
                        layer_index=target_layer_idx,
                        source_index=int(tgt_neuron_idx)
                    )
                    heapq.heappush(self.event_queue, new_event)

                    # --- LTP (Long-Term Potentiation) ---
                    if self.enable_learning:
                        pre_spike_times = last_spike_times.get(
                            src_layer_idx, {})

                        for pre_idx, t_pre in pre_spike_times.items():
                            dt = self.current_time - t_pre
                            if 0 <= dt < self.stdp_window:
                                stdp_factor = math.exp(-dt / self.stdp_window)
                                dw = self.learning_rate * stdp_factor
                                W[tgt_neuron_idx, pre_idx] += dw
                                weight_updates += 1

        logger.info(f"âœ… Simulation complete. Updates: {weight_updates}")
        return {
            "processed_events": processed_events,
            "total_ops": self.total_ops,
            "output_spikes": output_spikes_count,
            "weight_updates": weight_updates
        }
