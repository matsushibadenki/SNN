# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/social/theory_of_mind.py
# æ—¥æœ¬èªžã‚¿ã‚¤ãƒˆãƒ«: Theory of Mind (ToM) Module v1.0
# ç›®çš„ãƒ»å†…å®¹:
#   ROADMAP Phase 2.4 "Social Intelligence" å¯¾å¿œã€‚
#   ä»–è€…ã®è¡Œå‹•è¦³æ¸¬ãƒ‡ãƒ¼ã‚¿ï¼ˆä½ç½®ã€è¦–ç·šã€ç™ºè©±ãªã©ï¼‰ã‹ã‚‰ã€
#   ãã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã€Œéš ã•ã‚ŒãŸæ„å›³ï¼ˆGoal/Intentï¼‰ã€ã‚’æŽ¨è«–ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚
#   é«˜é€ŸãªBitSpikeMambaã‚’ç”¨ã„ã¦ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã«ç›¸æ‰‹ã®å¿ƒã‚’èª­ã¿å–ã‚‹ã€‚

import torch
import torch.nn as nn
import logging
from typing import Dict, Any, Optional, Tuple

# é«˜é€ŸæŽ¨è«–ã®ãŸã‚Mambaã‚’åˆ©ç”¨
try:
    from snn_research.models.experimental.bit_spike_mamba import BitSpikeMamba
except ImportError:
    BitSpikeMamba = None

logger = logging.getLogger(__name__)


class TheoryOfMindEncoder(nn.Module):
    """
    å¿ƒã®ç†è«–ï¼ˆToMï¼‰ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã€‚
    ä»–è€…ã®è¡Œå‹•ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’å…¥åŠ›ã¨ã—ã€ãã®æ„å›³ï¼ˆIntent Vectorï¼‰ã‚’å‡ºåŠ›ã™ã‚‹ã€‚
    """

    def __init__(
        self,
        input_dim: int,  # è¦³æ¸¬æ¬¡å…ƒ (ä¾‹: ç›¸æ‰‹ã®åº§æ¨™ x,y + é€Ÿåº¦ vx,vy = 4)
        hidden_dim: int = 64,
        intent_dim: int = 8,  # äºˆæ¸¬ã™ã‚‹æ„å›³ã®ã‚¯ãƒ©ã‚¹æ•°ã‚„åº§æ¨™æ¬¡å…ƒ
        model_type: str = "mamba"  # 'mamba' or 'lstm'
    ):
        super().__init__()
        self.input_dim = input_dim
        self.model_type = model_type

        logger.info(
            f"ðŸ§  Initializing Theory of Mind (ToM) Engine... (Type: {model_type})")

        # 1. Sequence Modeler (Trajectory -> Latent)
        if model_type == "mamba" and BitSpikeMamba is not None:
            # æ™‚ç³»åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ã«Mambaã‚’ä½¿ç”¨
            self.core = BitSpikeMamba(
                vocab_size=0,  # Continuous input
                d_model=hidden_dim,
                d_state=16,
                d_conv=4,
                expand=2,
                num_layers=2,
                time_steps=16,  # ãƒ’ã‚¹ãƒˆãƒªãƒ¼é•·
                neuron_config={"type": "lif"}
            )
            self.input_proj = nn.Linear(input_dim, hidden_dim)

        else:
            # Fallback to LSTM/GRU if Mamba not available
            if model_type == "mamba":
                logger.warning("BitSpikeMamba not found. Falling back to GRU.")
            self.core = nn.GRU(
                input_size=input_dim,
                hidden_size=hidden_dim,
                num_layers=2,
                batch_first=True
            )
            self.input_proj = nn.Identity()

        # 2. Intent Decoder (Latent -> Intent/Goal)
        self.intent_head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, intent_dim)
        )

    def forward(self, observation_sequence: torch.Tensor) -> torch.Tensor:
        """
        Args:
            observation_sequence: [Batch, Time, Input_Dim]
            (e.g., past 10 steps of another agent's position)

        Returns:
            predicted_intent: [Batch, Intent_Dim]
            (e.g., predicted target coordinates)
        """
        B, T, D = observation_sequence.shape

        # Feature Projection
        x = self.input_proj(observation_sequence)  # [B, T, Hidden]

        # Sequence Modeling
        if isinstance(self.core, nn.GRU):
            out, _ = self.core(x)
            final_state = out[:, -1, :]  # Last hidden state
        else:
            # Mamba Forward
            # Mamba returns (logits/features, spikes, mem)
            mamba_out = self.core(x)
            if isinstance(mamba_out, tuple):
                features = mamba_out[0]
            else:
                features = mamba_out

            # Use the feature at the last time step
            if features.dim() == 3:
                final_state = features[:, -1, :]
            else:
                # If T=1 or collapsed
                final_state = features

        # Decode Intent
        intent = self.intent_head(final_state)

        return intent

    def predict_goal(self, trajectory: torch.Tensor) -> torch.Tensor:
        """æŽ¨è«–ç”¨ãƒ©ãƒƒãƒ‘ãƒ¼"""
        self.eval()
        with torch.no_grad():
            return self.forward(trajectory)
