# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/social/theory_of_mind.py
# Title: Theory of Mind (ToM) Module v1.0
# Description:
#   ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒä»–è€…ã®è¡Œå‹•æ„å›³ã‚„ä¿¡å¿µã‚’æ¨å®šã™ã‚‹ãŸã‚ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚
#   ç›¸äº’ä½œç”¨å±¥æ­´ã«åŸºã¥ãã€ç›¸æ‰‹ã®æ¬¡ã®è¡Œå‹•ï¼ˆæŠ•ç¥¨ã€å”åŠ›ãªã©ï¼‰ã‚’äºˆæ¸¬ã™ã‚‹ã€‚

import torch
import torch.nn as nn
from typing import Dict, Deque
from collections import deque
import logging

logger = logging.getLogger(__name__)


class TheoryOfMindModule(nn.Module):
    """
    ç°¡æ˜“çš„ãªå¿ƒã®ç†è«–ï¼ˆToMï¼‰ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚
    ç›¸æ‰‹ã®IDã¨éå»ã®è¡Œå‹•ã‹ã‚‰ã€ãƒ¡ãƒ³ã‚¿ãƒ«ãƒ¢ãƒ‡ãƒ«ï¼ˆå†…éƒ¨çŠ¶æ…‹ã®æ¨å®šï¼‰ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚
    """

    def __init__(self, observation_dim: int = 10, hidden_dim: int = 32, history_len: int = 5):
        super().__init__()
        self.history_len = history_len
        self.observation_dim = observation_dim

        # ãƒ¡ãƒ³ã‚¿ãƒ«ãƒ¢ãƒ‡ãƒ«ç”¨SNN (ç°¡æ˜“çš„ãªMLP/RNNã¨ã—ã¦å®Ÿè£…)
        # å…¥åŠ›: [history_len * observation_dim] -> å‡ºåŠ›: [action_prob]
        self.predictor = nn.Sequential(
            nn.Linear(history_len * observation_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1),  # 0.0 (Reject) ~ 1.0 (Approve/Cooperate)
            nn.Sigmoid()
        )

        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã”ã¨ã®å±¥æ­´: AgentID -> Deque[Observation]
        self.interaction_history: Dict[str, Deque[torch.Tensor]] = {}

        logger.info("ğŸ§  TheoryOfMindModule initialized.")

    def observe_agent(self, agent_id: str, action_vector: torch.Tensor):
        """
        ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¡Œå‹•ã‚’è¦³å¯Ÿã—ã€å±¥æ­´ã«è¿½åŠ ã™ã‚‹ã€‚
        action_vector: è¡Œå‹•ã®ç‰¹å¾´é‡ (ä¾‹: æŠ•ç¥¨å†…å®¹ã€ç™ºè¨€å†…å®¹ã®åŸ‹ã‚è¾¼ã¿)
        """
        if agent_id not in self.interaction_history:
            self.interaction_history[agent_id] = deque(maxlen=self.history_len)

        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†
        if action_vector.shape[0] < self.observation_dim:
            padded = torch.zeros(self.observation_dim)
            padded[:action_vector.shape[0]] = action_vector
            action_vector = padded

        self.interaction_history[agent_id].append(action_vector)

    def predict_action(self, agent_id: str) -> float:
        """
        ç‰¹å®šã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ¬¡ã®è¡Œå‹•ï¼ˆä¾‹ãˆã°ã€è³›æˆç¢ºç‡ï¼‰ã‚’äºˆæ¸¬ã™ã‚‹ã€‚
        """
        if agent_id not in self.interaction_history or len(self.interaction_history[agent_id]) < 1:
            return 0.5  # æƒ…å ±ãªã—

        history = list(self.interaction_history[agent_id])
        # è¶³ã‚Šãªã„åˆ†ã¯ã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
        while len(history) < self.history_len:
            history.insert(0, torch.zeros(self.observation_dim))

        input_tensor = torch.cat(history).unsqueeze(
            0)  # [1, history_len * obs_dim]

        with torch.no_grad():
            prediction = self.predictor(input_tensor).item()

        return prediction

    def update_model(self, agent_id: str, actual_outcome: float):
        """
        äºˆæ¸¬ã¨å®Ÿéš›ã®çµæœã¨ã®èª¤å·®ã«åŸºã¥ã„ã¦ãƒ¡ãƒ³ã‚¿ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’æ›´æ–°ã™ã‚‹ï¼ˆã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ï¼‰ã€‚
        """
        # (ç°¡æ˜“å®Ÿè£…ã®ãŸã‚çœç•¥ã€‚æœ¬æ¥ã¯ã“ã“ã§predictorã®é€†ä¼æ’­ã‚’è¡Œã†)
        pass
