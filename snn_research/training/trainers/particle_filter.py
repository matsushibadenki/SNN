# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/training/trainers/particle_filter.py
# Title: Particle Filter Trainer
# Description: ç²’å­ãƒ•ã‚£ãƒ«ã‚¿ã‚’ç”¨ã„ãŸéå‹¾é…å­¦ç¿’ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã€‚

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Dict, Any
import copy

from snn_research.models.bio.simple_network import BioSNN

class ParticleFilterTrainer:
    def __init__(self, base_model: BioSNN, config: Dict[str, Any], device: str):
        self.base_model = base_model.to(device)
        self.device = device
        self.config = config
        self.num_particles: int = config['training']['biologically_plausible']['particle_filter']['num_particles']
        self.noise_std: float = config['training']['biologically_plausible']['particle_filter']['noise_std']
        self.particles: List[nn.Module] = [copy.deepcopy(self.base_model) for _ in range(self.num_particles)]
        self.particle_weights = torch.ones(self.num_particles, device=self.device) / self.num_particles
        print(f"ğŸŒªï¸ ParticleFilterTrainer initialized with {self.num_particles} particles.")
        
    def train_step(self, data: torch.Tensor, targets: torch.Tensor) -> float:
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ãƒã‚¤ã‚ºã‚’æ³¨å…¥ï¼ˆæ¢ç´¢ã‚¹ãƒ†ãƒƒãƒ—ï¼‰
        for particle in self.particles:
            with torch.no_grad():
                for param in particle.parameters(): 
                    param.add_(torch.randn_like(param) * self.noise_std)
        
        log_likelihoods: List[float] = []
        for particle in self.particles:
            particle.eval()
            with torch.no_grad():
                # ä¿®æ­£: BioSNNã¯[Batch, Input]ã®å½¢çŠ¶ã‚’æœŸå¾…ã™ã‚‹ãŸã‚ã€ãƒãƒƒãƒæ¬¡å…ƒã‚’ä¿æŒã¾ãŸã¯è¿½åŠ ã™ã‚‹
                # dataãŒ[1, N]ã®å ´åˆã¯ãã®ã¾ã¾ã€[N]ã®å ´åˆã¯[1, N]ã«unsqueezeã™ã‚‹
                input_tensor = data
                if input_tensor.dim() == 1:
                    input_tensor = input_tensor.unsqueeze(0)
                
                probs = torch.clamp(input_tensor, 0.0, 1.0)
                input_spikes = torch.bernoulli(probs)
                outputs, _ = particle(input_spikes) # type: ignore[operator]
                
                if targets is not None:
                     # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚‚å‡ºåŠ›ã®æ¬¡å…ƒã«åˆã‚ã›ã‚‹ (outputs: [Batch, Output])
                     target_tensor = targets
                     if target_tensor.dim() == 1:
                         target_tensor = target_tensor.unsqueeze(0)
                     
                     loss = F.mse_loss(outputs, target_tensor)
                     log_likelihoods.append(-loss.item())
                else: 
                    log_likelihoods.append(0.0)
        
        log_likelihoods_tensor = torch.tensor(log_likelihoods, device=self.device)
        # é‡ã¿ã®æ›´æ–° (å°¤åº¦ã«åŸºã¥ã)
        self.particle_weights *= torch.exp(log_likelihoods_tensor - log_likelihoods_tensor.max())
        
        if self.particle_weights.sum() > 0: 
            self.particle_weights /= self.particle_weights.sum()
        else: 
            self.particle_weights.fill_(1.0 / self.num_particles)
            
        # ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚° (Effective Sample Sizeã«åŸºã¥ã)
        if 1.0 / (self.particle_weights**2).sum() < self.num_particles / 2.0:
            indices = torch.multinomial(self.particle_weights, self.num_particles, replacement=True)
            self.particles = [copy.deepcopy(self.particles[i]) for i in indices]
            self.particle_weights.fill_(1.0 / self.num_particles)
            
        return -log_likelihoods_tensor.max().item()