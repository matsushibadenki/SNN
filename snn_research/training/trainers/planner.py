# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/training/trainers/planner.py
# æ—¥æœ¬èªžã‚¿ã‚¤ãƒˆãƒ«: Planner Trainer v2.3 - Mypy Fix
# ç›®çš„ãƒ»å†…å®¹:
#   HierarchicalPlannerãŠã‚ˆã³PlannerSNNã®å­¦ç¿’ã‚’æ‹…å½“ã™ã‚‹ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚¯ãƒ©ã‚¹ã€‚
#   - mypyã‚¨ãƒ©ãƒ¼ä¿®æ­£: self.optimizer ãŒ None ã§ãªã„ã“ã¨ã‚’æ˜Žç¤ºçš„ã«ãƒã‚§ãƒƒã‚¯(assert)ã™ã‚‹å‡¦ç†ã‚’è¿½åŠ ã€‚

import torch
import torch.nn as nn
from typing import Dict, Any, Optional
from torch.utils.data import DataLoader
from transformers import PreTrainedTokenizerBase
import logging

from snn_research.training.base_trainer import AbstractTrainer
from snn_research.cognitive_architecture.planner_snn import PlannerSNN

logger = logging.getLogger(__name__)


class PlannerTrainer(AbstractTrainer):
    """
    PlannerSNNãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’è¡Œã†ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã€‚
    AbstractTrainerã‚’ç¶™æ‰¿ã—ã€train_epoch / validate ã‚’å®Ÿè£…ã™ã‚‹ã€‚
    """

    def __init__(
        self,
        model: PlannerSNN,
        optimizer: torch.optim.Optimizer,
        tokenizer: PreTrainedTokenizerBase,
        device: str = "cuda" if torch.cuda.is_available() else "cpu",
        config: Optional[Dict[str, Any]] = None
    ):
        # è¦ªã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–
        # è¦ªã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ– (args: model, config, optimizer, ...)
        super().__init__(model, config=config, optimizer=optimizer, device=device)
        self.model = model  # åž‹ãƒ’ãƒ³ãƒˆã®ãŸã‚ã«å†ä»£å…¥(å®Ÿä½“ã¯è¦ªã‚¯ãƒ©ã‚¹ã¨åŒã˜)
        self.tokenizer = tokenizer
        # self.config is already set by super().__init__ as DictConfig
        self.criterion = nn.CrossEntropyLoss()

        logger.info(f"ðŸ—ºï¸ PlannerTrainer initialized on {device}.")

    def train_epoch(self, train_loader: DataLoader) -> Dict[str, float]:
        """
        1ã‚¨ãƒãƒƒã‚¯åˆ†ã®å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã€‚
        """
        # mypy fix: OptimizerãŒNoneã§ãªã„ã“ã¨ã‚’ä¿è¨¼
        assert self.optimizer is not None, "Optimizer is required for training."

        self.model.train()
        total_loss = 0.0
        total_acc = 0.0
        steps = 0

        for batch in train_loader:
            self.optimizer.zero_grad()

            # 1. ãƒ‡ãƒ¼ã‚¿ã®å±•é–‹ã¨å‰å‡¦ç†
            goal_texts = batch["goal_text"]
            target_skill_ids = batch["skill_id"].to(self.device)

            # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º
            encoded_inputs = self.tokenizer(
                goal_texts,
                padding=True,
                truncation=True,
                return_tensors="pt",
                max_length=128
            ).input_ids.to(self.device)

            # 2. Forward Pass
            logits = self.model(encoded_inputs)

            # 3. Loss Calculation
            loss = self.criterion(logits, target_skill_ids)

            # 4. Backward Pass & Optimize
            loss.backward()

            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(), max_norm=1.0)
            self.optimizer.step()

            # 5. Metrics
            with torch.no_grad():
                preds = torch.argmax(logits, dim=1)
                acc = (preds == target_skill_ids).float().mean().item()

            total_loss += loss.item()
            total_acc += acc
            steps += 1

            self.global_step += 1

        if steps == 0:
            return {"train_loss": 0.0, "train_accuracy": 0.0}

        return {
            "train_loss": total_loss / steps,
            "train_accuracy": total_acc / steps
        }

    def validate(self, val_loader: DataLoader) -> Dict[str, float]:
        """
        æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ã€‚
        """
        self.model.eval()
        total_loss = 0.0
        total_acc = 0.0
        steps = 0

        with torch.no_grad():
            for batch in val_loader:
                goal_texts = batch["goal_text"]
                target_skill_ids = batch["skill_id"].to(self.device)

                encoded_inputs = self.tokenizer(
                    goal_texts,
                    padding=True,
                    truncation=True,
                    return_tensors="pt",
                    max_length=128
                ).input_ids.to(self.device)

                logits = self.model(encoded_inputs)
                loss = self.criterion(logits, target_skill_ids)

                preds = torch.argmax(logits, dim=1)
                acc = (preds == target_skill_ids).float().mean().item()

                total_loss += loss.item()
                total_acc += acc
                steps += 1

        if steps == 0:
            return {"val_loss": 0.0, "val_accuracy": 0.0}

        return {
            "val_loss": total_loss / steps,
            "val_accuracy": total_acc / steps
        }
