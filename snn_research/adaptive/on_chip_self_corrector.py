# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/adaptive/on_chip_self_corrector.py
# Title: On-Chip Self Corrector (LNN/RSNN Engine)
# Description:
# - Objective 5 & 6: éå‹¾é…å‹å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ (Non-gradient Learning)ã€‚
# - ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½¿ã‚ãšã€å±€æ‰€çš„ãªã‚¹ãƒ‘ã‚¤ã‚¯æ´»å‹•ã¨å ±é…¬ä¿¡å·ã®ã¿ã§é‡ã¿ã‚’æ›´æ–°ã™ã‚‹ã€‚
# - Liquid State Machine / Reservoir Computing ã®æ¦‚å¿µã‚’æ‹¡å¼µã—ãŸè‡ªå·±çµ„ç¹”åŒ–ãƒ­ã‚¸ãƒƒã‚¯ã€‚

import torch
import torch.nn as nn
import logging

logger = logging.getLogger(__name__)


class OnChipSelfCorrector(nn.Module):
    """
    ã‚ªãƒ³ãƒãƒƒãƒ—è‡ªå·±ä¿®æ­£ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚
    æ¨è«–ä¸­ã«ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ã‚·ãƒŠãƒ—ã‚¹è·é‡ã‚’å¾®èª¿æ•´ã—ã€ç’°å¢ƒã«é©å¿œã™ã‚‹ã€‚
    """

    def __init__(self, learning_rate: float = 1e-4, stdp_window: int = 20, device: str = 'cpu'):
        super().__init__()
        self.lr = learning_rate
        self.window = stdp_window
        self.device = device

        # å ±é…¬äºˆæ¸¬èª¤å·® (RPE) ã®å±¥æ­´
        self.rpe_trace = 0.0

        logger.info("ğŸ”§ On-Chip Self Corrector initialized (Non-gradient mode).")

    def observe_and_correct(self,
                            layer_weights: torch.Tensor,
                            pre_spikes: torch.Tensor,
                            post_spikes: torch.Tensor,
                            reward_signal: float) -> torch.Tensor:
        """
        ã‚¹ãƒ‘ã‚¤ã‚¯æ´»å‹•ã¨å ±é…¬ã«åŸºã¥ã„ã¦é‡ã¿ã‚’ä¿®æ­£ã™ã‚‹ (R-STDP: Reward-modulated STDP)ã€‚

        Args:
            layer_weights: å¯¾è±¡ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®é‡ã¿ (å‚ç…§æ¸¡ã—æƒ³å®šã ãŒã€ã“ã“ã§ã¯æ›´æ–°å¾Œã®Tensorã‚’è¿”ã™)
            pre_spikes: ãƒ—ãƒ¬ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®ã‚¹ãƒ‘ã‚¤ã‚¯å±¥æ­´ [Batch, Time, In_Features]
            post_spikes: ãƒã‚¹ãƒˆãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®ã‚¹ãƒ‘ã‚¤ã‚¯å±¥æ­´ [Batch, Time, Out_Features]
            reward_signal: ç’°å¢ƒã‹ã‚‰ã®å ±é…¬ (-1.0 to 1.0)

        Returns:
            updated_weights: æ›´æ–°ã•ã‚ŒãŸé‡ã¿
        """
        # å‹¾é…è¨ˆç®—ã‚’ç„¡åŠ¹åŒ– (å®Œå…¨ãªæ¨è«–ãƒ¢ãƒ¼ãƒ‰ã§ã®å­¦ç¿’)
        with torch.no_grad():
            # ç°¡æ˜“çš„ãªåŒæ™‚ç™ºç«æ¤œå‡º (Correlation-based Hebbian term)
            # Pre[b, t, i] * Post[b, t, j] -> Weight[Out, In] ã®ç›¸é–¢
            # Note: PyTorch Linear weight is [Out_features, In_features]

            # æ™‚é–“æ¬¡å…ƒã§ã®å¹³å‡æ´»å‹•ç‡
            pre_rate = pre_spikes.float().mean(dim=(0, 1))  # [In]
            post_rate = post_spikes.float().mean(dim=(0, 1))  # [Out]

            # ãƒ˜ãƒ–å‰‡é …: "Fire together, wire together"
            # outer(post, pre) ã§ [Out, In] ã®å½¢çŠ¶ã‚’ä½œæˆ
            hebbian_term = torch.outer(post_rate, pre_rate)  # [Out, In]

            # æ’å¸¸æ€§ç¶­æŒé … (LTD): ç™ºç«ã—ã™ãã‚’é˜²ã
            # Postå´ã®ç™ºç«ç‡ãŒé«˜ã„å ´åˆã€å…¨ä½“çš„ã«æŠ‘åˆ¶ã™ã‚‹
            homeostatic_term = post_rate.unsqueeze(
                1) * 0.1  # [Out, 1] -> broadcast

            # ãƒ‰ãƒ¼ãƒ‘ãƒŸãƒ³å¤‰èª¿ (Reward Modulation)
            # å ±é…¬ãŒæ­£ãªã‚‰å¼·åŒ–ã€è² ãªã‚‰æŠ‘åˆ¶ (Anti-Hebbian)
            modulation = reward_signal

            # 3è¦ç´ å‰‡ (Three-factor rule) ã®é©ç”¨
            # delta_w = LearningRate * Reward * (Hebbian - Homeostatic)
            delta_w = self.lr * modulation * (hebbian_term - homeostatic_term)

            # é‡ã¿ã®æ›´æ–°
            new_weights = layer_weights + delta_w.to(layer_weights.device)

            # é‡ã¿ã®ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚° (ç™ºæ•£é˜²æ­¢)
            new_weights = torch.clamp(new_weights, -1.0, 1.0)

            return new_weights

    def compute_local_error(self, desired_activity: torch.Tensor, actual_activity: torch.Tensor) -> float:
        """
        å±€æ‰€çš„ãªäºˆæ¸¬èª¤å·®ã‚’è¨ˆç®—ã™ã‚‹ (Predictive Codingçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ)ã€‚
        """
        with torch.no_grad():
            error = torch.mean(
                (desired_activity - actual_activity) ** 2).item()
        return error
