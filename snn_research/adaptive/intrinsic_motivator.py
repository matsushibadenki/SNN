# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/adaptive/intrinsic_motivator.py
# æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: Intrinsic Motivator (Curiosity & Empowerment)
# ç›®çš„ãƒ»å†…å®¹:
#   ROADMAP Phase 2.1 "Intrinsic Reward" å¯¾å¿œã€‚
#   å¤–éƒ¨å ±é…¬ãŒãªã„ç’°å¢ƒã§ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¡Œå‹•åŸç†ã¨ãªã‚‹ã€Œå†…ç™ºçš„å ±é…¬ã€ã‚’ç”Ÿæˆã™ã‚‹ã€‚
#   1. Curiosity (Prediction Error): äºˆæ¸¬ã§ããªã„ã“ã¨ã‚’çŸ¥ã‚ŠãŸã„æ¬²æ±‚ã€‚
#   2. Empowerment (Control Authority): ç’°å¢ƒã‚’æ€ã„é€šã‚Šã«å‹•ã‹ã—ãŸã„æ¬²æ±‚ã€‚

import torch
import torch.nn as nn
import torch.nn.functional as F
import logging
from typing import Dict, Any, Optional

logger = logging.getLogger(__name__)

class IntrinsicMotivator(nn.Module):
    """
    å†…ç™ºçš„å ±é…¬ï¼ˆCuriosity, Empowermentï¼‰ã‚’è¨ˆç®—ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚
    SNNã®å¯å¡‘æ€§åˆ¶å¾¡ã‚·ã‚°ãƒŠãƒ«(Dopamine equivalent)ã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹ã€‚
    """
    
    def __init__(self, config: Dict[str, Any] = {}):
        super().__init__()
        self.curiosity_weight = config.get("curiosity_weight", 1.0)
        self.empowerment_weight = config.get("empowerment_weight", 0.5)
        self.decay_rate = config.get("decay_rate", 0.99) # é©šãã«å¯¾ã™ã‚‹æ…£ã‚Œ
        
        # ç§»å‹•å¹³å‡ã«ã‚ˆã‚‹ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³èª¤å·®ï¼ˆæ…£ã‚Œã®ãŸã‚ï¼‰
        self.register_buffer("running_error_mean", torch.tensor(0.1))
        
        logger.info("ğŸ§  Intrinsic Motivator initialized.")

    def compute_reward(
        self, 
        predicted_state: torch.Tensor, 
        actual_state: torch.Tensor,
        action_impact: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        å†…ç™ºçš„å ±é…¬ã‚’è¨ˆç®—ã™ã‚‹ã€‚
        
        Args:
            predicted_state: ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ãŒäºˆæ¸¬ã—ãŸæ¬¡ã®çŠ¶æ…‹ (z_pred)
            actual_state: å®Ÿéš›ã«è¦³æ¸¬ã•ã‚ŒãŸæ¬¡ã®çŠ¶æ…‹ (z_actual)
            action_impact: (Optional) è¡Œå‹•ã«ã‚ˆã‚‹çŠ¶æ…‹å¤‰åŒ–é‡ (Empowermentç”¨)
            
        Returns:
            intrinsic_reward: ã‚¹ã‚«ãƒ©ãƒ¼å ±é…¬å€¤
        """
        # 1. Curiosity: Prediction Error (MSE)
        # äºˆæ¸¬ã¨ç¾å®Ÿã®ã‚ºãƒ¬ãŒå¤§ãã„ã»ã©ã€Œé©šãã€ï¼ã€Œå ±é…¬ã€ã¨ã™ã‚‹ï¼ˆæ–°ã—ã„çŸ¥è­˜ã®ç²å¾—ï¼‰
        # ãŸã ã—ã€ãƒã‚¤ã‚ºã¸ã®éé©åˆã‚’é˜²ããŸã‚ã€ã‚ã¾ã‚Šã«ãƒ©ãƒ³ãƒ€ãƒ ãªã‚‚ã®ã¯é™¤å¤–ã™ã‚‹å·¥å¤«ãŒå¿…è¦ã ãŒã€
        # ã“ã“ã§ã¯ã‚·ãƒ³ãƒ—ãƒ«ã«äºˆæ¸¬èª¤å·®ã‚’ç”¨ã„ã‚‹ã€‚
        
        with torch.no_grad():
            prediction_error = F.mse_loss(predicted_state, actual_state, reduction='none').mean(dim=-1)
            # [Batch, Time] -> [Batch] (å¹³å‡)
            batch_error = prediction_error.mean()
            
            # Update baseline (habituation)
            self.running_error_mean = self.running_error_mean * self.decay_rate + batch_error * (1 - self.decay_rate)
            
            # Normalize curiosity: æ™®æ®µã‚ˆã‚Šã©ã‚Œã ã‘é©šã„ãŸã‹
            # running_meanã‚ˆã‚Šå¤§ãã„æ™‚ã ã‘ãƒ—ãƒ©ã‚¹ã«ã™ã‚‹ï¼ˆé€€å±ˆã‚’é˜²ãï¼‰
            curiosity = torch.relu(batch_error - self.running_error_mean) * 10.0
        
        # 2. Empowerment: Action Impact
        # è‡ªåˆ†ã®è¡Œå‹•ãŒç’°å¢ƒã«å¤‰åŒ–ã‚’ä¸ãˆãŸã‹ï¼Ÿ (z_t+1 - z_t ã®å¤§ãã•ãªã©)
        empowerment = 0.0
        if action_impact is not None:
             empowerment = action_impact.norm(p=2, dim=-1).mean()
             
        total_reward = (self.curiosity_weight * curiosity) + (self.empowerment_weight * empowerment)
        
        return total_reward

    def get_stats(self) -> Dict[str, float]:
        return {
            "baseline_error": self.running_error_mean.item()
        }