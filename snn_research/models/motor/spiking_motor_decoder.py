# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/models/motor/spiking_motor_decoder.py
# æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: Spiking Motor Decoder (Neural Action Generator)
# ç›®çš„ãƒ»å†…å®¹:
#   ROADMAP Phase 2 "Multimodal Integration" å¯¾å¿œã€‚
#   çµ±åˆã•ã‚ŒãŸæ½œåœ¨è¡¨ç¾ï¼ˆFused Latentsï¼‰ã‹ã‚‰ã€ç‰©ç†çš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ä¿¡å·ã‚’ç”Ÿæˆã™ã‚‹ã€‚
#   LIFãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’ç”¨ã„ã¦ã€ã‚¹ãƒ‘ã‚¤ã‚¯é »åº¦ã‚’é€£ç¶šå€¤ï¼ˆãƒ¢ãƒ¼ã‚¿ãƒ¼å‡ºåŠ›ï¼‰ã‚„é›¢æ•£å€¤ï¼ˆã‚³ãƒãƒ³ãƒ‰ï¼‰ã«å¤‰æ›ã™ã‚‹ã€‚

import torch
import torch.nn as nn
import logging
from typing import Dict, Any

from snn_research.core.factories import NeuronFactory

logger = logging.getLogger(__name__)


class SpikingMotorDecoder(nn.Module):
    """
    é«˜æ¬¡ã®æ¦‚å¿µï¼ˆã‚¹ãƒ‘ã‚¤ã‚¯åˆ—ï¼‰ã‚’é‹å‹•æŒ‡ä»¤ã«ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€‚
    Continuous Control (ãƒ­ãƒœãƒƒãƒˆã‚¢ãƒ¼ãƒ ç­‰) ã¨ Discrete Control (ç§»å‹•ã‚³ãƒãƒ³ãƒ‰ç­‰) ã®ä¸¡æ–¹ã«å¯¾å¿œã€‚
    """

    def __init__(
        self,
        input_dim: int,
        action_dim: int,
        action_type: str = "continuous",  # 'continuous' or 'discrete'
        hidden_dim: int = 128,
        time_steps: int = 16,
        neuron_config: Dict[str, Any] = {"type": "lif"}
    ):
        super().__init__()
        self.input_dim = input_dim
        self.action_dim = action_dim
        self.action_type = action_type
        self.time_steps = time_steps

        # 1. Motor Planning Layer (Hidden State)
        self.plan_layer = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            NeuronFactory.create(features=hidden_dim, config=neuron_config)
        )

        # 2. Motor Execution Layer (Output)
        self.exec_layer = nn.Linear(hidden_dim, action_dim)

        # å‡ºåŠ›å±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ (é€£ç¶šå€¤åˆ¶å¾¡ã®å ´åˆã¯ç©åˆ†ã—ã¦ã‚¢ãƒŠãƒ­ã‚°å€¤ã«ã™ã‚‹ãŸã‚ã€LIFã‚’é€šã—ãŸå¾Œã«å¹³å‡åŒ–ãªã©ãŒä¸€èˆ¬çš„)
        # ã“ã“ã§ã¯æœ€çµ‚å±¤ã¯Linearå‡ºåŠ›ã¨ã—ã€æå¤±é–¢æ•°å´ã§åˆ¶å¾¡ã™ã‚‹æ§‹æˆã‚’ã¨ã‚‹ï¼ˆDirect Code Predictionï¼‰
        # ã¾ãŸã¯ã€Last Layer Spiking -> Low Pass Filter ã‚‚ã‚ã‚Šã ãŒã€å­¦ç¿’å®‰å®šæ€§ã®ãŸã‚Linear Readoutã‚’æ¡ç”¨ã€‚

        logger.info(
            f"ğŸ¦¾ SpikingMotorDecoder initialized. Type: {action_type}, Out: {action_dim}")

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: Input Latents [Batch, Time, Input_Dim] (from VLM Fused Representation)

        Returns:
            action_output: [Batch, Action_Dim] (Continuous values or Logits)
        """
        B, T, D = x.shape

        # æ™‚é–“æ–¹å‘ã®æƒ…å ±ã‚’é›†ç´„ã—ã¤ã¤ã€é‹å‹•è¨ˆç”»ã‚’ç”Ÿæˆ
        # SNNçš„ã«ã¯å„ã‚¹ãƒ†ãƒƒãƒ—ã§å‡¦ç†ã—ã€æœ€å¾Œã«Readoutã™ã‚‹

        # Reset neurons if stateful (assuming NeuronFactory handles state internally or functional calls)
        # For simplicity in this module, we assume functional or auto-reset in forward loop if implemented
        # Here using simple feedforward for demonstration of structure

        # Input x is already a sequence of features (spikes or embeddings)
        # We process it to extract the "Action Intent"

        # Flatten time or Average pooling based on strategy
        # Strategy: Temporal Average of Plan Layer -> Execution

        # Apply Plan Layer per step (if applicable) or on aggregated features
        # ã“ã“ã§ã¯ã€Œæ–‡è„ˆå…¨ä½“ã‹ã‚‰è¡Œå‹•ã‚’æ±ºå®šã™ã‚‹ã€ãŸã‚ã€æ™‚é–“å¹³å‡ã‚’ã¨ã£ã¦ã‹ã‚‰Decodeã™ã‚‹
        # ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ¶å¾¡ã®å ´åˆã¯ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã®å‡ºåŠ›ãŒå¿…è¦ã ãŒã€VLMé€£æºã§ã¯ã€Œåˆ¤æ–­ã€ãŒä¸»ï¼‰

        x_mean = x.mean(dim=1)  # [B, Input_Dim]

        plan = self.plan_layer(x_mean)  # [B, Hidden_Dim] (Spikes/Activation)

        if isinstance(plan, tuple):  # Some neuron models return (spikes, mem)
            plan = plan[0]

        action_out = self.exec_layer(plan)  # [B, Action_Dim]

        if self.action_type == "continuous":
            # Tanh for normalized motor control (-1 to 1)
            action_out = torch.tanh(action_out)

        return action_out
