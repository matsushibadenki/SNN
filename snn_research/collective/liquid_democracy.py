# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: snn_research/collective/liquid_democracy.py
# æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: Liquid Democracy Protocol (Type Safe)
# ä¿®æ­£å†…å®¹: Tensorã¨floatã®å‹ä¸ä¸€è‡´ã‚¨ãƒ©ãƒ¼ã‚’ä¿®æ­£ (.item()ã®è¿½åŠ )ã€‚

import torch
import logging
from typing import Dict, Optional, Any, Tuple
from dataclasses import dataclass

from snn_research.agent.synesthetic_agent import SynestheticAgent
from snn_research.social.theory_of_mind import TheoryOfMindModule

logger = logging.getLogger(__name__)


@dataclass
class Vote:
    agent_id: str
    decision: int  # 0 or 1
    weight: float = 1.0


@dataclass
class Proposal:
    """
    æŠ•ç¥¨å¯¾è±¡ã¨ãªã‚‹ææ¡ˆã‚„èª²é¡Œã€‚
    """
    id: str
    content: Any  # torch.Tensor or Text
    description: str = ""


class LiquidDemocracyProtocol:
    """
    æµå‹•çš„æ°‘ä¸»ä¸»ç¾©ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã€‚
    ToMã‚’ç”¨ã„ãŸå§”ä»»(Delegation)ã¨ã€åŠ é‡æŠ•ç¥¨(Weighted Voting)ã‚’ç®¡ç†ã™ã‚‹ã€‚
    """

    def __init__(self, agents: Dict[str, SynestheticAgent], toms: Dict[str, TheoryOfMindModule]):
        self.agents = agents
        self.toms = toms  # AgentID -> TheoryOfMindModule
        if agents:
            self.device = next(iter(agents.values())).device
        else:
            self.device = 'cpu'

    def conduct_vote(self, task_input: torch.Tensor, ground_truth: Optional[int] = None) -> Dict[str, Any]:
        """
        æŠ•ç¥¨ã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
        """
        # 1. å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®åˆæœŸåˆ¤æ–­ã¨è‡ªä¿¡åº¦
        initial_decisions: Dict[str, Tuple[int, float]] = {}

        for agent_id, agent in self.agents.items():
            # å…¥åŠ›å½¢å¼ã®èª¿æ•´
            obs = {'vision': task_input.unsqueeze(
                0) if task_input.dim() == 1 else task_input}

            with torch.no_grad():
                # Agentã®stepæˆ»ã‚Šå€¤ã¯ {"action_pred": Tensor, ...} ãªã©ã‚’æƒ³å®š
                # ç°¡æ˜“çš„ã« action_pred ãŒ Tensor(1, dim) ã§è¿”ã‚‹ã¨ä»®å®š
                result = agent.step(obs)

                # çµæœã®å–ã‚Šå‡ºã—ï¼ˆè¾æ›¸ã¾ãŸã¯Tensorã«å¯¾å¿œï¼‰
                if isinstance(result, dict):
                    action = result.get("action_pred", torch.tensor([[0.0]]))
                else:
                    action = result

                if isinstance(action, torch.Tensor):
                    val = action.mean().item()  # å¹³å‡å€¤ã§ç°¡æ˜“åˆ¤å®š
                else:
                    val = 0.0

            decision = 1 if val > 0 else 0
            confidence = abs(val)
            initial_decisions[agent_id] = (decision, confidence)

        # 2. å§”ä»»ãƒ•ã‚§ãƒ¼ã‚º (Delegation)
        vote_powers: Dict[str, float] = {
            aid: 1.0 for aid in self.agents.keys()}
        delegation_map: Dict[str, str] = {}  # from -> to

        for agent_id, (my_dec, my_conf) in initial_decisions.items():
            # è‡ªä¿¡ãŒä½ã„å ´åˆã¯å§”ä»»ã‚’æ¤œè¨
            if my_conf < 0.3:
                best_target = None
                max_trust = -1.0

                tom = self.toms.get(agent_id)
                if tom is None:
                    continue

                for other_id in self.agents.keys():
                    if other_id == agent_id:
                        continue

                    # ToMã«ã‚ˆã‚‹ä¿¡é ¼åº¦äºˆæ¸¬ (Tensor -> float)
                    trust_tensor = tom.predict_action(other_id)
                    trust_val = trust_tensor.item()

                    if trust_val > max_trust:
                        max_trust = trust_val
                        best_target = other_id

                # ä¿¡é ¼ã§ãã‚‹ç›¸æ‰‹ãŒã„ã‚Œã°å§”ä»»
                if best_target and max_trust > 0.6:
                    delegation_map[agent_id] = best_target
                    logger.debug(
                        f"ğŸ”„ {agent_id} delegates to {best_target} (Trust: {max_trust:.2f})")

        # ç¥¨ã®ç§»å‹• (Single Hop)
        final_voters = []
        for agent_id in self.agents.keys():
            if agent_id in delegation_map:
                target = delegation_map[agent_id]
                # å§”ä»»å…ˆã®ç¥¨ã‚’å¢—ã‚„ã™
                if target in vote_powers:
                    vote_powers[target] += vote_powers[agent_id]
                vote_powers[agent_id] = 0.0
            else:
                final_voters.append(agent_id)

        # 3. é›†è¨ˆ (Aggregation)
        weighted_sum = 0.0
        total_power = 0.0

        for voter_id in final_voters:
            decision, _ = initial_decisions[voter_id]
            power = vote_powers[voter_id]

            weighted_sum += decision * power
            total_power += power

        final_score = weighted_sum / total_power if total_power > 0 else 0.0
        final_decision = 1 if final_score >= 0.5 else 0

        # 4. ç¤¾ä¼šçš„å­¦ç¿’ (Social Learning)
        is_correct = None
        if ground_truth is not None:
            is_correct = (final_decision == ground_truth)

            for agent_id in self.agents.keys():
                target_id = delegation_map.get(agent_id, agent_id)

                # å§”ä»»å…ˆã®åˆ¤æ–­ãŒæ­£ã—ã‹ã£ãŸã‹è©•ä¾¡
                target_dec, _ = initial_decisions[target_id]
                target_correct = (target_dec == ground_truth)

                outcome_val = 1.0 if target_correct else 0.0

                # ToMãƒ¢ãƒ‡ãƒ«ã®æ›´æ–°
                if agent_id in self.toms:
                    self.toms[agent_id].update_model(target_id, outcome_val)

        return {
            'consensus_decision': final_decision,
            'vote_ratio': final_score,
            'delegation_count': len(delegation_map),
            'correct': is_correct
        }
