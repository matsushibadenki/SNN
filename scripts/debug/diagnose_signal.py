from transformers import AutoTokenizer
from torch.utils.data import DataLoader
from snn_research.data.datasets import SimpleTextDataset
from snn_research.core.snn_core import SNNCore
import sys
import os
import torch
from pathlib import Path
from omegaconf import OmegaConf

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).resolve().parent.parent))


def main():
    print("ğŸ” SNNè©³ç´°ä¿¡å·è¨ºæ–­ (Full Forward / Fixed) ã‚’é–‹å§‹ã—ã¾ã™...")

    # 1. è¨­å®šã¨ãƒ¢ãƒ‡ãƒ«
    config_path = "configs/models/bit_rwkv_micro.yaml"
    if not os.path.exists(config_path):
        print(f"âŒ Config not found: {config_path}")
        return

    cfg = OmegaConf.load(config_path)

    print("  - Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    device = "cpu"

    print("  - Building model...")
    try:
        model_container = SNNCore(config=cfg.model, vocab_size=len(tokenizer))
        model = model_container.model
        model.to(device)
        model.eval()
    except Exception as e:
        print(f"âŒ ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ã‚¨ãƒ©ãƒ¼: {e}")
        return

    # 2. ãƒ‡ãƒ¼ã‚¿æº–å‚™
    data_path = "data/smoke_test_data.jsonl"
    if not os.path.exists(data_path):
        print(f"âŒ Data not found: {data_path}")
        return

    dataset = SimpleTextDataset(data_path, tokenizer, max_seq_len=16)
    loader = DataLoader(dataset, batch_size=1, shuffle=False)

    try:
        batch = next(iter(loader))
        if isinstance(batch, (list, tuple)):
            input_ids = batch[0].to(device)
        elif isinstance(batch, dict):
            input_ids = batch['input_ids'].to(device)
        else:
            print(f"âŒ Unexpected batch type: {type(batch)}")
            return
    except StopIteration:
        print("âŒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒç©ºã§ã™ã€‚")
        return

    # 3. è©³ç´°è¨ºæ–­å®Ÿè¡Œ
    print("\nğŸ“Š ãƒ¬ã‚¤ãƒ¤ãƒ¼åˆ¥ä¿¡å·è¿½è·¡:")

    # ãƒ•ãƒƒã‚¯é–¢æ•°: å…¥å‡ºåŠ›ã®çµ±è¨ˆã‚’è¡¨ç¤º (ä¿®æ­£: floatã‚­ãƒ£ã‚¹ãƒˆè¿½åŠ )
    def debug_hook(name):
        def hook(module, input, output):
            if isinstance(input, tuple):
                input = input[0]
            if isinstance(output, tuple):
                output = output[0]

            # float()ã«ã‚­ãƒ£ã‚¹ãƒˆã—ã¦ã‹ã‚‰è¨ˆç®—ã™ã‚‹ã“ã¨ã§ã‚¨ãƒ©ãƒ¼ã‚’å›é¿
            in_mean = input.float().abs().mean().item(
            ) if isinstance(input, torch.Tensor) else 0.0
            out_mean = output.float().abs().mean().item(
            ) if isinstance(output, torch.Tensor) else 0.0
            out_max = output.float().abs().max().item(
            ) if isinstance(output, torch.Tensor) else 0.0

            # ã‚¹ãƒ‘ã‚¤ã‚¯æ•° (ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®å ´åˆ)
            spike_info = ""
            if "lif" in name.lower() or "neuron" in name.lower():
                if isinstance(output, torch.Tensor):
                    spike_count = output.sum().item()
                    spike_rate = output.float().mean().item() * 100
                    spike_info = f" | Spikes: {int(spike_count)} (Rate: {spike_rate:.2f}%)"

            print(f"  ğŸ”¹ [{name}]")
            print(f"      Input Mean: {in_mean:.6f}")
            print(
                f"      Output Mean: {out_mean:.6f} | Max: {out_max:.6f}{spike_info}")

            if out_max == 0 and "lif" not in name.lower() and "neuron" not in name.lower():
                # Embeddingå…¥åŠ›(Long)ã¯é™¤ã
                if "embedding" not in name.lower():
                    print(f"      ğŸš¨ ä¿¡å·æ¶ˆå¤±è­¦å ±: {name} ã®å‡ºåŠ›ãŒã‚¼ãƒ­ã§ã™ï¼")
        return hook

    # ä¸»è¦ãªå±¤ã«ãƒ•ãƒƒã‚¯ã‚’ç™»éŒ²
    hooks = []

    # Embedding
    hooks.append(model.embedding.register_forward_hook(
        debug_hook("Embedding")))

    # å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’æ¢ç´¢ã—ã¦ãƒ•ãƒƒã‚¯
    if hasattr(model, 'layers'):
        for i, layer in enumerate(model.layers):
            # BitLinear (time_mix_k)
            if hasattr(layer, 'time_mix_k'):
                hooks.append(layer.time_mix_k.register_forward_hook(
                    debug_hook(f"Layer{i}.TimeMix_K (BitLinear)")))

            # LIF (time_key_lif)
            if hasattr(layer, 'time_key_lif'):
                # è¨­å®šå€¤ã®ç¢ºèª
                neuron = layer.time_key_lif
                thresh = neuron.base_threshold
                if isinstance(thresh, torch.Tensor):
                    thresh = thresh.mean().item()
                tau_val = 0.0
                if hasattr(neuron, 'log_tau_mem') and isinstance(neuron.log_tau_mem, torch.Tensor):
                    tau_val = (torch.exp(neuron.log_tau_mem) +
                               1.1).mean().item()
                elif hasattr(neuron, 'tau_mem'):
                    tau_val = float(neuron.tau_mem)

                print(
                    f"\n  [Layer {i} Config Check] Threshold: {thresh}, Tau: {tau_val:.2f}")
                hooks.append(layer.time_key_lif.register_forward_hook(
                    debug_hook(f"Layer{i}.LIF_K")))

    # å®Ÿè¡Œ
    with torch.no_grad():
        try:
            print("\n  --- Forward Pass Start ---")
            model(input_ids, return_spikes=True)
            print("  --- Forward Pass End ---")
        except Exception as e:
            print(f"âŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            traceback.print_exc()

    # å¾Œå§‹æœ«
    for h in hooks:
        h.remove()
    print("\nâœ… è¨ºæ–­çµ‚äº†")


if __name__ == "__main__":
    main()
