# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: scripts/runners/run_distillation_demo.py
# Title: System 1/2 Distillation Demo
# Description:
#   Symbolic Teacher (System 2) ãŒç”Ÿæˆã—ãŸç®—æ•°ã®è§£æ³•ãƒ—ãƒ­ã‚»ã‚¹ã‚’ã€
#   BitSpikeStudent (System 1) ã«è’¸ç•™ã—ã€æ€è€ƒèƒ½åŠ›ã®ç²å¾—ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã™ã‚‹ã€‚

import sys
import os
import torch
import torch.nn as nn
import logging
import random

# ãƒ‘ã‚¹è¨­å®š
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../../')))

from snn_research.distillation.thought_distiller import ThoughtDistillationManager, SymbolicTeacher

# ãƒ­ã‚¬ãƒ¼è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(message)s')
logger = logging.getLogger("DistillationDemo")

# --- Mock Student Model (BitSpikeMamba Placeholder) ---
class MockBitSpikeStudent(nn.Module):
    """
    ãƒ‡ãƒ¢ç”¨ã®ç°¡æ˜“BitSpikeãƒ¢ãƒ‡ãƒ«ã€‚
    å®Ÿéš›ã«ã¯ã“ã“ã« BitSpikeMamba ãªã©ã®å®Ÿãƒ¢ãƒ‡ãƒ«ãŒå…¥ã‚‹ã€‚
    """
    def __init__(self):
        super().__init__()
        # å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (ãƒ€ãƒŸãƒ¼)
        self.weights = nn.Parameter(torch.randn(10, 10))
        self.knowledge_level = 0.0 # 0.0 (Baka) -> 1.0 (Genius)
    
    def forward_text_loss(self, input_text, target_text):
        """
        ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã‚’å—ã‘å–ã‚Šã€Lossã‚’è¿”ã™ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®ãƒ¢ãƒƒã‚¯ã€‚
        å­¦ç¿’ãŒé€²ã‚€ã«ã¤ã‚Œã¦LossãŒä¸‹ãŒã‚‹æŒ™å‹•ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã€‚
        """
        # ç°¡æ˜“ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³: å­¦ç¿’ãŒé€²ã‚€ã¨LossãŒä¸‹ãŒã‚‹
        current_loss = max(2.0 - self.knowledge_level * 2.0, 0.1)
        # ãƒ©ãƒ³ãƒ€ãƒ ãªæºã‚‰ã
        noise = random.uniform(-0.1, 0.1)
        
        # å­¦ç¿’åŠ¹æœã®è“„ç© (æœ¬æ¥ã¯OptimizerãŒè¡Œã†ãŒã€ã“ã“ã§ã¯ç°¡ç•¥åŒ–)
        self.knowledge_level += 0.05
        
        return torch.tensor(current_loss + noise, requires_grad=True)

    def generate(self, input_text):
        """æ¨è«–"""
        if self.knowledge_level < 0.3:
            return "I don't know... maybe 10?"
        elif self.knowledge_level < 0.8:
            return "First add ones... something... Answer: ??"
        else:
            # æ•™å¸«ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’æ¨¡å€£ã—ãŸã‚ˆã†ãªå‡ºåŠ›ã‚’è¿”ã™
            return "First, add ones... Write 2, carry 1... Answer: Correct!"

def main():
    print("âš—ï¸ --- System 1/2 Distillation Demo ---")
    print("   Target: Distill arithmetic reasoning into BitSpike Model")
    
    # 1. ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆæœŸåŒ–
    student = MockBitSpikeStudent()
    teacher = SymbolicTeacher()
    manager = ThoughtDistillationManager(student, teacher)
    
    # 2. å•é¡Œã‚»ãƒƒãƒˆã®ä½œæˆ
    problems = [
        "15 + 27", "12 + 19", "35 + 46", "8 + 5", "50 + 50",
        "22 + 39", "9 + 91", "14 + 16", "77 + 3", "25 + 25"
    ]
    
    # 3. æ•™å¸«ã«ã‚ˆã‚‹æ€è€ƒãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ (System 2 Generation)
    print("\nğŸ”¹ Phase 1: Teacher (System 2) Thought Generation")
    dataset = manager.generate_thought_dataset(problems)
    
    # ãƒ‡ãƒ¼ã‚¿ã®ä¾‹ã‚’è¡¨ç¤º
    sample = dataset[0]
    print("   Sample Data:")
    print(f"   Q: {sample['input']}")
    print(f"   CoT: {sample['thought_chain']}")
    print(f"   A: {sample['answer']}")
    
    # 4. è’¸ç•™å‰ã®ç”Ÿå¾’ã®æ€§èƒ½ç¢ºèª
    print("\nğŸ”¹ Phase 2: Pre-Distillation Student Performance")
    test_q = "Q: 15 + 27"
    print(f"   Student says: {student.generate(test_q)}")
    
    # 5. è’¸ç•™å­¦ç¿’ (Distillation)
    print("\nğŸ”¹ Phase 3: Distillation Training")
    manager.distill(dataset, epochs=5)
    
    # 6. è’¸ç•™å¾Œã®ç”Ÿå¾’ã®æ€§èƒ½ç¢ºèª
    print("\nğŸ”¹ Phase 4: Post-Distillation Student Performance")
    print(f"   Student says: {student.generate(test_q)}")
    
    if "Correct" in student.generate(test_q):
        print("\nâœ… SUCCESS: System 1 successfully internalized System 2's reasoning!")
    else:
        print("\nâš ï¸ PARTIAL: Training complete, but mimicking needs improvement.")

if __name__ == "__main__":
    main()