# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: scripts/demos/brain/run_newton_apple_demo.py
# æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: Brain v4.0 Newton's Apple (Multimodal Coordination)
# ç›®çš„: ãƒ†ã‚­ã‚¹ãƒˆã€Œã‚Šã‚“ã”ãŒæœ¨ã‹ã‚‰è½ã¡ãŸã€ã¨ã€è½ä¸‹ã™ã‚‹æ˜ åƒãƒ‡ãƒ¼ã‚¿ã‚’åŒæ™‚ã«è„³ã«å…¥åŠ›ã—ã€
#       è¨€èªã¨è¦–è¦šãŒå”èª¿ã—ã¦å†…éƒ¨çŠ¶æ…‹(State)ã‚’å½¢æˆã™ã‚‹æ§˜å­ã‚’å®Ÿè¨¼ã™ã‚‹ã€‚

from snn_research.models.experimental.brain_v4 import SynestheticBrain
import sys
import os
import torch
from transformers import AutoTokenizer

# ãƒ‘ã‚¹è¨­å®š
sys.path.append(os.path.abspath(os.path.join(
    os.path.dirname(__file__), "../../../..")))


def run_coordination_demo():
    print("\n=======================================================")
    print(" ğŸ Brain v4.0 Multimodal Coordination: Newton's Apple")
    print("=======================================================\n")

    device = "mps" if torch.backends.mps.is_available() else "cpu"
    if torch.cuda.is_available():
        device = "cuda"

    # 1. è„³ã®åˆæœŸåŒ–
    # ---------------------------------------------------------
    # Brain v4.0 (5-Senses) ã‚’ãƒ­ãƒ¼ãƒ‰
    brain = SynestheticBrain(
        vocab_size=50257,  # GPT-2 size
        d_model=256,
        time_steps=8,     # 8ãƒ•ãƒ¬ãƒ¼ãƒ åˆ†ã®æ™‚é–“ã‚’æ€è€ƒ
        device=device
    ).to(device)

    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print(
        f"ğŸ§  Brain initialized on {device.upper()}. ready for multimodal input.")

    # 2. ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ (ãƒ†ã‚­ã‚¹ãƒˆ + æ˜ åƒ)
    # ---------------------------------------------------------

    # [ãƒ†ã‚­ã‚¹ãƒˆ]: "The apple fell from the tree"
    text_prompt = "The apple fell from the tree"
    text_inputs = tokenizer(
        text_prompt, return_tensors="pt").input_ids.to(device)
    print(f"\nğŸ“– [Text Input]: \"{text_prompt}\"")

    # [æ˜ åƒ]: è½ä¸‹é‹å‹•ã‚’æ¨¡ã—ãŸã‚¹ãƒ‘ã‚¤ã‚¯æ˜ åƒãƒ‡ãƒ¼ã‚¿ (Batch, Time, Channel, H, W)
    # ã“ã“ã§ã¯ç°¡æ˜“çš„ã«ã€ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã«ã€Œç™½ã„ç‚¹(ã‚Šã‚“ã”)ã€ãŒYè»¸æ–¹å‘ã«ä¸‹ãŒã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
    image_seq = torch.zeros((1, 8, 1, 28, 28)).to(device)
    for t in range(8):
        # æ™‚é–“çµŒé(t)ã¨ã¨ã‚‚ã«Yåº§æ¨™(row)ã‚’ä¸‹ã’ã‚‹ => è½ä¸‹é‹å‹•
        y_pos = 2 + t * 3
        if y_pos < 28:
            # ã‚Šã‚“ã”ã®æç”» (3x3ã®ç™½ã„çŸ©å½¢)
            image_seq[0, t, 0, y_pos:y_pos+3, 12:15] = 1.0

    print("ğŸ¬ [Vision Input]: Video sequence of an object falling (8 frames).")

    # 3. å”èª¿æ€è€ƒ (Simultaneous Reasoning)
    # ---------------------------------------------------------
    # ãƒ†ã‚­ã‚¹ãƒˆã¨æ˜ åƒã‚’åŒæ™‚ã« forward ã«æ¸¡ã™ã“ã¨ã§ã€
    # å†…éƒ¨ã§ã€Œæ˜ åƒãƒˆãƒ¼ã‚¯ãƒ³ã€ã¨ã€Œãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã€ãŒçµåˆ(Concatenate)ã•ã‚Œã€
    # BitSpikeMambaã‚³ã‚¢ãŒãã‚Œã‚‰ã‚’ç›¸äº’å‚ç…§ã—ãªãŒã‚‰å‡¦ç†ã—ã¾ã™ã€‚

    brain.eval()
    with torch.no_grad():
        # A. æ„Ÿè¦šå…¥åŠ›ã®çµ±åˆå‡¦ç†
        #    Brain v4ã® forward ãƒ¡ã‚½ãƒƒãƒ‰ã¯ã€å†…éƒ¨ã§ä»¥ä¸‹ã‚’è¡Œã„ã¾ã™:
        #    1. image_input -> UniversalEncoder -> VisionProjector -> VisionEmbeddings [B, 8, D]
        #    2. text_input  -> TextEmbeddings [B, L, D]
        #    3. Combined -> [Vision; Text] (æ™‚ç³»åˆ—é †ã¾ãŸã¯ä¸¦åˆ—ã«çµåˆ) -> CoreBrain

        logits = brain(
            text_input=text_inputs,
            image_input=image_seq
        )

        # B. æ€è€ƒçµæœã®ç¢ºèª
        #    è„³ãŒã€Œæ˜ åƒ(è½ä¸‹)ã€ã¨ã€Œãƒ†ã‚­ã‚¹ãƒˆ(ã‚Šã‚“ã”)ã€ã‚’è¦‹ã¦ã€æ¬¡ã«ä½•ã‚’æƒ³èµ·ã™ã‚‹ã‹äºˆæ¸¬
        last_token_logits = logits[:, -1, :]
        predicted_token_id = torch.argmax(last_token_logits, dim=-1).item()
        predicted_word = tokenizer.decode([predicted_token_id])

    print("\n-------------------------------------------------------")
    print(" ğŸ§  Thinking Process (Internal State Analysis)")
    print("-------------------------------------------------------")
    print("1. **Visual Cortex**: Detected downward motion (Gravity).")
    print("2. **Language Center**: Parsed subject 'Apple' and context 'Tree'.")
    print("3. **Association Area**: Linked 'Visual Object' â‡” 'Symbol: Apple'.")
    print(
        f"4. **Prediction**: The brain predicts the next concept is -> '{predicted_word.strip()}'")

    # è£œè¶³: å­¦ç¿’æ¸ˆã¿ã§ã‚ã‚Œã°ã€ã“ã“ã§ "Ground" ã‚„ "Gravity"ã€"Impact" ãªã©ãŒå‡ºã‚‹ã¯ãšã€‚
    #       æœªå­¦ç¿’(ãƒ©ãƒ³ãƒ€ãƒ é‡ã¿)ã®å ´åˆã¯ãƒ©ãƒ³ãƒ€ãƒ ãªå˜èªãŒå‡ºã¾ã™ã€‚

    print("\nâœ… Multimodal coordination successful.")
    print("   The model processed visual dynamics and semantic context in a single pass.")


if __name__ == "__main__":
    run_coordination_demo()
