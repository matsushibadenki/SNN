# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: scripts/runners/run_world_model_demo.py
# Title: Meta-Cognition & World Model Simulation Demo (Fixed)
# Description:
#   System 1 (ç›´æ„Ÿ) ã¨ System 2 (ç†Ÿè€ƒ/ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³) ã®åˆ‡ã‚Šæ›¿ãˆãƒ‡ãƒ¢ã€‚
#   ä¿®æ­£: å¼•æ•°åã®æ•´åˆæ€§ã‚’ä¿®æ­£ã€‚

import sys
import os
import torch
import logging

# ãƒ‘ã‚¹è¨­å®š
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../../')))

from snn_research.cognitive_architecture.meta_cognitive_snn import MetaCognitiveSNN
from snn_research.models.experimental.world_model_snn import SpikingWorldModel

logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(message)s')
logger = logging.getLogger("SWM_Demo")

def main():
    print("ðŸŒ --- Spiking World Model & Meta-Cognition Demo ---")
    
    # 1. ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
    # System 1 Monitor
    meta_snn = MetaCognitiveSNN(d_model=10) 
    
    # System 2 Simulator (World Model)
    # ä¿®æ­£: vocab_size=0 (é€£ç¶šå€¤å…¥åŠ›), latent_dim -> d_model
    swm = SpikingWorldModel(
        vocab_size=0, 
        input_dim=64, 
        action_dim=4, 
        d_model=128
    )
    
    # 2. System 1 ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ (ä¸ç¢ºå®Ÿæ€§ã®æ¤œçŸ¥)
    print("\nðŸ”¹ Phase 1: Meta-Cognitive Monitoring")
    
    # ã‚±ãƒ¼ã‚¹A: è‡ªä¿¡ãŒã‚ã‚‹ (Low Entropy)
    logits_confident = torch.tensor([[10.0, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5, 0.5]])
    res_a = meta_snn.monitor_system1_output(logits_confident)
    print(f"   Case A (Confident): Entropy={res_a['entropy']:.4f} -> Trigger System 2? {res_a['trigger_system2']}")
    
    # ã‚±ãƒ¼ã‚¹B: è¿·ã£ã¦ã„ã‚‹ (High Entropy)
    logits_uncertain = torch.tensor([[2.0, 2.0, 2.1, 1.9, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0]])
    res_b = meta_snn.monitor_system1_output(logits_uncertain)
    print(f"   Case B (Uncertain): Entropy={res_b['entropy']:.4f} -> Trigger System 2? {res_b['trigger_system2']}")
    
    if res_b['trigger_system2']:
        print("   ðŸš¨ Uncertainty detected! Switching to Deep Thought Mode (System 2)...")
        
    # 3. è„³å†…ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ (World Modelã«ã‚ˆã‚‹è¨ˆç”»)
    print("\nðŸ”¹ Phase 2: Mental Simulation (Planning)")
    print("   Simulating 3 different action plans to resolve uncertainty...")
    
    # ç¾åœ¨ã®è¦³æ¸¬ (Dummy Input)
    current_obs = torch.randn(1, 64)
    initial_latent = swm.encode(current_obs)
    
    # 3ã¤ã®è¡Œå‹•ãƒ—ãƒ©ãƒ³ (Action Sequences)
    # (Batch, Steps, ActionDim)
    # Plan 1: ãƒ©ãƒ³ãƒ€ãƒ ãªè¡Œå‹•
    plan_1 = torch.randn(1, 5, 4) 
    # Plan 2: å¾…æ©Ÿ (å…¨ã¦0.0ã«è¿‘ã„)
    plan_2 = torch.zeros(1, 5, 4) + 0.1
    # Plan 3: ç‰¹å®šã®è¡Œå‹• (å³ã¸è¡Œããªã©)
    plan_3 = torch.zeros(1, 5, 4)
    plan_3[:, :, 0] = 2.0 # Strong action on dim 0
    
    plans = {"Random": plan_1, "Wait": plan_2, "Act": plan_3}
    best_plan = None
    max_reward = -float('inf')
    
    for name, actions in plans.items():
        # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
        sim_result = swm.simulate_trajectory(initial_latent, actions)
        
        # äºˆæ¸¬ã•ã‚ŒãŸç´¯ç©å ±é…¬
        total_reward = sim_result["rewards"].sum().item()
        
        print(f"   - Plan '{name}': Expected Reward = {total_reward:.4f}")
        
        if total_reward > max_reward:
            max_reward = total_reward
            best_plan = name
            
    print(f"\nâœ… Decision: Selected Plan '{best_plan}' based on mental simulation.")
    
    # 4. ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã¨å­¦ç¿’ (Surprise Detection)
    print("\nðŸ”¹ Phase 3: Reality Check (Surprise)")
    
    predicted_next = initial_latent 
    # å®Ÿéš›ã®çµæžœãŒäºˆæ¸¬ã¨å¤§ããç•°ãªã‚‹ã¨ã™ã‚‹ (Surprise!)
    actual_next = initial_latent + torch.randn_like(initial_latent) * 3.0 
    
    surprise = meta_snn.evaluate_surprise(predicted_next, actual_next)
    print(f"   Surprise Value: {surprise:.4f}")
    
    if surprise > 0.5:
        print("   ðŸ§  Brain detects high surprise! Updating World Model weights (Simulated).")

    print("\nðŸŽ‰ Demo Completed.")

if __name__ == "__main__":
    main()