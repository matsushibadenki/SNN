# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: scripts/demos/systems/run_multimodal_vlm_demo.py
# æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: Multimodal VLM Training & Inference Demo (Log Fix)
# ç›®çš„ãƒ»å†…å®¹:
#   SpikingVLM ã¨ MultimodalTrainer ã‚’çµ±åˆã—ã€ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦
#   å­¦ç¿’ãƒ«ãƒ¼ãƒ—ãŒæ­£å¸¸ã«å›ã‚‹ã“ã¨ã€ãŠã‚ˆã³æ¨è«–ï¼ˆã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆï¼‰ãŒå¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’å®Ÿè¨¼ã™ã‚‹ã€‚
#   [Fix] ãƒ­ã‚°ãŒè¡¨ç¤ºã•ã‚Œãªã„å•é¡Œã‚’ä¿®æ­£ (force=True)ã€‚

from snn_research.training.trainers.multimodal_trainer import MultimodalVLMTrainer
from snn_research.core.architecture_registry import ArchitectureRegistry
import os
import sys
import torch
import logging
from torch.utils.data import DataLoader, Dataset

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã¸ã®ãƒ‘ã‚¹è¿½åŠ 
sys.path.append(os.path.join(os.path.dirname(__file__), "../../../"))

# ãƒ­ã‚°è¨­å®š: force=True ã§æ—¢å­˜ã®è¨­å®šã‚’ä¸Šæ›¸ãã—ã€ç¢ºå®Ÿã«è¡¨ç¤ºã•ã›ã‚‹
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    force=True
)
logger = logging.getLogger(__name__)


class DummyMultimodalDataset(Dataset):
    """
    ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€‚
    ãƒ©ãƒ³ãƒ€ãƒ ãªç”»åƒã¨ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’è¿”ã™ã€‚
    """

    def __init__(self, size: int = 100, seq_len: int = 16, img_size: int = 32):
        self.size = size
        self.seq_len = seq_len
        self.img_size = img_size
        self.vocab_size = 1000  # Demoç”¨

    def __len__(self):
        return self.size

    def __getitem__(self, idx):
        # Random Image: [3, 32, 32]
        image = torch.randn(3, self.img_size, self.img_size)
        # Random Text: [SeqLen]
        text = torch.randint(1, self.vocab_size, (self.seq_len,))
        # Labels (Same as text for simple autoregressive test)
        labels = text.clone()

        return {
            "images": image,
            "input_ids": text,
            "labels": labels
        }


def run_demo():
    logger.info("ğŸ¬ Starting Multimodal VLM Demo...")

    # 1. Configuration
    device = "cuda" if torch.cuda.is_available() else "cpu"
    vocab_size = 1000
    img_size = 32
    time_steps = 4  # SNN Time Steps

    config = {
        "vision_config": {
            "type": "cnn",
            "hidden_dim": 128,
            "img_size": img_size,
            "time_steps": time_steps,
            "neuron": {"type": "lif"}
        },
        "text_config": {
            "d_model": 128,
            "vocab_size": vocab_size,
            "num_layers": 2,
            "time_steps": time_steps
        },
        "projection_dim": 128,
        "use_bitnet": False  # Demoã§ã¯è»½é‡åŒ–ã®ãŸã‚Falseã‚‚å¯
    }

    # 2. Build Model using Registry (Corrected Interface)

    full_config = {
        "vision_config": config["vision_config"],
        "language_config": config["text_config"],
        "projector_config": {"projection_dim": config["projection_dim"]},
        # ãƒ“ãƒ«ãƒ€ãŒæœŸå¾…ã™ã‚‹å½¢å¼ã«åˆã‚ã›ã‚‹
        "sensory_inputs": {"vision": config["vision_config"]},
        "use_bitnet": config["use_bitnet"]
    }

    logger.info("ğŸ—ï¸ Building SpikingVLM model via Registry...")
    try:
        model = ArchitectureRegistry.build(
            "spiking_vlm", full_config, vocab_size)
    except Exception as e:
        logger.error(f"Failed to build model via Registry: {e}")
        logger.info(
            "âš ï¸ Falling back to direct instantiation for demo purposes.")
        # ãƒ¬ã‚¸ã‚¹ãƒˆãƒªçµŒç”±ãŒå¤±æ•—ã—ãŸå ´åˆã®ç›´æ¥ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        from snn_research.models.transformer.spiking_vlm import SpikingVLM
        model = SpikingVLM(
            vocab_size=vocab_size,
            vision_config=config["vision_config"],
            text_config=config["text_config"],
            projection_dim=config["projection_dim"],
            use_bitnet=config["use_bitnet"]
        )

    model = model.to(device)
    logger.info(f"âœ… Model built successfully. Device: {device}")

    # 3. Prepare Data
    dataset = DummyMultimodalDataset(size=50, seq_len=16, img_size=img_size)
    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)

    # 4. Trainer Setup
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)
    trainer = MultimodalVLMTrainer(
        model=model,
        optimizer=optimizer,
        device=device,
        config={
            "lambda_align": 0.5,
            "lambda_gen": 1.0
        }
    )

    # 5. Training Loop
    epochs = 2
    logger.info(f"ğŸ”„ Starting training loop for {epochs} epochs...")

    for epoch in range(epochs):
        metrics = trainer.train_epoch(dataloader, epoch)
        logger.info(f"   Epoch {epoch} Metrics: {metrics}")

    # 6. Inference / Generation Test
    logger.info("ğŸ§ª Testing Caption Generation...")
    sample_img = torch.randn(1, 3, img_size, img_size).to(device)

    try:
        generated_ids = model.generate_caption(sample_img, max_len=10)
        logger.info(f"ğŸ–¼ï¸ Input Image Shape: {sample_img.shape}")
        logger.info(f"ğŸ“ Generated Token IDs: {generated_ids.cpu().tolist()}")
        logger.info("âœ… Generation executed successfully.")
    except Exception as e:
        logger.error(f"Generation failed: {e}")
        import traceback
        traceback.print_exc()

    logger.info("ğŸ‰ Demo completed!")


if __name__ == "__main__":
    run_demo()
