# „Éï„Ç°„Ç§„É´„Éë„Çπ: scripts/run_cross_modal_demo.py
# Title: Cross-Modal Association Demo [Optimized]
# Description:
#   Áô∫ÁÅ´Áéá„ÇíÁ¢∫‰øù„Åô„Çã„Åü„ÇÅ„ÅÆ„Éë„É©„É°„Éº„Çø„ÉÅ„É•„Éº„Éã„É≥„Ç∞ÈÅ©Áî®Ê∏à„Åø„ÄÇ

import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
import sys
import os

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from snn_research.core.networks.liquid_association_cortex import LiquidAssociationCortex
from snn_research.learning_rules.stdp import STDP
from snn_research.io.universal_encoder import UniversalSpikeEncoder

def run_demo():
    print("üß† --- Phase 9-10: Cross-Modal Association Demo (Hearing Colors) ---")
    
    # Ë®≠ÂÆö
    DEVICE = "cpu"
    TIME_STEPS = 50
    BATCH_SIZE = 1
    
    # „Éë„É©„É°„Éº„ÇøË™øÊï¥
    LEARNING_RATE = 0.02
    A_PLUS = 0.1
    EPOCHS = 30
    INPUT_SCALE = 20.0 # ÂÖ•Âäõ‰ø°Âè∑„ÇíÂº∑Âäõ„Å´Â¢óÂπÖ
    THRESHOLD = 0.5    # Áô∫ÁÅ´ÈñæÂÄ§„Çí‰∏ã„Åí„Çã
    
    # ÂÖ•ÂäõÊ¨°ÂÖÉ
    DIM_VISUAL = 100
    DIM_AUDIO = 50
    RESERVOIR_SIZE = 500
    
    # 1. ÂàùÊúüÂåñ
    stdp_rule = STDP(
        learning_rate=LEARNING_RATE,
        a_plus=A_PLUS,
        a_minus=0.05,
        tau_trace=20.0
    )
    
    lac = LiquidAssociationCortex(
        num_visual_inputs=DIM_VISUAL,
        num_audio_inputs=DIM_AUDIO,
        num_text_inputs=10,
        num_somato_inputs=10,
        reservoir_size=RESERVOIR_SIZE,
        sparsity=0.2, # Â∞ë„ÅóÂØÜ„Å´„Åô„Çã
        tau=5.0,      # ÊôÇÂÆöÊï∞„ÇíÈï∑„Åè„Åó„Å¶Ë®òÊÜ∂„Çí‰øù„Å°„ÇÑ„Åô„Åè„Åô„Çã
        threshold=THRESHOLD,
        input_scale=INPUT_SCALE,
        learning_rule=stdp_rule
    ).to(DEVICE)
    
    encoder = UniversalSpikeEncoder(time_steps=TIME_STEPS, device=DEVICE)
    
    print(f"‚úÖ System Initialized. Input Scale={INPUT_SCALE}, Threshold={THRESHOLD}")

    # 2. „Éë„Çø„Éº„É≥ÁîüÊàê
    torch.manual_seed(42)
    vis_A = torch.rand(BATCH_SIZE, DIM_VISUAL) > 0.7
    aud_A = torch.rand(BATCH_SIZE, DIM_AUDIO) > 0.7
    
    vis_A_spikes = encoder.encode(vis_A.float(), 'image', 'rate')
    aud_A_spikes = encoder.encode(aud_A.float(), 'audio', 'rate')
    
    # 3. „Éô„Éº„Çπ„É©„Ç§„É≥ÂèñÂæó
    print("\nüéß [Pre-Training] Testing Audio-only response...")
    lac.eval()
    lac.reset_state()
    
    pre_activity = []
    for t in range(TIME_STEPS):
        out = lac(visual_spikes=None, audio_spikes=aud_A_spikes[:, t, :])
        pre_activity.append(out.detach())
    pre_activity = torch.stack(pre_activity).squeeze(1)
    
    lac.reset_state()
    target_vis_activity = []
    for t in range(TIME_STEPS):
        # „Çø„Éº„Ç≤„ÉÉ„Éà„ÅØË¶ñË¶ö+Èü≥Â£∞ÂêåÊôÇÂÖ•ÂäõÊôÇ„ÅÆ„É™„Ç∂„Éº„ÉêÁä∂ÊÖã„Å®„Åô„ÇãÔºà„Çà„ÇäËá™ÁÑ∂„Å™ÈÄ£ÂêàÔºâ
        # „Åæ„Åü„ÅØÁ¥îÁ≤ã„Å™Ë¶ñË¶öÊÉ≥Ëµ∑„ÇíÁõÆÊåá„Åô„Å™„ÇâË¶ñË¶ö„ÅÆ„Åø„Åß„ÇÇËâØ„ÅÑ„Åå„ÄÅ„Åì„Åì„Åß„ÅØ„ÄåA„Å®„ÅÑ„ÅÜÊ¶ÇÂøµ„ÄçÂÖ®‰Ωì„Çí„Çø„Éº„Ç≤„ÉÉ„Éà„Å®„Åô„Çã
        out = lac(visual_spikes=vis_A_spikes[:, t, :], audio_spikes=aud_A_spikes[:, t, :])
        target_vis_activity.append(out.detach())
    target_vis_activity = torch.stack(target_vis_activity).squeeze(1)

    sim_pre = F.cosine_similarity(pre_activity.mean(dim=0).unsqueeze(0), target_vis_activity.mean(dim=0).unsqueeze(0))
    print(f"   Similarity to Target Concept (Before Learning): {sim_pre.item():.4f}")

    # 4. ÈÄ£ÂêàÂ≠¶Áøí„Éï„Çß„Éº„Ç∫
    print(f"\nüîó [Training] Associating Visual-A with Audio-A ({EPOCHS} Epochs)...")
    lac.train()
    
    for epoch in range(EPOCHS):
        lac.reset_state()
        total_spikes = 0
        for t in range(TIME_STEPS):
            out = lac(
                visual_spikes=vis_A_spikes[:, t, :], 
                audio_spikes=aud_A_spikes[:, t, :]
            )
            total_spikes += out.sum().item()
        
        if (epoch+1) % 5 == 0:
            print(f"   Epoch {epoch+1}: Total Reservoir Spikes: {total_spikes}")

    # 5. ÊÉ≥Ëµ∑„ÉÜ„Çπ„Éà
    print("\nüé® [Post-Training] Testing 'Hearing Colors' (Audio-only Input)...")
    lac.eval()
    lac.reset_state()
    
    post_activity = []
    for t in range(TIME_STEPS):
        # Èü≥Â£∞„ÅÆ„ÅøÂÖ•Âäõ
        out = lac(visual_spikes=None, audio_spikes=aud_A_spikes[:, t, :])
        post_activity.append(out.detach())
    post_activity = torch.stack(post_activity).squeeze(1)
    
    # 6. ÁµêÊûúÊ§úË®º
    sim_post = F.cosine_similarity(post_activity.mean(dim=0).unsqueeze(0), target_vis_activity.mean(dim=0).unsqueeze(0))
    
    print(f"   Similarity to Target Concept (After Learning):  {sim_post.item():.4f}")
    
    improvement = sim_post.item() - sim_pre.item()
    print(f"\nüìà Association Improvement: {improvement:+.4f}")
    
    # Âà§ÂÆöÂü∫Ê∫ñ
    if improvement > 0.05:
        print("‚úÖ SUCCESS: Cross-modal association detected.")
    else:
        print("‚ö†Ô∏è WARNING: Association weak.")

    # ÂèØË¶ñÂåñ
    try:
        os.makedirs("results", exist_ok=True)
        plt.figure(figsize=(12, 6))
        
        plt.subplot(1, 3, 1)
        plt.imshow(target_vis_activity.T.numpy(), aspect='auto', interpolation='nearest', cmap='Greys')
        plt.title("Target Concept Activity")
        plt.xlabel("Time")
        plt.ylabel("Neurons")
        
        plt.subplot(1, 3, 2)
        plt.imshow(pre_activity.T.numpy(), aspect='auto', interpolation='nearest', cmap='Greys')
        plt.title(f"Audio Response (Pre)\nSim: {sim_pre.item():.2f}")
        plt.axis('off')
        
        plt.subplot(1, 3, 3)
        plt.imshow(post_activity.T.numpy(), aspect='auto', interpolation='nearest', cmap='Greys')
        plt.title(f"Audio Response (Post)\nSim: {sim_post.item():.2f}")
        plt.axis('off')
        
        plt.tight_layout()
        plt.savefig("workspace/results/cross_modal_demo.png")
        print("   Raster plot saved to results/cross_modal_demo.png")
    except Exception as e:
        print(f"   (Plotting skipped: {e})")

if __name__ == "__main__":
    run_demo()