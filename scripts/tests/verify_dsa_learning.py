# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: scripts/tests/verify_dsa_learning.py
# æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: DSAå­¦ç¿’èƒ½åŠ›æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# ç›®çš„ãƒ»å†…å®¹:
#   å®Ÿè£…ã—ãŸ DSASpikingTransformer ãŒå®Ÿéš›ã«å­¦ç¿’ï¼ˆLossã®ä½ä¸‹ã¨ç²¾åº¦ã®å‘ä¸Šï¼‰
#   å¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’æ¤œè¨¼ã™ã‚‹ãŸã‚ã®è»½é‡ãªå­¦ç¿’ãƒ«ãƒ¼ãƒ—ã€‚
#   ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: ãƒ©ãƒ³ãƒ€ãƒ ãªãƒ™ã‚¯ãƒˆãƒ«åˆ—ã®åˆ†é¡ï¼ˆãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ï¼‰

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import sys
import os
import matplotlib.pyplot as plt

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
# scripts/tests/ ã‹ã‚‰è¦‹ã¦ ../../ ãŒãƒ«ãƒ¼ãƒˆ(snn_researchãŒã‚ã‚‹å ´æ‰€)ã¨æƒ³å®š
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../')))

# ãƒ‘ã‚¹èª¿æ•´ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ (scripts/ã«snn_researchãŒã‚ã‚‹å ´åˆãªã©)
try:
    from snn_research.models.transformer.dsa_transformer import DSASpikingTransformer
except ImportError:
    sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
    from snn_research.models.transformer.dsa_transformer import DSASpikingTransformer

def generate_dummy_data(num_samples=200, seq_len=20, input_dim=16, num_classes=2):
    """
    ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆã€‚
    ã‚¯ãƒ©ã‚¹0: å‰åŠã®å€¤ãŒå¤§ãã„å‚¾å‘
    ã‚¯ãƒ©ã‚¹1: å¾ŒåŠã®å€¤ãŒå¤§ãã„å‚¾å‘
    ã¨ã„ã†å˜ç´”ãªæ™‚é–“çš„ç‰¹å¾´ã‚’æŒãŸã›ã‚‹ã€‚
    """
    X = torch.randn(num_samples, seq_len, input_dim)
    Y = torch.zeros(num_samples, dtype=torch.long)
    
    for i in range(num_samples):
        if torch.rand(1).item() > 0.5:
            # Class 1: Add bias to second half
            X[i, seq_len//2:, :] += 1.0
            Y[i] = 1
        else:
            # Class 0: Add bias to first half
            X[i, :seq_len//2, :] += 1.0
            Y[i] = 0
            
    return X, Y

def main():
    print("ğŸš€ Starting SNN-DSA Learning Verification...")
    
    # è¨­å®š
    DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    BATCH_SIZE = 16
    EPOCHS = 10 # å‹•ä½œç¢ºèªç”¨ãªã®ã§çŸ­ã
    LR = 1e-3
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (Phase 8-2: Sparse Attention)
    # ä¿®æ­£: SpikingDSATransformer.__init__ ã®å¼•æ•°ã«åˆã‚ã›ã¦ä¿®æ­£
    model_config = {
        'input_dim': 16,
        'num_classes': 2,       # ä¿®æ­£: output_dim -> num_classes
        'd_model': 32,
        'num_heads': 4,
        'num_layers': 2,
        'dim_feedforward': 64,  # è¿½åŠ : å¿…é ˆå¼•æ•°
        'time_window': 20,      # ä¿®æ­£: max_len -> time_window
        # 'top_k': 5,           # å‰Šé™¤: ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«å®šç¾©ã«å­˜åœ¨ã—ãªã„å¼•æ•°
    }
    
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    X, Y = generate_dummy_data(num_samples=200, seq_len=20, input_dim=16)
    dataset = TensorDataset(X, Y)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)
    
    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    try:
        model = DSASpikingTransformer(**model_config).to(DEVICE)
    except TypeError as e:
        print(f"âŒ Model initialization failed: {e}")
        return

    optimizer = optim.AdamW(model.parameters(), lr=LR)
    criterion = nn.CrossEntropyLoss()
    
    print(f"Model initialized on {DEVICE}. Config: {model_config}")
    print(f"Dataset: {len(dataset)} samples.")
    
    # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
    loss_history = []
    acc_history = []
    
    model.train()
    for epoch in range(EPOCHS):
        total_loss = 0
        correct = 0
        total = 0
        
        for batch_x, batch_y in dataloader:
            batch_x, batch_y = batch_x.to(DEVICE), batch_y.to(DEVICE)
            
            # SNNçŠ¶æ…‹ãƒªã‚»ãƒƒãƒˆ
            # ãƒ¢ãƒ‡ãƒ«ã«reset_stateãŒå®Ÿè£…ã•ã‚Œã¦ã„ã‚‹å ´åˆã®ã¿å‘¼ã³å‡ºã™
            if hasattr(model, 'reset_state'):
                model.reset_state()
            
            # Forward
            outputs = model(batch_x)
            loss = criterion(outputs, batch_y)
            
            # Backward
            optimizer.zero_grad()
            loss.backward()
            
            # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚° (SNNå­¦ç¿’å®‰å®šåŒ–ã®ãŸã‚)
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            
            optimizer.step()
            
            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()
            
        avg_loss = total_loss / len(dataloader)
        accuracy = 100 * correct / total
        loss_history.append(avg_loss)
        acc_history.append(accuracy)
        
        print(f"Epoch [{epoch+1}/{EPOCHS}] Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%")
        
    # æ¤œè¨¼çµæœã®åˆ¤å®š
    final_acc = acc_history[-1]
    print("\nğŸ“Š Verification Result:")
    if final_acc > 80.0:
        print(f"âœ… PASSED: Model achieved {final_acc:.2f}% accuracy (Target > 80%).")
        print("   DSA mechanism successfully propagated gradients and learned temporal patterns.")
    else:
        print(f"âš ï¸  WARNING: Final accuracy {final_acc:.2f}% is low. Parameter tuning might be needed.")

    # å¯è¦–åŒ–ç”»åƒã‚’ä¿å­˜ (ã‚ªãƒ—ã‚·ãƒ§ãƒ³)
    try:
        os.makedirs("results", exist_ok=True)
        plt.figure(figsize=(10, 4))
        plt.subplot(1, 2, 1)
        plt.plot(loss_history, label='Loss')
        plt.title('Training Loss')
        plt.xlabel('Epoch')
        plt.legend()
        
        plt.subplot(1, 2, 2)
        plt.plot(acc_history, label='Accuracy', color='orange')
        plt.title('Training Accuracy')
        plt.xlabel('Epoch')
        plt.legend()
        
        # ä¿å­˜ãƒ‘ã‚¹ã®èª¿æ•´
        save_path = "results/dsa_verification_plot.png"
        if os.path.exists("workspace"):
            save_path = "workspace/results/dsa_verification_plot.png"
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            
        plt.savefig(save_path)
        print(f"   Plot saved to {save_path}")
    except Exception as e:
        print(f"   (Plotting skipped: {e})")

if __name__ == "__main__":
    main()