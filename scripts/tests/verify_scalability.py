# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: scripts/verify_scalability.py
# Title: Scalability Verification Script
# æ©Ÿèƒ½: ãƒ¢ãƒ‡ãƒ«è¦æ¨¡ã‚’æ‹¡å¤§ã—ãŸéš›ã®å½±éŸ¿(ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã€ãƒ¡ãƒ¢ãƒªã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°)ã‚’è¨ˆæ¸¬ã™ã‚‹ã€‚

import torch
import time
import os
import sys
import yaml
import psutil

# ãƒ‘ã‚¹è¨­å®š
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from snn_research.models.transformer.dsa_transformer import SpikingDSATransformer

def get_memory_usage():
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024  # MB

def verify_scalability(config_path):
    print(f"ğŸš€ Loading Config: {config_path}")
    
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    device = "cpu"
    if torch.cuda.is_available():
        device = "cuda"
    elif torch.backends.mps.is_available():
        device = "mps"
    
    print(f"   Device: {device}")
    
    # 1. ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ & ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°è¨ˆæ¸¬
    print("\nğŸ“¦ Building Scaled Model...")
    mem_before = get_memory_usage()
    
    # Configã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æŠ½å‡º
    m_conf = config['model']
    model = SpikingDSATransformer(
        input_dim=m_conf['d_model'], # å…¥åŠ›æ¬¡å…ƒã‚‚åˆã‚ã›ã‚‹
        d_model=m_conf['d_model'],
        num_heads=m_conf['num_heads'],
        num_layers=m_conf['num_layers'],
        dim_feedforward=m_conf['dim_feedforward'],
        time_window=m_conf['time_window'],
        use_bitnet=m_conf.get('use_bitnet', False)
    ).to(device)
    
    mem_after = get_memory_usage()
    
    param_count = sum(p.numel() for p in model.parameters())
    print("   âœ… Model Built Successfully.")
    print(f"   ğŸ“Š Total Parameters: {param_count:,}")
    print(f"   ğŸ’¾ Memory Footprint: {mem_after - mem_before:.2f} MB")
    
    # 2. æ¨è«–ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¨ˆæ¸¬ (T=1 Real-time Mode)
    print("\nâš¡ Measuring Inference Latency (T=1)...")
    model.eval()
    
    # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
    dummy_input = torch.randn(1, 1, m_conf['d_model']).to(device)
    for _ in range(10):
        with torch.no_grad():
            _ = model(dummy_input)
            
    # è¨ˆæ¸¬
    start_time = time.time()
    steps = 100
    for _ in range(steps):
        with torch.no_grad():
            _ = model(dummy_input)
    
    if device == "cuda":
        torch.cuda.synchronize()
        
    avg_latency = (time.time() - start_time) * 1000 / steps
    print(f"   â±ï¸  Average Latency: {avg_latency:.2f} ms")
    
    # åˆ¤å®š
    target_latency = 10.0 # ms
    if avg_latency < target_latency:
        print(f"\nâœ… SUCCESS: Scaled model is FAST enough! (< {target_latency}ms)")
    else:
        print("\nâš ï¸  WARNING: Latency exceeds target. Optimization needed.")

if __name__ == "__main__":
    config_path = "configs/models/phase2_scaled.yaml"
    verify_scalability(config_path)