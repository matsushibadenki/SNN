# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: scripts/experiments/applications/run_web_learning.py
# ã‚¿ã‚¤ãƒˆãƒ«: è‡ªå¾‹å‹ Webå­¦ç¿’ãƒ©ãƒ³ãƒŠãƒ¼ (Autonomous Web Learning)
# ç›®çš„: Webã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚’åˆ©ç”¨ã—ã¦è‡ªå¾‹çš„ã«ãƒˆãƒ”ãƒƒã‚¯ã‚’æ¢ç´¢ã—ã€ç¶™ç¶šçš„ã«çŸ¥è­˜è’¸ç•™å­¦ç¿’ã‚’è¡Œã†ç’°å¢ƒã‚’æä¾›ã™ã‚‹ã€‚
# å†…å®¹:
#   - è‡ªå¾‹å­¦ç¿’ãƒ«ãƒ¼ãƒ—ï¼ˆCuriosity Loopï¼‰ã®å®Ÿè£…
#   - å­¦ç¿’ã—ãŸå†…å®¹ã‹ã‚‰æ¬¡ã®èˆˆå‘³ï¼ˆãƒˆãƒ”ãƒƒã‚¯ï¼‰ã‚’ç”Ÿæˆã™ã‚‹æ©Ÿèƒ½
#   - ç¶™ç¶šçš„ãªãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã¨è©•ä¾¡

# â—¾ï¸ DistillationTrainer ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from snn_research.training.trainers import DistillationTrainer
from app.containers import TrainingContainer  # DIã‚³ãƒ³ãƒ†ãƒŠã‚’åˆ©ç”¨
from snn_research.distillation.knowledge_distillation_manager import KnowledgeDistillationManager
from app.services.web_crawler import WebCrawler
from omegaconf import DictConfig
from torch.optim.lr_scheduler import LRScheduler
import torch
from typing import Optional, Any, Dict, List
import asyncio
import argparse
import sys
import os
import random

# ------------------------------------------------------------------------------
# [Auto-inserted by fix_script_paths.py]
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’sys.pathã«è¿½åŠ 
# ------------------------------------------------------------------------------
project_root = os.path.abspath(
    os.path.join(os.path.dirname(__file__), "../../../.."))
if project_root not in sys.path:
    sys.path.insert(0, project_root)
# ------------------------------------------------------------------------------


def extract_next_topics(data_path: str, current_topic: str) -> List[str]:
    """
    åé›†ã—ãŸãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€æ¬¡ã«å­¦ç¿’ã™ã¹ããƒˆãƒ”ãƒƒã‚¯ï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰ã‚’æŠ½å‡ºã™ã‚‹ã€‚
    ç°¡æ˜“çš„ãªã€Œå¥½å¥‡å¿ƒã€ã®å®Ÿè£…ã€‚
    """
    if not os.path.exists(data_path):
        return []

    with open(data_path, 'r', encoding='utf-8') as f:
        text = f.read().lower()

    # ç°¡æ˜“çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆæœ¬æ¥ã¯NLPãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã†ã¹ãç®‡æ‰€ï¼‰
    # ã“ã“ã§ã¯ã€AI/è„³ç§‘å­¦ã«é–¢é€£ã—ãã†ãªé‡è¦å˜èªã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—ã—ã€å‡ºç¾æœ‰ç„¡ã§åˆ¤æ–­
    potential_keywords = [
        "synapse", "plasticity", "energy", "quantum", "consciousness",
        "robotics", "evolution", "chaos", "entropy", "optimization",
        "transformer", "neuromorphic", "spiking", "dopamine"
    ]

    found_topics = []
    for kw in potential_keywords:
        if kw in text and kw != current_topic.lower():
            found_topics.append(kw)

    # è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ãƒ©ãƒ³ãƒ€ãƒ ãªæœªçŸ¥èªï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰
    if not found_topics:
        found_topics = ["unknown_phenomenon", "future_tech", "deep_brain"]

    return list(set(found_topics))


def setup_distillation_manager(container: TrainingContainer) -> KnowledgeDistillationManager:
    """DIã‚³ãƒ³ãƒ†ãƒŠã‹ã‚‰å­¦ç¿’ãƒãƒãƒ¼ã‚¸ãƒ£ã‚’æ§‹ç¯‰ã—ã¦è¿”ã™ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°"""
    device: str = container.device()
    student_model: torch.nn.Module = container.snn_model()
    optimizer: torch.optim.Optimizer = container.optimizer(
        params=student_model.parameters())

    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«åŸºã¥ãã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’æ¡ä»¶ä»˜ãã§ä½œæˆ
    scheduler: Optional[LRScheduler] = container.scheduler(
        optimizer=optimizer) if container.config.training.gradient_based.use_scheduler() else None

    # Trainerã®æ§‹ç¯‰
    distillation_trainer: "DistillationTrainer" = container.distillation_trainer(
        model=student_model,
        optimizer=optimizer,
        scheduler=scheduler,
        device=device,
        rank=-1
    )

    # Managerã®æ§‹ç¯‰
    manager_config: DictConfig = container.config()
    distillation_manager = KnowledgeDistillationManager(
        student_model=student_model,
        # type: ignore[arg-type]
        trainer=distillation_trainer,
        teacher_model_name=container.config.training.gradient_based.distillation.teacher_model(),
        tokenizer_name=container.config.data.tokenizer_name(),
        model_registry=container.model_registry(),
        device=device,
        config=manager_config
    )
    return distillation_manager


async def run_autonomous_loop(initial_topic: str, start_url: str, max_cycles: int):
    """
    è‡ªå¾‹çš„ãªå­¦ç¿’ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
    Crawl -> Learn -> Generate Next Topic -> Repeat
    """
    print("\n" + "="*50)
    print(f" ğŸš€ Starting Autonomous Learning Loop (Max Cycles: {max_cycles})")
    print("="*50)

    current_topic = initial_topic
    current_url = start_url
    known_topics = set([initial_topic])

    # DIã‚³ãƒ³ãƒ†ãƒŠã®åˆæœŸåŒ–ï¼ˆãƒ«ãƒ¼ãƒ—å¤–ã§ãƒ¢ãƒ‡ãƒ«ã‚’ä¿æŒã™ã‚‹å ´åˆã¯ã“ã“ã§è¡Œã†ï¼‰
    container = TrainingContainer()
    container.config.from_yaml("configs/templates/base_config.yaml")
    container.config.from_yaml("configs/models/medium.yaml")

    # å­¦ç¿’ãƒãƒãƒ¼ã‚¸ãƒ£ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    distillation_manager = setup_distillation_manager(container)
    student_config_dict: Dict[str, Any] = container.config.model.to_dict()
    crawler = WebCrawler()

    for cycle in range(1, max_cycles + 1):
        print(f"\nğŸŒ€ [Cycle {cycle}/{max_cycles}] Topic: '{current_topic}'")

        # --- 1. Crawl ---
        crawled_data_path = crawler.crawl(
            start_url=current_url,
            max_pages=3,
            topic_filter=current_topic
        )

        if not os.path.exists(crawled_data_path) or os.path.getsize(crawled_data_path) == 0:
            print("âŒ Crawling failed or empty. Skipping cycle.")
            continue

        # --- 2. Learn ---
        print(f"    ğŸ§  Learning from gathered data about '{current_topic}'...")
        await distillation_manager.run_on_demand_pipeline(
            task_description=current_topic,
            unlabeled_data_path=crawled_data_path,
            force_retrain=True,
            student_config=student_config_dict
        )

        # --- 3. Plan Next ---
        # åé›†ã—ãŸãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ¬¡ã®èˆˆå‘³ï¼ˆãƒˆãƒ”ãƒƒã‚¯ï¼‰ã‚’è¦‹ã¤ã‘ã‚‹
        next_candidates = extract_next_topics(crawled_data_path, current_topic)

        # æ—¢çŸ¥ã®ãƒˆãƒ”ãƒƒã‚¯ã‚’é™¤å¤–
        new_candidates = [t for t in next_candidates if t not in known_topics]

        if new_candidates:
            next_topic = random.choice(new_candidates)
            print(
                f"    ğŸ’¡ Curiosity triggered! Found interesting new topic: '{next_topic}'")
        else:
            # æ–°ã—ã„ã‚‚ã®ãŒè¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°ã€å°‘ã—è¦–ç‚¹ã‚’å¤‰ãˆã‚‹
            next_topic = "general_intelligence"
            print(
                f"    ğŸ¤” No new topics found. Returning to base concept: '{next_topic}'")

        known_topics.add(next_topic)
        current_topic = next_topic

        # URLã‚‚å‹•çš„ã«å¤‰ãˆãŸã„ãŒã€ãƒ‡ãƒ¢ã§ã¯Wikipediaç­‰ã®æ¤œç´¢URLã‚’æ¨¡å€£ã™ã‚‹ã‹ã€ãƒ¢ãƒƒã‚¯URLã‚’ä½¿ç”¨
        current_url = f"https://en.wikipedia.org/wiki/{current_topic.replace(' ', '_')}"

        # ã‚µã‚¤ã‚¯ãƒ«é–“ã®ä¼‘æ†©
        print("    ğŸ’¤ Sleeping for consolidation (simulated)...")
        await asyncio.sleep(2)

    print("\n" + "="*50)
    print(" ğŸ‰ Autonomous Learning Session Completed.")
    print(f" ğŸ“š Learned Topics: {known_topics}")
    print("="*50)


def main() -> None:
    """
    Webã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã¨ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’é€£æºã•ã›ã€
    å…¨è‡ªå‹•ã§å­¦ç¿’ç’°å¢ƒã‚’å›ã™ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚
    """
    parser = argparse.ArgumentParser(
        description="Autonomous Web Learning Framework",
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument(
        "--topic",
        type=str,
        default="Neuromorphic Computing",
        help="æœ€åˆã®å­¦ç¿’ãƒˆãƒ”ãƒƒã‚¯ã€‚"
    )
    parser.add_argument(
        "--start_url",
        type=str,
        default="https://en.wikipedia.org/wiki/Neuromorphic_engineering",
        help="é–‹å§‹URLã€‚"
    )
    parser.add_argument(
        "--autonomous",
        action="store_true",
        help="è‡ªå¾‹å­¦ç¿’ãƒ«ãƒ¼ãƒ—ãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹ã«ã™ã‚‹ï¼ˆãƒˆãƒ”ãƒƒã‚¯ã‚’è‡ªå‹•é·ç§»ï¼‰ã€‚"
    )
    parser.add_argument(
        "--cycles",
        type=int,
        default=3,
        help="è‡ªå¾‹ãƒ¢ãƒ¼ãƒ‰æ™‚ã®æœ€å¤§ã‚µã‚¤ã‚¯ãƒ«æ•°ã€‚"
    )

    args: argparse.Namespace = parser.parse_args()

    if args.autonomous:
        # è‡ªå¾‹ãƒ«ãƒ¼ãƒ—ãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œ
        asyncio.run(run_autonomous_loop(
            args.topic, args.start_url, args.cycles))
    else:
        # å˜ç™ºå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ï¼ˆå¾“æ¥ã®æŒ™å‹•ï¼‰
        print("\n" + "="*20 + " ğŸŒ Step 1: Web Crawling (Single Shot) " + "="*20)
        crawler = WebCrawler()
        crawled_data_path: str = crawler.crawl(
            start_url=args.start_url, max_pages=5, topic_filter=args.topic)

        if not os.path.exists(crawled_data_path) or os.path.getsize(crawled_data_path) == 0:
            print("âŒ ãƒ‡ãƒ¼ã‚¿ãŒåé›†ã§ããªã‹ã£ãŸãŸã‚ã€å­¦ç¿’ã‚’ä¸­æ­¢ã—ã¾ã™ã€‚")
            return

        print("\n" + "="*20 + " ğŸ§  Step 2: On-demand Learning " + "="*20)
        container = TrainingContainer()
        container.config.from_yaml("configs/templates/base_config.yaml")
        container.config.from_yaml("configs/models/medium.yaml")

        manager = setup_distillation_manager(container)
        student_config_dict: Dict[str, Any] = container.config.model.to_dict()

        asyncio.run(manager.run_on_demand_pipeline(
            task_description=args.topic,
            unlabeled_data_path=crawled_data_path,
            force_retrain=True,
            student_config=student_config_dict
        ))
        print(f"\nğŸ‰ å­¦ç¿’å®Œäº†: ãƒˆãƒ”ãƒƒã‚¯ã€Œ{args.topic}ã€")


if __name__ == "__main__":
    main()
