# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: scripts/run_distillation_experiment.py
# Title: Chain-of-Thought (CoT) Distillation Runner
# Description:
#   ROADMAP v16.5 å®Ÿè£…ã€‚
#   System 2 (ReasoningEngine) ã®ã€Œæ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã€ã‚’æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ç”Ÿæˆã—ã€
#   System 1 (SFormer Small) ã«è’¸ç•™ã™ã‚‹å®Ÿé¨“ã‚’è¡Œã†ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€‚
#   ã“ã‚Œã«ã‚ˆã‚Šã€æ¨è«–æ™‚ã®è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å¤§å¹…ã«å‰Šæ¸›ï¼ˆçœã‚¨ãƒåŒ–ï¼‰ã™ã‚‹ã€‚

from snn_research.distillation.pipeline import AdvancedDistillationPipeline
from snn_research.cognitive_architecture.reasoning_engine import ReasoningEngine
from snn_research.cognitive_architecture.astrocyte_network import AstrocyteNetwork
from snn_research.core.snn_core import SNNCore
import sys
import os
import torch
import logging
import argparse
from transformers import AutoTokenizer  # type: ignore

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®è¨­å®š
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../"))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger("DistillationExp")


def main():
    parser = argparse.ArgumentParser(
        description="Run CoT Distillation Experiment")
    parser.add_argument("--epochs", type=int, default=3)
    parser.add_argument("--batch_size", type=int, default=2)
    parser.add_argument("--num_prompts", type=int, default=10)
    args = parser.parse_args()

    device = "cuda" if torch.cuda.is_available(
    ) else "mps" if torch.backends.mps.is_available() else "cpu"
    logger.info(f"ğŸš€ Starting Distillation Experiment on {device}")

    # 1. ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®æº–å‚™
    logger.info("1. Initializing Teacher (System 2) & Student (System 1)...")

    # Tokenizer (GPT-2 for demo)
    try:
        tokenizer = AutoTokenizer.from_pretrained("gpt2")
        tokenizer.pad_token = tokenizer.eos_token
    except Exception as e:
        logger.error(f"Tokenizer load failed: {e}")
        return

    # --- Teacher: Reasoning Engine (Large SFormer) ---
    # â€»ãƒ‡ãƒ¢ç”¨ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯å°ã•ã‚ã«è¨­å®šã—ã¦ã„ã¾ã™
    teacher_config = {
        "architecture_type": "sformer",
        "d_model": 128,
        "num_layers": 4,
        "n_head": 4,
        "neuron_config": {"base_threshold": 1.0}
    }
    teacher_core = SNNCore(
        teacher_config, vocab_size=tokenizer.vocab_size).to(device)
    astrocyte = AstrocyteNetwork()

    # æ•™å¸«ã¯ã€Œè€ƒãˆã‚‹ã€ã“ã¨ãŒã§ãã‚‹
    teacher_engine = ReasoningEngine(
        generative_model=teacher_core.model,  # type: ignore
        astrocyte=astrocyte,
        device=device,
        num_thinking_paths=2,  # è¤‡æ•°ã®æ€è€ƒãƒ‘ã‚¹ã‚’è©¦ã™
        max_thinking_steps=10
    )

    # --- Student: Lightweight SFormer (Small) ---
    student_config = {
        "architecture_type": "sformer",
        "d_model": 64,  # æ•™å¸«ã‚ˆã‚Šå°ã•ã„
        "num_layers": 2,  # å±¤ã‚‚å°‘ãªã„
        "n_head": 2,
        "neuron_config": {"base_threshold": 0.5}  # ç™ºç«ã—ã‚„ã™ã„
    }
    student_core = SNNCore(
        student_config, vocab_size=tokenizer.vocab_size).to(device)

    # 2. ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰
    pipeline = AdvancedDistillationPipeline(
        student_model=student_core.model,  # type: ignore
        device=device,
        tokenizer=tokenizer
    )

    # 3. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æº–å‚™ (ã‚¿ã‚¹ã‚¯ä¾‹: è«–ç†æ¨è«–)
    logger.info("2. Generating Reasoning Traces (Rollout)...")
    prompts = [
        "Problem: If A is bigger than B, and B is bigger than C, is A bigger than C? Answer:",
        "Problem: What happens if you drop a glass on concrete? Answer:",
        "Problem: 2 + 2 * 2 = ? Answer:",
        # ... æœ¬æ¥ã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ãƒ­ãƒ¼ãƒ‰
    ] * (args.num_prompts // 3 + 1)
    prompts = prompts[:args.num_prompts]

    # 4. è’¸ç•™å®Ÿè¡Œ (CoT Distillation)
    # TeacherãŒæ€è€ƒã‚’è¡Œã„ã€ãã®ãƒ—ãƒ­ã‚»ã‚¹ã‚’StudentãŒå­¦ç¿’ã™ã‚‹
    logger.info("3. Running CoT Distillation...")
    pipeline.run_cot_distillation(
        reasoning_engine=teacher_engine,
        prompts=prompts,
        epochs=args.epochs,
        batch_size=args.batch_size,
        lr=1e-4
    )

    # 5. è©•ä¾¡ (Before/Afterã®æ¯”è¼ƒã¯æœ¬æ¥å¿…è¦ã ãŒã€ã“ã“ã§ã¯å‹•ä½œç¢ºèª)
    logger.info("4. Verifying Student Capability...")
    test_input = "Problem: 2 + 2 * 2 = ? Answer:"
    input_ids = tokenizer.encode(test_input, return_tensors='pt').to(device)

    student_core.model.eval()
    with torch.no_grad():
        # SFormer.generate ãŒå®Ÿè£…ã•ã‚Œã¦ã„ã‚‹å‰æ
        if hasattr(student_core.model, 'generate'):
            output_ids = student_core.model.generate(
                input_ids, max_length=20, do_sample=False)  # type: ignore
            output_text = tokenizer.decode(
                output_ids[0], skip_special_tokens=True)
            logger.info(f"ğŸ“ Student Output: {output_text}")
        else:
            logger.warning("Student model does not support generation.")

    logger.info("âœ… Distillation Experiment Completed.")


if __name__ == "__main__":
    main()
