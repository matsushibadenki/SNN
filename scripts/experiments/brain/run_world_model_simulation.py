# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: scripts/experiments/brain/run_world_model_simulation.py
# æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: Multimodal World Model Simulation (Type Fixed)
# ä¿®æ­£å†…å®¹: generate_synthetic_world_data ã®æˆ»ã‚Šå€¤å‹ãƒ’ãƒ³ãƒˆã‚’ä¿®æ­£ã€‚

from snn_research.utils.efficiency_profiler import EfficiencyProfiler
from snn_research.core.architecture_registry import ArchitectureRegistry
import os
import sys
import torch
import torch.nn.functional as F
import logging
from typing import Dict, Any

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.getcwd())


# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("WorldModelSim")


def generate_synthetic_world_data(batch_size: int, seq_len: int, config: Dict) -> Dict[str, Any]:
    """
    ç‰©ç†æ³•å‰‡ã«å¾“ã†ã‚ˆã†ãªæ“¬ä¼¼çš„ãªæ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã™ã‚‹ã€‚
    ä¾‹: è¦–è¦šä¸Šã®ãƒœãƒ¼ãƒ«ãŒç§»å‹•ã™ã‚‹ã¨ã€ç‰¹å®šã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§è§¦è¦š/éŸ³ãŒåå¿œã™ã‚‹ã€‚

    Returns:
        Dict containing 'sensory' (dict of tensors) and 'actions' (tensor)
    """
    device = config['device']

    # 1. Action: ãƒ©ãƒ³ãƒ€ãƒ ãªç§»å‹•æŒ‡ä»¤ (dx, dy)
    actions = torch.randn(batch_size, seq_len,
                          config['action_dim'], device=device)

    # 2. Vision: å˜ç´”ãªç§»å‹•ã™ã‚‹ãƒ‰ãƒƒãƒˆã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ (ç°¡æ˜“ç‰ˆ)
    # ã“ã“ã§ã¯å®Œå…¨ãªç‰©ç†ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ãªãã€è¡Œå‹•ã«ç›¸é–¢ã—ãŸãƒ©ãƒ³ãƒ€ãƒ ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨
    vision_dim = config['sensory_configs']['vision']
    # è¡Œå‹•ã®è“„ç©ï¼ˆä½ç½®ï¼‰ã«å¿œã˜ã¦å¤‰åŒ–ã™ã‚‹æ³¢å½¢
    position = torch.cumsum(actions[:, :, 0], dim=1).unsqueeze(-1)  # (B, T, 1)
    # sinæ³¢ã‚’ä½¿ã£ã¦ä½ç½®æƒ…å ±ã‚’é«˜æ¬¡å…ƒãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    freqs = torch.linspace(0.1, 10.0, vision_dim,
                           device=device).unsqueeze(0).unsqueeze(0)
    vision_data = torch.sin(position * freqs)

    # 3. Tactile: ç‰¹å®šã®ä½ç½®ï¼ˆå£ãªã©ï¼‰ã«æ¥ãŸã¨ãã«åå¿œ
    tactile_dim = config['sensory_configs']['tactile']
    # ä½ç½®ãŒç‰¹å®šã®å€¤ã‚’è¶…ãˆãŸã‚‰ã€Œå£ã«å½“ãŸã£ãŸã€ã¨ã—ã¦è§¦è¦šä¿¡å·ç™ºç”Ÿ
    wall_collision = (torch.abs(position) > 2.0).float()
    tactile_data = wall_collision.expand(-1, -1, tactile_dim) * torch.randn(
        batch_size, seq_len, tactile_dim, device=device)

    return {
        'sensory': {
            'vision': vision_data,  # (B, T, D_vis)
            'tactile': tactile_data  # (B, T, D_tac)
        },
        'actions': actions  # (B, T, D_act)
    }


def main():
    logger.info("ğŸŒ Starting Multimodal World Model Simulation...")

    # --- Configuration ---
    device = "cuda" if torch.cuda.is_available() else "cpu"
    config = {
        'device': device,  # Added device here for generator
        'd_model': 256,
        'd_state': 64,
        'num_layers': 4,
        'time_steps': 8,
        'action_dim': 2,
        'sensory_configs': {
            'vision': 128,   # ç°¡æ˜“ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç”¨æ¬¡å…ƒ
            'tactile': 32
        },
        'neuron': {'type': 'lif'},
        'use_bitnet': True
    }

    # --- Build Model ---
    logger.info("ğŸ—ï¸ Building SpikingWorldModel...")
    # RegistryçµŒç”±ã§ãƒ“ãƒ«ãƒ‰ (ä¿®æ­£ã—ãŸArchitectureRegistryã‚’ä½¿ç”¨)
    model = ArchitectureRegistry.build(
        "spiking_world_model",
        config=config,
        vocab_size=0  # WorldModelã¯é›¢æ•£èªå½™å¿…é ˆã§ã¯ãªã„ãŸã‚0
    ).to(device)

    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)
    profiler = EfficiencyProfiler()

    # --- Training Loop (Self-Supervised) ---
    num_epochs = 10
    batch_size = 8
    seq_len = 20  # 20ã‚¹ãƒ†ãƒƒãƒ—åˆ†ã®æœªæ¥ã‚’äºˆæ¸¬ã—ãªãŒã‚‰å­¦ç¿’

    model.train()

    for epoch in range(num_epochs):
        # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
        data_batch = generate_synthetic_world_data(batch_size, seq_len, config)
        sensory_inputs = data_batch['sensory']
        actions = data_batch['actions']

        profiler.start_measurement()
        optimizer.zero_grad()

        # Forward: å…¨ã‚¹ãƒ†ãƒƒãƒ—ã‚’ä¸€æ‹¬å‡¦ç† (Training mode)
        # model.forward() ã¯ (z_pred, reconstructions, h_next) ã‚’è¿”ã™
        # ã“ã“ã§ã¯ã€Œéå»ã®è¦³æ¸¬+è¡Œå‹•ã€ã‹ã‚‰ã€Œæœªæ¥ã®è¦³æ¸¬ã€ã‚’äºˆæ¸¬ã•ã›ãŸã„

        # å…¥åŠ›ã‚’1ã‚¹ãƒ†ãƒƒãƒ—ãšã‚‰ã™ (tã®å…¥åŠ›ã§ t+1 ã‚’äºˆæ¸¬)
        # inputs: 0 ~ T-1
        # targets: 1 ~ T

        current_inputs = {k: v[:, :-1, :] for k, v in sensory_inputs.items()}
        current_actions = actions[:, :-1, :]
        target_observations = {k: v[:, 1:, :]
                               for k, v in sensory_inputs.items()}

        # æ¨è«–
        z_pred, reconstructions, _ = model(current_inputs, current_actions)

        # Lossè¨ˆç®—: å†æ§‹æˆèª¤å·® (MSE)
        total_loss = torch.tensor(0.0, device=device)
        losses_by_modality = {}

        for mod, pred in reconstructions.items():
            target = target_observations[mod]
            # æ¬¡å…ƒåˆã‚ã› (ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›é•·ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé•·)
            min_len = min(pred.size(1), target.size(1))

            recon_loss = F.mse_loss(
                pred[:, :min_len, :], target[:, :min_len, :])
            total_loss = total_loss + recon_loss
            losses_by_modality[mod] = recon_loss.item()

        # Backward
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()

        profiler.end_measurement()

        if (epoch + 1) % 2 == 0:
            logger.info(
                f"Epoch {epoch+1}/{num_epochs} | Total Loss: {total_loss.item():.4f}")
            logger.info(f"   Breakdown: {losses_by_modality}")

    # --- Prediction / Dreaming Test ---
    logger.info("ğŸ’¤ Testing Dreaming Capability (Open-Loop Prediction)...")
    model.eval()

    with torch.no_grad():
        # åˆæœŸçŠ¶æ…‹ (t=0)
        initial_obs = {k: v[:, 0:1, :] for k, v in sensory_inputs.items()}
        # æœªæ¥ã®è¡Œå‹•è¨ˆç”» (t=0~9)
        future_actions = actions[:, 0:10, :]

        # é–‰ãƒ«ãƒ¼ãƒ—äºˆæ¸¬: äºˆæ¸¬ã—ãŸçµæœã‚’æ¬¡ã®å…¥åŠ›ã¨ã—ã¦ä½¿ã„ã€å¤¢ã‚’è¦‹ç¶šã‘ã‚‹
        current_obs = initial_obs
        dreamed_trajectory = []

        # å†…éƒ¨çŠ¶æ…‹ã®ãƒªã‚»ãƒƒãƒˆ (å¿…è¦ã§ã‚ã‚Œã°)
        # model.reset_state() # ã‚‚ã—å®Ÿè£…ã•ã‚Œã¦ã„ã‚Œã°

        for t in range(10):
            action_t = future_actions[:, t, :]  # (B, ActDim)

            # 1ã‚¹ãƒ†ãƒƒãƒ—äºˆæ¸¬ (predict_next ãƒ¡ã‚½ãƒƒãƒ‰ä½¿ç”¨)
            next_obs_pred = model.predict_next(current_obs, action_t)

            # äºˆæ¸¬çµæœã‚’ä¿å­˜
            dreamed_trajectory.append(next_obs_pred)

            # æ¬¡ã®å…¥åŠ›ã¨ã—ã¦äºˆæ¸¬å€¤ã‚’ä½¿ç”¨ (é–‰ãƒ«ãƒ¼ãƒ—)
            # predict_next ã¯ (B, D) ã‚’è¿”ã™ã®ã§ (B, 1, D) ã«ãƒªã‚·ã‚§ã‚¤ãƒ—
            current_obs = {k: v.unsqueeze(1) for k, v in next_obs_pred.items()}

    logger.info("âœ… Dreaming verification completed.")
    logger.info(
        "   The model successfully generated a 10-step future trajectory without external input.")


if __name__ == "__main__":
    main()
