# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: scripts/experiments/brain/run_phase2_autonomous_agent.py
# Title: Phase 2 Autonomous Agent Experiment
# ä¿®æ­£å†…å®¹: Mypyã‚¨ãƒ©ãƒ¼ä¿®æ­£ (å‹ãƒ’ãƒ³ãƒˆã®è¿½åŠ )ã€‚

import torch
import logging
import sys
import os
import time
from typing import Dict, Any, List

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..')))

from snn_research.core.snn_core import SNNCore
from snn_research.adaptive.active_inference_agent import ActiveInferenceAgent
from snn_research.adaptive.intrinsic_motivator import IntrinsicMotivator

# ãƒ­ã‚¬ãƒ¼è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class Phase2AutonomousAgent:
    """Phase 2: è‡ªå¾‹å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    def __init__(self):
        self.device = "cpu"
        self.input_dim = 64
        self.hidden_dim = 128
        self.output_dim = 10
        
        # ã‚³ã‚¢è„³ (SNN)
        self.brain = SNNCore(
            in_features=self.input_dim,
            hidden_features=self.hidden_dim,
            out_features=self.output_dim
        ).to(self.device)
        
        # èƒ½å‹•çš„æ¨è«–ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
        self.active_inference = ActiveInferenceAgent(
            state_dim=self.hidden_dim,
            action_dim=self.output_dim
        )
        
        # å†…ç™ºçš„å‹•æ©Ÿã¥ã‘
        self.motivator = IntrinsicMotivator()
        
        # çŸ¥è­˜ãƒ™ãƒ¼ã‚¹
        # [Mypy Fix] å‹ãƒ’ãƒ³ãƒˆã‚’è¿½åŠ 
        self.knowledge_base: List[Dict[str, Any]] = [] 
        
        logger.info("ğŸ§  Autonomous Agent initialized.")

    def run_life_cycle(self, steps: int = 100):
        """ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã®å®Ÿè¡Œ"""
        logger.info(f"Starting life cycle for {steps} steps...")
        
        for t in range(steps):
            # 1. ç’°å¢ƒã‹ã‚‰ã®å…¥åŠ› (ãƒ€ãƒŸãƒ¼)
            sensory_input = torch.randn(1, self.input_dim).to(self.device)
            
            # 2. è„³ã«ã‚ˆã‚‹å‡¦ç† (çŸ¥è¦š)
            brain_state = self.brain(sensory_input)
            
            # 3. èƒ½å‹•çš„æ¨è«– (è¡Œå‹•é¸æŠ)
            action = self.active_inference.select_action(brain_state)
            
            # 4. å†…ç™ºçš„å ±é…¬ã®è¨ˆç®— (å¥½å¥‡å¿ƒ)
            intrinsic_reward = self.motivator.calculate_reward(brain_state)
            
            # 5. å­¦ç¿’ (å¯å¡‘æ€§æ›´æ–°)
            if intrinsic_reward > 0.5:
                # é©šããŒå¤§ãã„å ´åˆã€å­¦ç¿’ã‚’å¼·åŒ–
                target = torch.randn(1, self.output_dim).to(self.device) # ãƒ€ãƒŸãƒ¼
                self.brain.update_plasticity(sensory_input, target, learning_rate=0.05)
                
                # çŸ¥è­˜ã®è“„ç©
                self.knowledge_base.append({
                    "step": t,
                    "input_summary": sensory_input.mean().item(),
                    "surprise": intrinsic_reward
                })
            
            if (t+1) % 20 == 0:
                logger.info(f"Step {t+1}: Intrinsic Reward={intrinsic_reward:.4f}, Knowledge={len(self.knowledge_base)}")
                
        logger.info("âœ… Life cycle completed successfully.")

if __name__ == "__main__":
    agent = Phase2AutonomousAgent()
    agent.run_life_cycle()