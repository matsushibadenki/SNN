# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: scripts/experiments/brain/train_newton_concept.py
# æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: Brain v4.0 Newton's Training (Visual-Concept Alignment)
# ç›®çš„: ã€Œã‚Šã‚“ã”ãŒè½ã¡ã‚‹æ˜ åƒã€ã¨ã€ŒGravityã€ã¨ã„ã†å˜èªã‚’çµã³ã¤ã‘ã‚‹çŸ­æœŸé›†ä¸­å­¦ç¿’ã‚’è¡Œã†ã€‚

from snn_research.models.experimental.brain_v4 import SynestheticBrain
import sys
import os
import torch
import torch.nn as nn
import torch.optim as optim
from transformers import AutoTokenizer

sys.path.append(os.path.abspath(os.path.join(
    os.path.dirname(__file__), "../../../..")))


def train_newton():
    print("\n=======================================================")
    print(" ğŸ Newton's Training: Teaching 'Gravity' from Vision")
    print("=======================================================\n")

    device = "mps" if torch.backends.mps.is_available() else "cpu"
    if torch.cuda.is_available():
        device = "cuda"

    # 1. ãƒ¢ãƒ‡ãƒ«æº–å‚™
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    brain = SynestheticBrain(
        vocab_size=len(tokenizer),
        d_model=256,
        time_steps=8,
        device=device
    ).to(device)

    # å­¦ç¿’å¯¾è±¡: è¦–è¦šãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚¿ãƒ¼ï¼ˆç›®ã¨è„³ã®æ¥ç¶šéƒ¨ï¼‰ã¨è„³ã®ã‚³ã‚¢
    # ã“ã‚Œã«ã‚ˆã‚Šã€Œæ˜ åƒã€ã‚’ã€Œè¨€è‘‰ã®æ„å‘³ã€ã«ç¿»è¨³ã™ã‚‹å›è·¯ãŒå½¢æˆã•ã‚Œã¾ã™
    optimizer = optim.AdamW(brain.parameters(), lr=1e-3)
    criterion = nn.CrossEntropyLoss()

    # 2. æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
    # å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ: "The apple fell from the tree due to"
    text_prompt = "The apple fell from the tree due to"
    target_word = " gravity"

    # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
    inputs = tokenizer(text_prompt, return_tensors="pt").input_ids.to(device)
    target_id = tokenizer.encode(target_word, return_tensors="pt")[0, 0].item()
    target_tensor = torch.tensor([target_id], device=device)  # æ­£è§£ãƒ©ãƒ™ãƒ«

    # æ˜ åƒãƒ‡ãƒ¼ã‚¿ (è½ä¸‹é‹å‹•)
    image_seq = torch.zeros((1, 8, 1, 28, 28)).to(device)
    for t in range(8):
        y_pos = 2 + t * 3
        if y_pos < 28:
            image_seq[0, t, 0, y_pos:y_pos+3, 12:15] = 1.0

    print(f"ğŸ“– Context: '{text_prompt}'")
    print("ğŸ¬ Vision:  [Falling Apple Video]")
    print(f"ğŸ¯ Target:  '{target_word.strip()}'\n")

    # 3. å­¦ç¿’ãƒ«ãƒ¼ãƒ— (Overfitting Demo)
    brain.train()
    print(">>> Starting Training (Simulating 'Aha!' moment)...")

    for epoch in range(51):  # 50å›ã»ã©è¦‹ã›ã¦æ•™ãˆã‚‹
        optimizer.zero_grad()

        # æ€è€ƒ (Forward)
        logits = brain(text_input=inputs, image_input=image_seq)

        # äºˆæ¸¬ (æœ€å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã®æ¬¡ã®å˜èª)
        last_token_logits = logits[:, -1, :]
        loss = criterion(last_token_logits, target_tensor)

        loss.backward()
        optimizer.step()

        if epoch % 10 == 0:
            # ç¾åœ¨ã®äºˆæ¸¬ã‚’ç¢ºèª
            pred_id = torch.argmax(last_token_logits, dim=-1).item()
            pred_word = tokenizer.decode([pred_id]).strip()
            print(
                f"   Epoch {epoch:02d}: Loss = {loss.item():.4f} | Brain thinks: '{pred_word}'")

            if pred_word.lower() == "gravity":
                print(
                    f"\nâœ¨ Aha! The brain understood the concept at Epoch {epoch}!")
                break

    # 4. æœ€çµ‚ãƒ†ã‚¹ãƒˆ
    print("\n-------------------------------------------------------")
    print(" ğŸ§  Final Inference Test")
    print("-------------------------------------------------------")
    brain.eval()
    with torch.no_grad():
        logits = brain(text_input=inputs, image_input=image_seq)
        pred_id = torch.argmax(logits[:, -1, :], dim=-1).item()
        final_word = tokenizer.decode([pred_id])

    print(f"Input: \"{text_prompt}\" + [Falling Image]")
    print(f"Brain Answer: \"{final_word.strip()}\"")

    if "gravity" in final_word.lower():
        print("âœ… SUCCESS: The visual phenomenon is now grounded to the symbol 'Gravity'.")
    else:
        print("âŒ Failed to learn. More training needed.")


if __name__ == "__main__":
    train_newton()
