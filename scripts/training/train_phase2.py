# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: scripts/training/train_phase2.py
# æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: Phase 2 Advanced Training Script (MPS/tqdmå¯¾å¿œç‰ˆ)
# ç›®çš„: 1.58bit Spikformerãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦ã€é«˜ç²¾åº¦ã‹ã¤å …ç‰¢ãªãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹ã€‚
#       é€²æ—è¡¨ç¤ºã¨Mac M-series (MPS) åŠ é€Ÿã‚’è¿½åŠ ã€‚

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from torch.optim.lr_scheduler import CosineAnnealingLR
from tqdm import tqdm  # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ç”¨
import sys

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®è¨­å®š
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.abspath(os.path.join(current_dir, "../.."))
sys.path.insert(0, project_root)

from snn_research.models.transformer.spikformer import Spikformer
from snn_research.metrics.energy import EnergyMeter

                            
def train_phase2():
    # è¨­å®š
    BATCH_SIZE = 64
    EPOCHS = 50 
    LR = 1e-3
    
    # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š: Mac (MPS) > NVIDIA (CUDA) > CPU
    if torch.cuda.is_available():
        DEVICE = torch.device("cuda")
        print("ğŸš€ Using CUDA GPU")
    elif torch.backends.mps.is_available():
        DEVICE = torch.device("mps")
        print("ğŸš€ Using Apple MPS (M-series GPU)")
    else:
        DEVICE = torch.device("cpu")
        print("âš ï¸ Using CPU (Slow training expected)")
    
    print(f"   Target: Accuracy > 97.5%, Energy Efficiency 100x")

    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (CIFAR-10) with Augmentation
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.AutoAugment(transforms.AutoAugmentPolicy.CIFAR10),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])
    
    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])

    # Mac (MPS) ç”¨ã®æœ€é©åŒ–è¨­å®š
    if DEVICE.type == 'mps':
        num_workers = 0  # MPSã§ã¯0ã®æ–¹ãŒé€Ÿã„ã“ã¨ãŒå¤šã„
        pin_memory = False # MPSã§ã¯Trueã«ã™ã‚‹ã¨ä¸å®‰å®šãªå ´åˆãŒã‚ã‚‹ãŸã‚Falseæ¨å¥¨
        print("âš¡ Optimized for Apple MPS: num_workers=0")
    else:
        num_workers = 4
        pin_memory = True

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    trainset = datasets.CIFAR10(root='./workspace/data', train=True, download=True, transform=transform_train)
    trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, 
                             num_workers=num_workers, pin_memory=pin_memory)
    
    testset = datasets.CIFAR10(root='./workspace/data', train=False, download=True, transform=transform_test)
    testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, 
                            num_workers=num_workers, pin_memory=pin_memory)

    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    model = Spikformer(
        img_size_h=32, img_size_w=32,
        patch_size=4, in_channels=3,
        embed_dim=256, num_heads=8, num_layers=4,
        T=4, num_classes=10
    ).to(DEVICE)
    
    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶
    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=0.05)
    
    # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)
    
    # æå¤±é–¢æ•°
    criterion = nn.CrossEntropyLoss()

    # ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ¡ãƒ¼ã‚¿ãƒ¼
    energy_meter = EnergyMeter()

    # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
    for epoch in range(EPOCHS):
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        # tqdmã§ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’è¡¨ç¤º
        pbar = tqdm(enumerate(trainloader), total=len(trainloader), desc=f"Epoch {epoch+1}/{EPOCHS}")
        
        for i, (inputs, labels) in pbar:
            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)
            
            optimizer.zero_grad()
            
            # Forward
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            
            # Backward
            loss.backward()
            
            # Gradient Clipping
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            # çµ±è¨ˆæ›´æ–°
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã«æƒ…å ±ã‚’è¡¨ç¤º
            current_acc = 100. * correct / total
            pbar.set_postfix({'Loss': f"{running_loss/(i+1):.4f}", 'Acc': f"{current_acc:.2f}%"})

        train_acc = 100. * correct / total
        
        # è©•ä¾¡
        model.eval()
        test_correct = 0
        test_total = 0
        with torch.no_grad():
            for inputs, labels in testloader:
                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)
                outputs = model(inputs)
                _, predicted = outputs.max(1)
                test_total += labels.size(0)
                test_correct += predicted.eq(labels).sum().item()
        
        test_acc = 100. * test_correct / test_total
        
        scheduler.step()
        
        # ã‚¨ãƒãƒƒã‚¯çµ‚äº†å¾Œã®ãƒ­ã‚°
        print(f"Epoch [{epoch+1}/{EPOCHS}] Final -> "
              f"Train Acc: {train_acc:.2f}% | "
              f"Test Acc: {test_acc:.2f}% | "
              f"LR: {optimizer.param_groups[0]['lr']:.6f}")
        
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
        if test_acc > 95.0:
            save_path = os.path.join(project_root, "workspace", "models", f"spikformer_phase2_epoch{epoch}.pth")
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            torch.save(model.state_dict(), save_path)
            print(f"  ğŸ’¾ High Accuracy Model Saved: {save_path}")

    print("ğŸ Training Finished.")

if __name__ == "__main__":
    train_phase2()