# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: scripts/training/trainers/train_brain_v25_fast.py
# æ—¥æœ¬èªžã‚¿ã‚¤ãƒˆãƒ«: Brain v2.5 Fast Trainer (Optimized for Mac/MPS)
# ç›®çš„:
#   Mac(MPS)ã§ã‚‚é«˜é€Ÿã«åŽæŸã•ã›ã‚‹ãŸã‚ã®è»½é‡è¨­å®šç‰ˆãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã€‚
#   time_stepsã‚’4ã«å‰Šæ¸›ã—ã€å­¦ç¿’çŽ‡ã‚’é«˜ã‚ã«è¨­å®šã—ã¦å¼·åˆ¶çš„ã«ä¼šè©±ã‚’è¦šãˆã•ã›ã‚‹ã€‚

from tqdm import tqdm
from transformers import AutoTokenizer
from torch.utils.data import Dataset, DataLoader
import torch.optim as optim
import torch.nn as nn
import torch
import logging
import os
import sys

# ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯å›žé¿è¨­å®š
os.environ["TOKENIZERS_PARALLELISM"] = "false"


# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆè¨­å®š
sys.path.append(os.path.abspath(os.path.join(
    os.path.dirname(__file__), "../../..")))

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    datefmt='%H:%M:%S',
    stream=sys.stdout,
    force=True
)
logger = logging.getLogger("BrainFastTrainer")

try:
    from snn_research.models.experimental.bit_spike_mamba import BitSpikeMamba
    from snn_research.activation_based import functional  # type: ignore
except ImportError:
    try:
        from snn_research.models.experimental.bit_spike_mamba import BitSpikeMamba
        from spikingjelly.activation_based import functional
    except Exception as e:
        logger.error(f"Import Error: {e}")
        sys.exit(1)

# --- 1. Dataset ---


class FastConversationalDataset(Dataset):
    def __init__(self, tokenizer, block_size=128):
        self.tokenizer = tokenizer
        self.block_size = block_size

        # è¦šãˆã•ã›ã‚‹ä¼šè©±ãƒªã‚¹ãƒˆ (å°‘æ•°ç²¾é‹­)
        self.conversations = [
            ("Hello.", "Hi! I am Brain v2.5."),
            ("Who are you?", "I am a Spiking Neural Network AI."),
            ("What is SNN?", "SNN simulates the human brain's neural spikes."),
            ("ã“ã‚“ã«ã¡ã¯", "ã“ã‚“ã«ã¡ã¯ï¼èª¿å­ã¯ã©ã†ã§ã™ã‹ï¼Ÿ"),
            ("ã‚ãªãŸã¯èª°ï¼Ÿ", "ç§ã¯Brain v2.5ã€AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"),
            ("å…ƒæ°—ï¼Ÿ", "ã¯ã„ã€ã‚·ã‚¹ãƒ†ãƒ ã¯æ­£å¸¸ã§ã™ã€‚"),
            ("Pythonæ›¸ã„ã¦", "æ‰¿çŸ¥ã—ã¾ã—ãŸã€‚\n```python\nprint('Hello SNN')\n```"),
            ("çµ‚äº†", "ã•ã‚ˆã†ãªã‚‰ã€‚"),
            # System 2 Trigger
            ("Calculate sum.", "Thinking... The sum is calculated by adding numbers."),
            ("ãƒ•ã‚£ãƒœãƒŠãƒƒãƒ", "æ€è€ƒä¸­... ãƒ•ã‚£ãƒœãƒŠãƒƒãƒæ•°åˆ—ã‚’è¨ˆç®—ã—ã¾ã™ã€‚")
        ]

        self.data = []
        # ãƒ‡ãƒ¼ã‚¿é‡ã‚’ç¢ºä¿ (Epochã‚ãŸã‚Šã®ã‚¹ãƒ†ãƒƒãƒ—æ•°ã‚’ç¨¼ã)
        for _ in range(100):
            for user_text, brain_text in self.conversations:
                full_text = f"User: {user_text}\nBrain: {brain_text}"
                ids = tokenizer.encode(full_text, add_special_tokens=True)
                ids.append(tokenizer.eos_token_id)
                self.data.extend(ids)

    def __len__(self):
        return (len(self.data) - 1) // self.block_size

    def __getitem__(self, idx):
        start = idx * self.block_size
        end = start + self.block_size + 1
        if end > len(self.data):
            chunk = torch.tensor(self.data[start:], dtype=torch.long)
            pad_len = self.block_size + 1 - len(chunk)
            chunk = torch.cat(
                [chunk, torch.full((pad_len,), self.tokenizer.eos_token_id, dtype=torch.long)])
        else:
            chunk = torch.tensor(self.data[start:end], dtype=torch.long)
        return chunk[:-1], chunk[1:]

# --- 2. Training ---


def train():
    # â˜…é«˜é€ŸåŒ–è¨­å®šâ˜…
    CONFIG = {
        "d_model": 256,
        "d_state": 16,
        "d_conv": 4,
        "expand": 2,
        "num_layers": 4,      # 6 -> 4å±¤ã¸è»½é‡åŒ–
        "time_steps": 4,      # â˜…é‡è¦: 16 -> 4 (4å€é€Ÿ)
        "neuron_config": {"type": "lif", "tau_mem": 2.0},

        "batch_size": 8,      # ãƒãƒƒãƒã‚µã‚¤ã‚ºå¢—
        "epochs": 15,         # çŸ­æœŸé›†ä¸­
        "lr": 2e-3,           # å­¦ç¿’çŽ‡å¼·ã‚
        "save_path": "models/checkpoints/trained_brain_v25_fast.pth"
    }

    device = "mps" if torch.backends.mps.is_available() else "cpu"
    if torch.cuda.is_available():
        device = "cuda"

    print(f"\n>>> ðŸš€ Starting Fast Training on {device.upper()}", flush=True)
    print(
        f">>> Configuration: Layers={CONFIG['num_layers']}, TimeSteps={CONFIG['time_steps']}", flush=True)

    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = BitSpikeMamba(
        vocab_size=tokenizer.vocab_size,
        d_model=CONFIG["d_model"],
        d_state=CONFIG["d_state"],
        d_conv=CONFIG["d_conv"],
        expand=CONFIG["expand"],
        num_layers=CONFIG["num_layers"],
        time_steps=CONFIG["time_steps"],
        neuron_config=CONFIG["neuron_config"]
    ).to(device)

    model.train()

    dataset = FastConversationalDataset(tokenizer)
    dataloader = DataLoader(
        dataset, batch_size=CONFIG["batch_size"], shuffle=True, num_workers=0)

    optimizer = optim.AdamW(
        model.parameters(), lr=CONFIG["lr"], weight_decay=0.01)
    criterion = nn.CrossEntropyLoss()
    scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=CONFIG["epochs"])

    print(">>> Training Loop Started...", flush=True)
    if device == "mps":
        print("    (Note: First batch compilation might take ~30-60 seconds.)", flush=True)

    for epoch in range(CONFIG["epochs"]):
        total_loss = 0.0
        progress_bar = tqdm(
            dataloader, desc=f"Epoch {epoch+1}/{CONFIG['epochs']}")

        for inputs, targets in progress_bar:
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad()
            functional.reset_net(model)

            logits, _, _ = model(inputs)
            loss = criterion(logits.view(-1, logits.size(-1)),
                             targets.view(-1))

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

            total_loss += loss.item()
            progress_bar.set_postfix(loss=f"{loss.item():.4f}")

        scheduler.step()
        avg_loss = total_loss / len(dataloader)
        print(f"   Avg Loss: {avg_loss:.4f}", flush=True)

        # ç°¡æ˜“ç”Ÿæˆãƒã‚§ãƒƒã‚¯
        if (epoch + 1) % 5 == 0:
            generate_sample(model, tokenizer, device, "User: Hello.\nBrain:")

    # ä¿å­˜
    os.makedirs(os.path.dirname(CONFIG["save_path"]), exist_ok=True)
    state_dict = {k: v for k, v in model.state_dict(
    ).items() if "spikes" not in k and "mem" not in k}
    torch.save(state_dict, CONFIG["save_path"])
    print(f"\n>>> âœ… Model saved to {CONFIG['save_path']}")


def generate_sample(model, tokenizer, device, prompt):
    print("   [Sampling...]", flush=True)
    model.eval()
    try:
        with torch.no_grad():
            functional.reset_net(model)
            curr = tokenizer.encode(prompt, return_tensors='pt').to(device)
            out_tokens = []
            for _ in range(10):
                functional.reset_net(model)
                logits, _, _ = model(curr)
                next_tk = torch.argmax(logits[:, -1, :], dim=-1).unsqueeze(0)
                if next_tk.item() == tokenizer.eos_token_id:
                    break
                out_tokens.append(next_tk.item())
                curr = torch.cat([curr, next_tk], dim=1)
            print(f"   Brain: {tokenizer.decode(out_tokens)}", flush=True)
    except Exception:
        pass
    model.train()


if __name__ == "__main__":
    train()
