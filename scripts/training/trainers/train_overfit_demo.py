# „Éï„Ç°„Ç§„É´„Éë„Çπ: scripts/trainers/train_overfit_demo.py
# Êó•Êú¨Ë™û„Çø„Ç§„Éà„É´: BitSpikeMamba ÈÅéÂ≠¶Áøí„Éá„É¢ („Éê„Ç∞‰øÆÊ≠£ & È´òÈÄüÊ§úË®ºÁâà)
# ÁõÆÁöÑ: 1.58bit„É¢„Éá„É´„ÅÆÂü∫Êú¨Âãï‰Ωú„ÇíÊ§úË®º„Åô„Çã„ÄÇ„Éò„É´„Çπ„ÉÅ„Çß„ÉÉ„ÇØÁî®„Å´„Çπ„ÉÜ„ÉÉ„ÉóÂà∂Èôê„Å´ÂØæÂøú„ÄÇ

from snn_research.models.experimental.bit_spike_mamba import BitSpikeMamba  # E402 fixed
import os
import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import logging
import argparse

# „Éë„ÇπË®≠ÂÆö
project_root = os.path.dirname(os.path.dirname(
    os.path.dirname(os.path.abspath(__file__))))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# „É≠„Ç∞Ë®≠ÂÆö
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DummyTextDataset(Dataset):
    """Ê§úË®ºÁî®„ÅÆ„ÉÄ„Éü„Éº„ÉÜ„Ç≠„Çπ„Éà„Éá„Éº„Çø„Çª„ÉÉ„Éà„ÄÇ"""

    def __init__(self, vocab_size=1000, seq_len=16, num_samples=32):
        self.data = torch.randint(0, vocab_size, (num_samples, seq_len + 1))

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        x = self.data[idx, :-1]
        y = self.data[idx, 1:]
        return x, y


def train_demo(epochs=1, max_steps=None):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"üöÄ Starting BitSpikeMamba training demo on {device}")

    # 1. „É¢„Éá„É´„Å®„Éá„Éº„Çø„ÅÆÊ∫ñÂÇô
    vocab_size = 1000
    model = BitSpikeMamba(
        d_model=128, d_state=16, d_conv=4, expand=2,
        num_layers=2, time_steps=4,
        neuron_config={"threshold": 1.0}, vocab_size=vocab_size
    ).to(device)

    dataset = DummyTextDataset(vocab_size=vocab_size)
    # [Fix] „Åì„Åì„Åß dataloader „ÇíÊ≠£„Åó„ÅèÂÆöÁæ©
    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)

    optimizer = optim.Adam(model.parameters(), lr=1e-3)
    criterion = nn.CrossEntropyLoss()

    # 2. Â≠¶Áøí„É´„Éº„Éó
    model.train()
    for epoch in range(epochs):
        total_loss = 0.0
        # ÊòéÁ§∫ÁöÑ„Å´ÂÆöÁæ©„Åï„Çå„Åü dataloader „Çí‰ΩøÁî®
        for i, (x, y) in enumerate(dataloader):
            if max_steps is not None and i >= max_steps:
                break

            x, y = x.to(device), y.to(device)
            optimizer.zero_grad()

            # È†Ü‰ºùÊí≠
            logits, _, _ = model(x)
            loss = criterion(logits.view(-1, vocab_size), y.view(-1))

            # ÈÄÜ‰ºùÊí≠
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            if i % 2 == 0:
                logger.info(
                    f"  Epoch {epoch+1}, Step {i}, Loss: {loss.item():.4f}")

    logger.info("‚úÖ Training demo finished successfully.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--epochs", type=int, default=1,
                        help="Number of epochs to train")
    parser.add_argument("--max_steps", type=int, default=5,
                        help="Max steps per epoch for quick health-check")
    args = parser.parse_args()

    train_demo(epochs=args.epochs, max_steps=args.max_steps)
