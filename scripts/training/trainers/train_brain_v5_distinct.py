# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: scripts/trainers/train_brain_v5_distinct.py
# æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: Brain v2.0 Distinct Trainer (Spartan Mode)
# ç›®çš„ãƒ»å†…å®¹:
#   ãƒ¢ãƒ¼ãƒ‰å´©å£Šï¼ˆé‡‘å¤ªéƒé£´çŠ¶æ…‹ï¼‰ã‚’æ‰“ç ´ã—ã€è³ªå•ã”ã¨ã«æ˜ç¢ºã«ç•°ãªã‚‹å›ç­”ãŒã§ãã‚‹ã¾ã§å¾¹åº•å­¦ç¿’ã•ã›ã‚‹ã€‚
#   Target Loss: 0.05 (å®Œå…¨æš—è¨˜)
#   [Fix] mypyã‚¨ãƒ©ãƒ¼ä¿®æ­£: layer.set_statefulå‘¼ã³å‡ºã—æ™‚ã®å‹ã‚­ãƒ£ã‚¹ãƒˆã‚’è¿½åŠ ã€‚
#   [Fix] AttributeErrorä¿®æ­£: SimpleLIFNeuronã«set_statefulã‚’è¿½åŠ ã€‚

from spikingjelly.activation_based import functional, surrogate, base
from snn_research.core.layers.bit_spike_layer import BitSpikeLinear
from snn_research.core.mamba_core import SpikingMambaBlock
from snn_research.models.experimental.bit_spike_mamba import BitSpikeMamba
import os
import sys
import logging
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer
from typing import Any, cast, Tuple  # è¿½åŠ 

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆè¨­å®š
sys.path.append(os.path.abspath(
    os.path.join(os.path.dirname(__file__), "../..")))


logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s | %(message)s', datefmt='%H:%M:%S')
logger = logging.getLogger("BrainTrainer")

# --- 1. å®šç¾© ---


class SimpleLIFNeuron(base.MemoryModule):
    def __init__(self, features, tau_mem=4.0, base_threshold=0.01):  # tau_memã‚’10.0->4.0ã«çŸ­ç¸®
        super().__init__()
        self.tau_mem = tau_mem
        self.base_threshold = base_threshold
        self.surrogate_function = surrogate.ATan(alpha=2.0)
        self.register_buffer("mem", None)
        self.register_buffer("spikes", torch.zeros(features))
        self.stateful = False

    def reset(self):
        super().reset()
        self.mem = None
        self.spikes.zero_()

    # [Fix] Added missing method required by SpikingMambaBlock
    def set_stateful(self, stateful: bool):
        self.stateful = stateful

    def forward(self, x: torch.Tensor):
        if self.mem is None or self.mem.shape != x.shape:
            self.mem = torch.zeros_like(x)
        decay = 1.0 / self.tau_mem
        self.mem = self.mem * (1.0 - decay) + x
        spike = self.surrogate_function(self.mem - self.base_threshold)
        self.spikes = spike.detach()
        self.mem = self.mem - spike.detach() * self.base_threshold
        return spike


class FixedBitSpikeMamba(BitSpikeMamba):
    # [Fix] Aligned signature with parent (x instead of input_ids)
    def forward(self, x: torch.Tensor, return_spikes: bool = False, **kwargs: Any) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        input_ids = x # Alias for compatibility
        functional.reset_net(self)

        # Use embedding based on input type check (as done in parent)
        # But this script assumes input_ids is LongTensor
        x_embed = self.embedding(input_ids)
        x_out = x_embed

        # [Fix] ã‚­ãƒ£ã‚¹ãƒˆã—ã¦ mypy ã‚¨ãƒ©ãƒ¼ "Tensor not callable" ã‚’å›é¿
        for layer in self.layers:
            if hasattr(layer, 'set_stateful'):
                cast(Any, layer).set_stateful(True)

        for _ in range(self.time_steps):
            x_internal = x_embed
            for layer in self.layers:
                x_internal = layer(x_internal)
            x_out = x_internal

        # [Fix] ã‚­ãƒ£ã‚¹ãƒˆã—ã¦ mypy ã‚¨ãƒ©ãƒ¼å›é¿
        for layer in self.layers:
            if hasattr(layer, 'set_stateful'):
                cast(Any, layer).set_stateful(False)

        # è¦ªã‚¯ãƒ©ã‚¹ã«è¿½åŠ ã—ãŸ self.norm ã¨ self.output_projection ã‚’ä½¿ç”¨
        logits = self.output_projection(self.norm(x_out))

        # Return tuple matching signature
        device = input_ids.device
        return logits, torch.tensor(0.0, device=device), torch.tensor(0.0, device=device)
        

def force_replace_components(model, device):
    for name, module in model.named_modules():
        for child_name, child in module.named_children():
            if isinstance(child, BitSpikeLinear):
                new_layer = nn.Linear(
                    child.in_features, child.out_features, bias=child.bias is not None)
                setattr(module, child_name, new_layer.to(device))

    for layer in model.layers:
        if isinstance(layer, SpikingMambaBlock):
            feat_conv = layer.lif_conv.features
            feat_out = layer.lif_out.features
            # tau_mem = 4.0 ã«è¨­å®š
            layer.lif_conv = SimpleLIFNeuron(
                features=feat_conv, tau_mem=4.0, base_threshold=0.01).to(device)
            layer.lif_out = SimpleLIFNeuron(
                features=feat_out, tau_mem=4.0, base_threshold=0.01).to(device)


def generate_response(model, tokenizer, device, prompt):
    model.eval()
    functional.reset_net(model)
    with torch.no_grad():
        input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)
        generated = input_ids
        for _ in range(15):
            functional.reset_net(model)
            # forward returns tuple
            logits, _, _ = model(generated)
            next_token = torch.argmax(logits[:, -1, :], dim=-1).unsqueeze(0)
            generated = torch.cat([generated, next_token], dim=1)
            if next_token.item() == tokenizer.eos_token_id:
                break
    return tokenizer.decode(generated[0], skip_special_tokens=True)

# --- 2. Dataset ---


class DistinctDataset(Dataset):
    def __init__(self, tokenizer, block_size=64):
        self.tokenizer = tokenizer
        self.block_size = block_size
        corpus = [
            "User: Hello. AI: Hi there.",
            "User: Who are you? AI: I am Brain v2.0.",
            "User: What is SNN? AI: Spiking Neural Network.",  # çŸ­ãã‚·ãƒ³ãƒ—ãƒ«ã«
            "User: I see an apple. AI: An apple is red.",
            "User: Do you dream? AI: Yes, I dream.",
            "User: How are you? AI: I am normal.",
        ]
        self.data = []
        # åå¾©å›æ•°ã‚’èª¿æ•´ (100å›)
        for _ in range(100):
            for text in corpus:
                ids = tokenizer.encode(text, add_special_tokens=False)
                self.data.extend(ids)
                self.data.append(tokenizer.eos_token_id)

    def __len__(self): return (len(self.data) - 1) // self.block_size

    def __getitem__(self, idx):
        start = idx * self.block_size
        if start + self.block_size + 1 > len(self.data):
            start = len(self.data) - self.block_size - 1
        chunk = torch.tensor(
            self.data[start: start + self.block_size + 1], dtype=torch.long)
        return chunk[:-1], chunk[1:]

# --- 3. Trainer ---


def train():
    BATCH_SIZE = 8
    EPOCHS = 100
    LR = 1e-3
    TARGET_LOSS = 0.05  # â˜…é¬¼ã®ç›®æ¨™å€¤

    device = "cuda" if torch.cuda.is_available() else "cpu"
    if torch.backends.mps.is_available():
        device = "mps"
    logger.info(f"ğŸš€ Starting Distinct Training on {device}")

    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = FixedBitSpikeMamba(
        vocab_size=tokenizer.vocab_size, d_model=128, d_state=32, d_conv=4, expand=2, num_layers=4, time_steps=4,
        neuron_config={"type": "lif"}
    ).to(device)

    force_replace_components(model, device)

    dataset = DistinctDataset(tokenizer)
    loader = DataLoader(dataset, batch_size=BATCH_SIZE,
                        shuffle=True, drop_last=True)
    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)
    criterion = nn.CrossEntropyLoss()

    logger.info(">>> Training Loop Started...")

    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0.0

        for inputs, targets in loader:
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad()
            functional.reset_net(model)
            # forward returns tuple
            logits, _, _ = model(inputs)
            loss = criterion(logits.view(-1, logits.size(-1)),
                             targets.view(-1))
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            total_loss += loss.item()

        scheduler.step()
        avg_loss = total_loss / len(loader)

        print(f"Epoch {epoch+1}/{EPOCHS} | Loss: {avg_loss:.4f}")

        if (epoch+1) % 5 == 0:
            # â˜…2ã¤ã®ç•°ãªã‚‹è³ªå•ã‚’ã—ã¦ã€ç­”ãˆãŒä½¿ã„åˆ†ã‘ã‚‰ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
            r1 = generate_response(
                model, tokenizer, device, "User: Hello. AI:")
            r2 = generate_response(
                model, tokenizer, device, "User: Who are you? AI:")
            print(f"   [Test 1] {r1}")
            print(f"   [Test 2] {r2}")

        if avg_loss < TARGET_LOSS:
            logger.info(
                f"ğŸ¯ Perfect Convergence (Loss < {TARGET_LOSS}). Stopping.")
            break

        torch.save(model.state_dict(),
                   "models/checkpoints/trained_brain_v20.pth")

    logger.info("ğŸ‰ Done.")


if __name__ == "__main__":
    train()
