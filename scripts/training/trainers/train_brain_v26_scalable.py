# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: scripts/training/trainers/train_brain_v26_scalable.py
# æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: Brain v2.6 Scalable Trainer (Mypy Fixed)
# ç›®çš„: å­¦ç¿’ãƒ‡ãƒ¼ã‚¿é‡ã‚’ä»¥å‰ã®æˆåŠŸæ™‚(v25_fast)ã¨åŒç­‰ä»¥ä¸Šã«å¢—ã‚„ã—ã€ç¢ºå®Ÿã«åæŸã•ã›ã‚‹ã€‚

import os
import sys
import json
import logging
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer
from tqdm import tqdm

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆè¨­å®š
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../../..")))

# ãƒ‡ãƒƒãƒ‰ãƒ­ãƒƒã‚¯å›é¿
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(message)s', stream=sys.stdout, force=True)
logger = logging.getLogger("BrainScalable")

# â˜…ä¿®æ­£: ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼å›é¿ã€‚spikingjellyã‚’ç›´æ¥ä½¿ç”¨ã€‚
try:
    from snn_research.models.experimental.bit_spike_mamba import BitSpikeMamba
    from spikingjelly.activation_based import functional
except ImportError:
    # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒç­‰ã§ãƒ‘ã‚¹ãŒè§£æ±ºã§ããªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆmypyç”¨ã«ã¯ç„¡è¦–ã•ã›ã‚‹ï¼‰
    pass

class JsonConversationalDataset(Dataset):
    def __init__(self, tokenizer, json_path, block_size=128):
        self.tokenizer = tokenizer
        self.block_size = block_size
        self.data = []
        
        if not os.path.exists(json_path):
            raise FileNotFoundError(f"Data file not found: {json_path}")
            
        with open(json_path, 'r', encoding='utf-8') as f:
            conversations = json.load(f)
            
        logger.info(f"ğŸ“š Loading {len(conversations)} conversation pairs from {json_path}...")
        
        # ãƒ‡ãƒ¼ã‚¿é‡ãŒå°‘ãªã„å ´åˆã¯å¾¹åº•çš„ã«ç¹°ã‚Šè¿”ã—ã¦ã€Œæš—è¨˜ã€ã•ã›ã‚‹
        if len(conversations) < 20:
            repeat_factor = 150
        elif len(conversations) < 100:
            repeat_factor = 50
        else:
            repeat_factor = 10
            
        logger.info(f"ğŸ”„ Data Augmentation: Repeating dataset {repeat_factor} times to ensure convergence.")
        
        for _ in range(repeat_factor):
            for item in conversations:
                user_text = item.get("user", "")
                brain_text = item.get("brain", "")
                
                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå½¢å¼ã®çµ±ä¸€
                full_text = f"User: {user_text}\nBrain: {brain_text}"
                
                ids = tokenizer.encode(full_text, add_special_tokens=True)
                ids.append(tokenizer.eos_token_id)
                self.data.extend(ids)
                
    def __len__(self):
        return (len(self.data) - 1) // self.block_size

    def __getitem__(self, idx):
        start = idx * self.block_size
        end = start + self.block_size + 1
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†
        chunk = torch.tensor(self.data[start:end], dtype=torch.long)
        if len(chunk) < self.block_size + 1:
            pad = torch.full((self.block_size + 1 - len(chunk),), self.tokenizer.eos_token_id, dtype=torch.long)
            chunk = torch.cat([chunk, pad])
        return chunk[:-1], chunk[1:]

def train():
    # --- Config ---
    CONFIG = {
        "data_path": "data/training_data.json",
        "save_path": "models/checkpoints/trained_brain_v25_fast.pth", # èª­ã¿è¾¼ã¿å´ã¨åˆã‚ã›ã‚‹ãŸã‚åŒã˜ãƒ‘ã‚¹
        "d_model": 256,
        "num_layers": 4,
        "time_steps": 4,  # é«˜é€ŸåŒ–ç¶­æŒ
        "batch_size": 8,
        "epochs": 15,     # ãƒ‡ãƒ¼ã‚¿ãŒå¢—ãˆãŸã®ã§15ã‚¨ãƒãƒƒã‚¯ã§ååˆ†åæŸã™ã‚‹ã¯ãš
        "lr": 2e-3        # å­¦ç¿’ç‡ã‚’å°‘ã—å¼·ã‚ã«ç¶­æŒ
    }

    device = "mps" if torch.backends.mps.is_available() else "cpu"
    if torch.cuda.is_available(): device = "cuda"
    print(f"\n>>> ğŸš€ Starting Boosted Scalable Training on {device.upper()}")

    # Tokenizer
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token

    # Model
    model = BitSpikeMamba(
        vocab_size=tokenizer.vocab_size,
        d_model=CONFIG["d_model"],
        d_state=16, d_conv=4, expand=2,
        num_layers=CONFIG["num_layers"],
        time_steps=CONFIG["time_steps"],
        neuron_config={"type": "lif", "tau_mem": 2.0}
    ).to(device)
    
    # Dataset
    dataset = JsonConversationalDataset(tokenizer, CONFIG["data_path"])
    dataloader = DataLoader(dataset, batch_size=CONFIG["batch_size"], shuffle=True, num_workers=0)
    
    optimizer = optim.AdamW(model.parameters(), lr=CONFIG["lr"], weight_decay=0.01)
    criterion = nn.CrossEntropyLoss()
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG["epochs"])

    print(">>> Training Loop Started...", flush=True)

    model.train()
    for epoch in range(CONFIG["epochs"]):
        total_loss = 0.0
        progress = tqdm(dataloader, desc=f"Epoch {epoch+1}/{CONFIG['epochs']}")
        
        for inputs, targets in progress:
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad()
            functional.reset_net(model)
            
            logits, _, _ = model(inputs)
            loss = criterion(logits.view(-1, logits.size(-1)), targets.view(-1))
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            total_loss += loss.item()
            progress.set_postfix(loss=f"{loss.item():.4f}")
        
        scheduler.step()
        
        avg_loss = total_loss / len(dataloader)
        print(f"   Avg Loss: {avg_loss:.4f}")

    # ä¿å­˜
    torch.save(model.state_dict(), CONFIG["save_path"])
    print(f"\n>>> âœ… Model updated from {CONFIG['data_path']}")

if __name__ == "__main__":
    train()