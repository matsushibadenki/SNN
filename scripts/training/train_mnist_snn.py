# scripts/train_mnist_snn.py
# æ—¥æœ¬èªžã‚¿ã‚¤ãƒˆãƒ«: MNIST SNNå­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (Metrics Export Ready)
# æ¦‚è¦: MNISTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’ã‚’è¡Œã„ã€ç›®æ¨™ç²¾åº¦96.89%é”æˆã‚’ç›®æŒ‡ã™ã€‚
#       æ¤œè¨¼ãƒ„ãƒ¼ãƒ«(verify_performance.py)ç”¨ã®JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•ç”Ÿæˆã™ã‚‹ã€‚

import os
import sys
import time
import logging
import random
import json
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from tqdm import tqdm

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®è¨­å®š
from pathlib import Path
project_root = Path(__file__).resolve().parent.parent
if str(project_root) not in sys.path:
    sys.path.append(str(project_root))

# å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from spikingjelly.activation_based import functional as SJ_F
    from snn_research.core.neurons import AdaptiveLIFNeuron
    from snn_research.core.base import BaseModel
except ImportError as e:
    print(f"âŒ Import Error: {e}")
    print("ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰å®Ÿè¡Œã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    sys.exit(1)

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger("MNIST_Trainer")

# --- 1. ãƒ¢ãƒ‡ãƒ«å®šç¾© (MNIST Optimized Spiking CNN) ---


class MNIST_SpikingCNN(BaseModel):
    """
    MNIST (1x28x28) ã«ç‰¹åŒ–ã—ãŸSpiking CNNãƒ¢ãƒ‡ãƒ«ã€‚
    """

    def __init__(self, num_classes=10, time_steps=4, neuron_config=None):
        super().__init__()
        self.time_steps = time_steps

        if neuron_config is None:
            neuron_config = {'features': 0,
                             'tau_mem': 2.0, 'base_threshold': 1.0}

        def create_neuron(features):
            params = neuron_config.copy()
            params['features'] = features
            valid_keys = [
                "features", "tau_mem", "base_threshold", "adaptation_strength",
                "target_spike_rate", "noise_intensity", "threshold_decay",
                "threshold_step", "v_reset", "homeostasis_rate", "refractory_period"
            ]
            clean_params = {k: v for k, v in params.items() if k in valid_keys}
            return AdaptiveLIFNeuron(**clean_params)

        # Layer 1: Conv(1->32) -> BN -> LIF -> MaxPool
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(32)
        self.lif1 = create_neuron(32)
        self.pool1 = nn.MaxPool2d(2, 2)

        # Layer 2: Conv(32->64) -> BN -> LIF -> MaxPool
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(64)
        self.lif2 = create_neuron(64)
        self.pool2 = nn.MaxPool2d(2, 2)

        # Layer 3: Flatten -> Linear -> LIF
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(64 * 7 * 7, 128, bias=False)
        self.lif3 = create_neuron(128)

        # Layer 4: Output
        self.fc2 = nn.Linear(128, num_classes, bias=False)

        self._init_weights()

    def forward(self, x, return_spikes=False):

        x_seq = x.unsqueeze(1).repeat(1, self.time_steps, 1, 1, 1)

        SJ_F.reset_net(self)

        outputs = []
        for m in self.modules():
            if hasattr(m, 'set_stateful'):
                m.set_stateful(True)

        for t in range(self.time_steps):
            x_t = x_seq[:, t]

            x_t = self.conv1(x_t)
            x_t = self.bn1(x_t)
            x_t, _ = self.lif1(x_t)
            x_t = self.pool1(x_t)

            x_t = self.conv2(x_t)
            x_t = self.bn2(x_t)
            x_t, _ = self.lif2(x_t)
            x_t = self.pool2(x_t)

            x_t = self.flatten(x_t)
            x_t = self.fc1(x_t)
            x_t, _ = self.lif3(x_t)

            x_t = self.fc2(x_t)
            outputs.append(x_t)

        for m in self.modules():
            if hasattr(m, 'set_stateful'):
                m.set_stateful(False)

        outputs = torch.stack(outputs).permute(1, 0, 2)
        logits = outputs.mean(dim=1)

        avg_spikes = torch.tensor(0.0)
        if return_spikes:
            # LIF3ã®ç™ºç«çŽ‡ã‚’ä»£è¡¨å€¤ã¨ã—ã¦ä½¿ç”¨
            avg_spikes = self.lif3.spikes.mean().detach()

        return logits, avg_spikes, torch.tensor(0.0)


def set_seed(seed=42):
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.backends.cudnn.deterministic = True


def main():
    # --- Config ---
    CONFIG = {
        "seed": 42,
        "epochs": 10,
        "batch_size": 128,
        "lr": 1e-3,
        "time_steps": 4,
        "num_classes": 10,
        "dataset_path": "./data/mnist",
        "output_json": "workspace/results/best_mnist_metrics.json",  # å‡ºåŠ›å…ˆ
        "neuron_config": {
            "tau_mem": 2.0,
            "base_threshold": 1.0,
            "v_reset": 0.0,
            "adaptation_strength": 0.1,
            "target_spike_rate": 0.1
        }
    }

    set_seed(CONFIG["seed"])

    if torch.cuda.is_available():
        device = torch.device("cuda")
        logger.info(f"ðŸš€ Using CUDA: {torch.cuda.get_device_name(0)}")
    elif torch.backends.mps.is_available():
        device = torch.device("mps")
        logger.info("ðŸš€ Using Apple MPS")
    else:
        device = torch.device("cpu")
        logger.info("âš ï¸ Using CPU")

    # --- Dataset ---
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    os.makedirs(CONFIG["dataset_path"], exist_ok=True)
    train_dataset = datasets.MNIST(
        root=CONFIG["dataset_path"], train=True, download=True, transform=transform)
    test_dataset = datasets.MNIST(
        root=CONFIG["dataset_path"], train=False, download=True, transform=transform)

    pin_memory = True if device.type == 'cuda' else False
    num_workers = 4 if os.name != 'nt' else 0
    train_loader = DataLoader(
        train_dataset, batch_size=CONFIG["batch_size"], shuffle=True, num_workers=num_workers, pin_memory=pin_memory)
    test_loader = DataLoader(
        test_dataset, batch_size=CONFIG["batch_size"], shuffle=False, num_workers=num_workers, pin_memory=pin_memory)

    # --- Model ---
    model = MNIST_SpikingCNN(
        num_classes=CONFIG["num_classes"],
        time_steps=CONFIG["time_steps"],
        neuron_config=CONFIG["neuron_config"]
    ).to(device)

    optimizer = optim.AdamW(
        model.parameters(), lr=CONFIG["lr"], weight_decay=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=CONFIG["epochs"])
    criterion = nn.CrossEntropyLoss()
    scaler = torch.amp.GradScaler('cuda', enabled=(device.type == 'cuda'))

    # --- Training Loop ---
    best_acc = 0.0

    os.makedirs("results", exist_ok=True)

    for epoch in range(CONFIG["epochs"]):
        model.train()
        train_loss = 0.0
        correct = 0
        total = 0

        pbar = tqdm(
            train_loader, desc=f"Epoch {epoch+1}/{CONFIG['epochs']}", unit="batch")

        for inputs, targets in pbar:
            inputs, targets = inputs.to(device), targets.to(device)
            optimizer.zero_grad()

            with torch.amp.autocast('cuda', enabled=(device.type == 'cuda')):
                logits, avg_spikes, _ = model(inputs, return_spikes=True)
                loss = criterion(logits, targets)
                if avg_spikes.item() > 0:
                    loss += 0.01 * torch.abs(avg_spikes - 0.1)

            if device.type == 'cuda':
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
            else:
                loss.backward()
                optimizer.step()

            SJ_F.reset_net(model)

            train_loss += loss.item()
            _, predicted = logits.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
            pbar.set_postfix({"Loss": f"{loss.item():.4f}",
                             "Acc": f"{100.*correct/total:.1f}%"})

        scheduler.step()

        # --- Validation with Metrics ---
        model.eval()
        test_correct = 0
        test_total = 0
        total_latency = 0.0
        total_batches = 0
        spike_rates = []

        with torch.no_grad():
            for inputs, targets in test_loader:
                inputs, targets = inputs.to(device), targets.to(device)

                # Latency Measurement
                start_event = time.time()
                logits, batch_avg_spikes, _ = model(inputs, return_spikes=True)
                end_event = time.time()

                # ãƒãƒƒãƒã‚µã‚¤ã‚ºã§å‰²ã£ã¦1ã‚µãƒ³ãƒ—ãƒ«ã‚ãŸã‚Šã®æ™‚é–“ã‚’æ¦‚ç®—ï¼ˆåŽ³å¯†ã«ã¯ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰å«ã‚€ï¼‰
                batch_latency = (end_event - start_event) * 1000  # ms
                total_latency += batch_latency / inputs.size(0)  # per sample
                total_batches += 1

                spike_rates.append(batch_avg_spikes.item())
                SJ_F.reset_net(model)

                _, predicted = logits.max(1)
                test_total += targets.size(0)
                test_correct += predicted.eq(targets).sum().item()

        test_acc = 100. * test_correct / test_total
        avg_latency = total_latency / total_batches if total_batches > 0 else 0.0
        avg_spike_rate = sum(spike_rates) / \
            len(spike_rates) if spike_rates else 0.0

        logger.info(
            f"Epoch {epoch+1}: Test Acc={test_acc:.2f}%, Latency={avg_latency:.2f}ms, SpikeRate={avg_spike_rate:.1%}")

        # --- Save Best Result ---
        if test_acc > best_acc:
            best_acc = test_acc

            # Model Save
            torch.save(model.state_dict(),
                       "workspace/results/best_mnist_snn.pth")

            # Metrics JSON Save
            metrics_data = {
                "accuracy": best_acc / 100.0,
                "latency_ms": avg_latency,
                "avg_spike_rate": avg_spike_rate,
                "estimated_energy_joules": 2.0e-3 * avg_spike_rate * 0.2,  # Simple estimation logic
                "epoch": epoch + 1
            }

            with open(CONFIG["output_json"], "w") as f:
                json.dump(metrics_data, f, indent=4)

            logger.info(f"ðŸ’¾ Best Model & Metrics Saved! Acc: {best_acc:.2f}%")

            if best_acc >= 96.89:
                logger.info("ðŸŽ‰ GOAL REACHED!")

    logger.info(f"âœ… Finished. Best Acc: {best_acc:.2f}%")


if __name__ == "__main__":
    main()
