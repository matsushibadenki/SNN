# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: scripts/runners/verify_and_train.py


from transformers import AutoTokenizer
from app.containers import TrainingContainer
from omegaconf import OmegaConf
import logging
import torch
import sys
import os

# ------------------------------------------------------------------------------
# [Auto-inserted by fix_script_paths.py]
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’sys.pathã«è¿½åŠ ã—ã¦ã€snn_researchãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’è§£æ±ºå¯èƒ½ã«ã™ã‚‹
# ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ scripts/runners/ ã«é…ç½®ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’æƒ³å®šã—ã¦ã„ã¾ã™ (ãƒ«ãƒ¼ãƒˆã‹ã‚‰2éšå±¤ä¸‹)
# ------------------------------------------------------------------------------
project_root = os.path.abspath(
    os.path.join(os.path.dirname(__file__), "../.."))
if project_root not in sys.path:
    sys.path.insert(0, project_root)
# ------------------------------------------------------------------------------

# verify_and_train.py

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.dirname(os.path.abspath(__file__)))


logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("VerifyTrain")


def main():
    # 1. è¨­å®šã®ãƒ­ãƒ¼ãƒ‰ (train.py ã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ã‚’å†ç¾ + å¼·åˆ¶ä¸Šæ›¸ã)
    config_path = "configs/models/bit_rwkv_micro.yaml"
    base_path = "configs/templates/base_config.yaml"

    base_conf = OmegaConf.load(base_path)
    model_conf = OmegaConf.load(config_path)

    # è¨­å®šã®ãƒãƒ¼ã‚¸
    conf = OmegaConf.merge(base_conf, model_conf)

    # --- ã€é‡è¦ã€‘ã“ã“ã§ã‚³ãƒ¼ãƒ‰å†…ã§ç›´æ¥å€¤ã‚’å¼·åˆ¶ä¸Šæ›¸ãã—ã¾ã™ ---
    # CLIå¼•æ•°ã«é ¼ã‚‰ãšã€ã“ã“ã§ç¢ºå®šã•ã›ã¾ã™
    conf.model.neuron.base_threshold = 0.00001
    conf.model.neuron.tau_mem = 1000.0
    conf.model.neuron.noise_intensity = 0.2

    conf.training.paradigm = "gradient_based"
    conf.training.gradient_based.type = "standard"  # è’¸ç•™ã§ã¯ãªãæ¨™æº–å­¦ç¿’
    conf.training.epochs = 5
    conf.data.path = "data/smoke_test_data.jsonl"

    print("\n" + "="*40)
    print("ğŸ§ [VERIFICATION] é©ç”¨ã•ã‚Œã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³è¨­å®š:")
    print(f"   Threshold: {conf.model.neuron.base_threshold}")
    print(f"   Tau Mem  : {conf.model.neuron.tau_mem}")
    print(f"   Noise    : {conf.model.neuron.noise_intensity}")
    print("="*40 + "\n")

    # 2. ã‚³ãƒ³ãƒ†ãƒŠã¨ãƒ¢ãƒ‡ãƒ«ã®æº–å‚™
    container = TrainingContainer()
    # OmegaConf -> Dict å¤‰æ›
    container.config.from_dict(OmegaConf.to_container(conf, resolve=True))

    device = "cpu"  # å®‰å…¨ã®ãŸã‚CPU
    model = container.snn_model()
    model.to(device)

    # 3. å®Ÿãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å€¤ã‚’æ¤œæŸ» (ã“ã‚ŒãŒçœŸå®Ÿã§ã™)
    print("ğŸ§ [VERIFICATION] ãƒ¢ãƒ‡ãƒ«å†…éƒ¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å®Ÿæ¸¬å€¤:")

    real_thresh = None
    real_tau = None

    # ãƒ¢ãƒ‡ãƒ«å†…éƒ¨ã‚’æ¢ç´¢ã—ã¦LIFãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’æ¢ã™
    for name, module in model.named_modules():
        if "lif" in name.lower() or "neuron" in name.lower():
            if hasattr(module, 'base_threshold'):
                # å€¤ã‚’å–å¾—
                th = module.base_threshold
                if isinstance(th, torch.Tensor):
                    th = th.mean().item()

                tau_param = getattr(module, 'log_tau_mem', None)
                if tau_param is not None:
                    tau_val = (torch.exp(tau_param) + 1.1).mean().item()
                elif hasattr(module, 'tau_mem'):
                    tau_val = module.tau_mem
                else:
                    tau_val = -1

                print(f"   - Layer: {name}")
                print(f"     -> Threshold: {th:.6f}")
                print(f"     -> Tau      : {tau_val:.2f}")

                if real_thresh is None:
                    real_thresh = th
                if real_tau is None:
                    real_tau = tau_val

                # æœ€åˆã®1ã¤ã ã‘ãƒã‚§ãƒƒã‚¯ã™ã‚Œã°ååˆ†
                break

    if real_thresh > 0.01:
        print("\nâŒ è­¦å‘Š: é–¾å€¤ãŒåæ˜ ã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼ è¨­å®šãŒä¸Šæ›¸ãã•ã‚Œã¦ã„ã¾ã™ã€‚")
    else:
        print("\nâœ… ç¢ºèª: è¨­å®šã¯æ­£ã—ãåæ˜ ã•ã‚Œã¦ã„ã¾ã™ã€‚")

    # 4. å­¦ç¿’ã®å®Ÿè¡Œ
    print("\nğŸš€ å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")
    tokenizer = AutoTokenizer.from_pretrained("gpt2")

    # --- â–¼ ä¿®æ­£: type: ignore è¿½åŠ  â–¼ ---
    from train import collate_fn  # type: ignore[import-not-found]
    # --- â–² ä¿®æ­£ â–² ---
    from snn_research.data.datasets import SimpleTextDataset

    dataset = SimpleTextDataset(conf.data.path, tokenizer, max_seq_len=16)
    loader = torch.utils.data.DataLoader(
        dataset,
        batch_size=4,
        shuffle=True,
        collate_fn=collate_fn(tokenizer, is_distillation=False)
    )

    optimizer = container.optimizer(params=model.parameters())
    scheduler = None

    trainer = container.standard_trainer(
        model=model,
        optimizer=optimizer,
        scheduler=scheduler,
        device=device,
        rank=-1
    )

    for epoch in range(conf.training.epochs):
        trainer.train_epoch(loader, epoch)
        metrics = trainer.evaluate(loader, epoch)  # æ¤œè¨¼ã‚‚åŒã˜ãƒ‡ãƒ¼ã‚¿ã§ç°¡æ˜“ãƒã‚§ãƒƒã‚¯

        spike_rate = metrics.get('spike_rate', 0.0)
        print(f"   -> Epoch {epoch} Spike Rate: {spike_rate:.6f}")

        if spike_rate > 0:
            print("ğŸ‰ æˆåŠŸï¼ã‚¹ãƒ‘ã‚¤ã‚¯ãŒç™ºç”Ÿã—ã¾ã—ãŸï¼")
            return


if __name__ == "__main__":
    main()
