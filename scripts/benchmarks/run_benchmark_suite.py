# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: scripts/benchmarks/run_benchmark_suite.py
# æ—¥æœ¬èªžã‚¿ã‚¤ãƒˆãƒ«: SNN Benchmark Suite v2.3 (Architecture Comparison)
# ç›®çš„ãƒ»å†…å®¹:
#   ãƒ¢ãƒ‡ãƒ«ã®æŽ¨è«–é€Ÿåº¦ï¼ˆLatencyï¼‰ã€å­¦ç¿’èƒ½åŠ›ï¼ˆThroughputï¼‰ã€ãŠã‚ˆã³ãƒ¡ãƒ¢ãƒªåŠ¹çŽ‡ã‚’æ¸¬å®šã™ã‚‹ã€‚
#   Transformer(SFormer) ã¨ State Space Model(BitSpikeMamba) ã®æ¯”è¼ƒã‚’è¡Œã†ã€‚

import sys
import os
import torch
import torch.nn as nn
import logging
import argparse
import time
import json
import datetime
from typing import Dict, Any, cast, Optional
from omegaconf import OmegaConf

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®è¨­å®š
project_root = os.path.abspath(
    os.path.join(os.path.dirname(__file__), "../../"))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    stream=sys.stdout
)
logger = logging.getLogger("Benchmark")

# å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆå¤±æ•—æ™‚ã¯è­¦å‘Šã®ã¿ï¼‰
try:
    from snn_research.core.snn_core import SNNCore
except ImportError:
    SNNCore = None  # type: ignore
    print("âš ï¸ SNNCore not found. Using mock for structure verification.")

try:
    from snn_research.models.experimental.bit_spike_mamba import BitSpikeMamba
except ImportError:
    BitSpikeMamba = None  # type: ignore
    print("âš ï¸ BitSpikeMamba not found.")


class BenchmarkSuite:
    def __init__(self, output_dir: str = "benchmarks/results"):
        print("âš™ï¸ Initializing Benchmark Suite v2.3...")
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)

        # ãƒ‡ãƒã‚¤ã‚¹é¸æŠž
        if torch.cuda.is_available():
            self.device = "cuda"
        elif torch.backends.mps.is_available():
            self.device = "mps"
        else:
            self.device = "cpu"

        print(f"   -> Device selected: {self.device}")

        self.results: Dict[str, Any] = {
            "timestamp": str(datetime.datetime.now()),
            "hardware": self.device,
            "tests": {}
        }

    def _get_dummy_config(self, architecture: str) -> Dict[str, Any]:
        """ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ç”¨ã®å…±é€šè¨­å®š"""
        base_config = {
            "vocab_size": 100,
            "d_model": 64,
            "num_layers": 2,
            "time_steps": 16,
            "neuron_config": {"type": "lif", "base_threshold": 1.0}
        }

        if architecture == "bit_spike_mamba":
            base_config.update({
                "architecture_type": "bit_spike_mamba",
                "d_state": 16,
                "d_conv": 4,
                "expand": 2
            })
        else:
            base_config.update({
                "architecture_type": architecture,
                "nhead": 4,
                "dim_feedforward": 256
            })

        return base_config

    def _build_model(self, model_name: str, config_path: Optional[str] = None) -> nn.Module:
        """ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰ãƒ˜ãƒ«ãƒ‘ãƒ¼"""
        # 1. Configæº–å‚™
        if config_path and os.path.exists(config_path):
            conf = OmegaConf.load(config_path)
            model_config = OmegaConf.to_container(
                conf.model if 'model' in conf else conf, resolve=True)
        else:
            # ãƒžãƒƒãƒ”ãƒ³ã‚°
            if "Mamba" in model_name:
                arch = "bit_spike_mamba"
            elif "DSA" in model_name:
                arch = "dsa_transformer"
            else:
                arch = "sformer"  # default
            model_config = self._get_dummy_config(arch)

        model_config = cast(Dict[str, Any], model_config)
        vocab_size = int(model_config.get("vocab_size", 100))

        # 2. ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
        # BitSpikeMambaã®ç›´æŽ¥ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ï¼ˆSNNCoreæœªå¯¾å¿œã®å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        if model_config.get("architecture_type") == "bit_spike_mamba" and BitSpikeMamba is not None:
            # BitSpikeMamba __init__ args check
            return BitSpikeMamba(
                vocab_size=vocab_size,
                d_model=model_config["d_model"],
                d_state=model_config["d_state"],
                d_conv=model_config["d_conv"],
                expand=model_config["expand"],
                num_layers=model_config["num_layers"],
                time_steps=model_config.get("time_steps", 16),
                neuron_config=model_config["neuron_config"]
            ).to(self.device)

        # SNNCoreçµŒç”±
        if SNNCore is not None:
            return SNNCore(config=model_config, vocab_size=vocab_size).to(self.device)

        raise ImportError("No suitable model class found.")

    def _measure_model_size(self, model: nn.Module) -> float:
        """ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µã‚¤ã‚º(MB)ã‚’è¨ˆç®—"""
        param_size = 0
        for param in model.parameters():
            param_size += param.nelement() * param.element_size()
        buffer_size = 0
        for buffer in model.buffers():
            buffer_size += buffer.nelement() * buffer.element_size()

        size_all_mb = (param_size + buffer_size) / 1024**2
        return size_all_mb

    def run_smoke_test(self, model_name: str, config_path: Optional[str] = None):
        """ã‚¹ãƒ¢ãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ: æ§‹ç¯‰ã¨æŽ¨è«–ã®ç¢ºèª"""
        print(f"\nðŸ§ª [Smoke Test] {model_name} ... ", end="", flush=True)

        try:
            model = self._build_model(model_name, config_path)
            model.eval()

            # Size Check
            size_mb = self._measure_model_size(model)

            # Input check
            vocab_size = getattr(model, 'vocab_size', 100)
            if hasattr(model, 'config'):
                vocab_size = model.config.get('vocab_size', 100)

            input_ids = torch.randint(0, vocab_size, (1, 16)).to(self.device)

            with torch.no_grad():
                _ = model(input_ids)

            print(f"âœ… PASSED (Size: {size_mb:.2f} MB)")
            self.results["tests"][f"smoke_{model_name}"] = {
                "status": "PASSED",
                "model_size_mb": size_mb
            }

        except Exception as e:
            print(f"âŒ FAILED: {e}")
            import traceback
            traceback.print_exc()
            self.results["tests"][f"smoke_{model_name}"] = {
                "status": "FAILED", "error": str(e)}

    def run_efficiency_benchmark(self, model_name: str):
        """
        åŠ¹çŽ‡ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ (Latency):
        T=1 (å˜ä¸€ã‚¹ãƒ†ãƒƒãƒ—) ã®æŽ¨è«–ã‚’è¡Œã„ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¿œç­”æ€§èƒ½ï¼ˆReaction Timeï¼‰ã‚’æ¸¬å®šã€‚
        """
        print(
            f"\nâš¡ [Efficiency Test] {model_name} (T=1 Latency) ... ", end="", flush=True)
        try:
            model = self._build_model(model_name)

            # T=1 å¼·åˆ¶ (ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦è¨­å®šæ–¹æ³•ãŒç•°ãªã‚‹ãŸã‚å±žæ€§ã‚»ãƒƒãƒˆ)
            if hasattr(model, 'time_steps'):
                model.time_steps = 1
            if hasattr(model, 'config'):
                model.config['time_steps'] = 1

            model.eval()

            # å…¥åŠ›é•·: 1 (å˜ä¸€ãƒˆãƒ¼ã‚¯ãƒ³/ãƒ•ãƒ¬ãƒ¼ãƒ )
            vocab_size = 100
            input_ids = torch.randint(0, vocab_size, (1, 1)).to(self.device)

            # Warmup
            for _ in range(10):
                _ = model(input_ids)

            num_runs = 100

            # åŒæœŸå‡¦ç† (CUDA/MPS)
            if self.device == "cuda":
                torch.cuda.synchronize()
            elif self.device == "mps":
                torch.mps.synchronize()

            start_time = time.time()
            with torch.no_grad():
                for _ in range(num_runs):
                    # SNN State Reset check
                    if hasattr(model, 'reset_net'):
                        model.reset_net()  # Direct logic
                    elif hasattr(model, 'model') and hasattr(model.model, 'reset_net'):
                        model.model.reset_net()

                    _ = model(input_ids)

            if self.device == "cuda":
                torch.cuda.synchronize()
            elif self.device == "mps":
                torch.mps.synchronize()

            end_time = time.time()
            avg_latency = ((end_time - start_time) / num_runs) * 1000

            print("âœ… DONE")
            print(f"   -> Latency: {avg_latency:.2f} ms / step")

            status = "PASSED"
            # Target: 10ms for Real-time
            if avg_latency > 10.0:
                print("   âš ï¸ WARNING: Latency > 10ms (Target not met)")
                status = "WARNING"
            elif avg_latency < 5.0:
                print("   ðŸš€ EXCELLENT: Latency < 5ms (Real-time capable)")

            self.results["tests"][f"efficiency_{model_name}"] = {
                "status": status,
                "latency_ms": avg_latency,
                "note": "Measured with T=1 input"
            }
        except Exception as e:
            print(f"âŒ FAILED: {e}")
            import traceback
            traceback.print_exc()

    def save_report(self):
        json_path = os.path.join(self.output_dir, "benchmark_latest.json")
        with open(json_path, 'w') as f:
            json.dump(self.results, f, indent=2)
        print(f"\nðŸ“ Report saved to {json_path}")


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--mode", type=str, default="full",
                        choices=["smoke", "full"])
    args = parser.parse_args()

    suite = BenchmarkSuite()

    # æ¯”è¼ƒå¯¾è±¡ã®ãƒ¢ãƒ‡ãƒ«å®šç¾©
    # 1. Baseline: å¾“æ¥ã®Transformer (Spikformer / SFormer)
    # 2. Target: æ–°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ (BitSpikeMamba)
    models = [
        ("SFormer_Baseline", None),  # Uses dummy config
        ("BitSpikeMamba_New", None)  # Uses dummy config
    ]

    print("==================================================")
    print("   ðŸŽï¸  SNN ARCHITECTURE BENCHMARK (Phase 2.3)   ")
    print("==================================================")

    for name, conf in models:
        suite.run_smoke_test(name, conf)

        if args.mode == "full":
            suite.run_efficiency_benchmark(name)
            # Note: Training benchmark is skipped to focus on Latency Gap Analysis

    suite.save_report()
    print("\nðŸ Benchmark Suite Completed.")


if __name__ == "__main__":
    main()
