# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: scripts/run_benchmark_suite.py
# Title: SNN Benchmark Suite v2.2 (Latency/Throughput Split)
# Description:
#   ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–é€Ÿåº¦ï¼ˆLatencyï¼‰ã¨å­¦ç¿’èƒ½åŠ›ï¼ˆThroughputï¼‰ã‚’åˆ†é›¢ã—ã¦æ¸¬å®šã™ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆã€‚
#   ä¿®æ­£: Efficiency Testã§ T=1 ã‚’å¼·åˆ¶ã—ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¿œç­”æ€§èƒ½ã‚’æ­£ã—ãè©•ä¾¡ã™ã‚‹ã€‚

import sys
import os
import torch
import torch.nn as nn
import logging
import argparse
import time
import json
import datetime
from typing import Dict, Any, cast
from omegaconf import OmegaConf

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã®è¨­å®š
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../"))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    stream=sys.stdout
)
logger = logging.getLogger("Benchmark")

try:
    from snn_research.core.snn_core import SNNCore
except ImportError as e:
    print(f"âŒ Import Error: {e}")
    sys.exit(1)


class BenchmarkSuite:
    def __init__(self, output_dir: str = "benchmarks/results"):
        print("âš™ï¸ Initializing Benchmark Suite v2.2...")
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)

        # ãƒ‡ãƒã‚¤ã‚¹é¸æŠ
        if torch.cuda.is_available():
            self.device = "cuda"
        elif torch.backends.mps.is_available():
            self.device = "mps"
        else:
            self.device = "cpu"

        print(f"   -> Device selected: {self.device}")

        self.results: Dict[str, Any] = {
            "timestamp": str(datetime.datetime.now()),
            "hardware": self.device,
            "tests": {}
        }

    def _get_dummy_config(self, architecture: str) -> Dict[str, Any]:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç”¨ã®å…±é€šè¨­å®š"""
        return {
            "architecture_type": architecture,
            "vocab_size": 100,
            "d_model": 64,
            "num_layers": 2,
            "time_steps": 16,  # Trainingç”¨ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
            "neuron_config": {"base_threshold": 1.0}
        }

    def run_smoke_test(self, model_name: str, config_path: str):
        """ã‚¹ãƒ¢ãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ: æ§‹ç¯‰ã¨æ¨è«–ã®ç¢ºèª"""
        print(f"\nğŸ§ª [Smoke Test] {model_name} ... ", end="", flush=True)

        try:
            # Configèª­ã¿è¾¼ã¿
            if os.path.exists(config_path):
                conf = OmegaConf.load(config_path)
                model_config = OmegaConf.to_container(
                    conf.model if 'model' in conf else conf, resolve=True)
            else:
                arch = "sformer" if "SFormer" in model_name else "dsa_transformer"
                model_config = self._get_dummy_config(arch)

            model_config = cast(Dict[str, Any], model_config)
            vocab_size = int(model_config.get("vocab_size", 100))

            model = SNNCore(config=model_config,
                            vocab_size=vocab_size).to(self.device)
            model.eval()

            # å…¥åŠ›é•·: 16
            input_ids = torch.randint(0, vocab_size, (1, 16)).to(self.device)
            with torch.no_grad():
                _ = model(input_ids)

            print("âœ… PASSED")
            self.results["tests"][f"smoke_{model_name}"] = {"status": "PASSED"}

        except Exception as e:
            print(f"âŒ FAILED: {e}")
            self.results["tests"][f"smoke_{model_name}"] = {
                "status": "FAILED", "error": str(e)}

    def run_training_benchmark(self, model_name: str, steps: int = 50):
        """
        å­¦ç¿’ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ (Throughput):
        T=16 ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ä¸€æ‹¬å‡¦ç†ã™ã‚‹éš›ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’æ¸¬å®šã€‚
        """
        print(
            f"\nğŸ“ˆ [Training Bench] {model_name} ({steps} steps, T=16) ... ", end="", flush=True)

        try:
            # ãƒ¢ãƒ‡ãƒ«æº–å‚™
            arch = "sformer" if "SFormer" in model_name else "dsa_transformer"
            config = self._get_dummy_config(arch)
            config["time_steps"] = 16  # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆæ¸¬ç”¨
            config["vocab_size"] = 10

            model = SNNCore(config=config, vocab_size=10).to(self.device)
            model.train()

            optimizer = torch.optim.AdamW(model.parameters(), lr=0.005)
            criterion = nn.CrossEntropyLoss()

            # ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ: [1, 2, 1, 2...]
            batch_size = 8
            seq_len = 16
            x = torch.tensor([[1, 2] * (seq_len // 2)
                             for _ in range(batch_size)]).to(self.device)
            y = torch.tensor([[2, 1] * (seq_len // 2)
                             for _ in range(batch_size)]).to(self.device)  # Next token

            start_time = time.time()
            initial_loss = 0.0
            final_loss = 0.0

            for step in range(steps):
                optimizer.zero_grad()
                outputs = model(x)  # (B, T, V) or (B, V) or Tuple

                # --- Output Handling Fix ---
                if isinstance(outputs, tuple):
                    outputs = outputs[0]

                if outputs.dim() == 3:
                    outputs_flat = outputs.reshape(-1, 10)
                    y_flat = y.reshape(-1)
                    loss = criterion(outputs_flat, y_flat)
                elif outputs.dim() == 2:
                    loss = criterion(outputs, y[:, -1])
                else:
                    raise ValueError(
                        f"Unexpected output shape: {outputs.shape}")
                # ---------------------------

                loss.backward()
                optimizer.step()

                if step == 0:
                    initial_loss = loss.item()
                final_loss = loss.item()

            duration = time.time() - start_time
            steps_per_sec = steps / duration

            print("âœ… DONE")
            print(f"   -> Speed: {steps_per_sec:.1f} steps/s (Batch T=16)")
            print(f"   -> Loss: {initial_loss:.4f} -> {final_loss:.4f}")

            self.results["tests"][f"train_{model_name}"] = {
                "status": "PASSED" if final_loss < initial_loss else "WARNING",
                "steps_per_sec": steps_per_sec,
                "initial_loss": initial_loss,
                "final_loss": final_loss
            }

        except Exception as e:
            print(f"âŒ FAILED: {e}")
            self.results["tests"][f"train_{model_name}"] = {
                "status": "FAILED", "error": str(e)}

    def run_efficiency_benchmark(self, model_name: str):
        """
        åŠ¹ç‡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ (Latency):
        T=1 (å˜ä¸€ã‚¹ãƒ†ãƒƒãƒ—) ã®æ¨è«–ã‚’è¡Œã„ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¿œç­”æ€§èƒ½ï¼ˆReaction Timeï¼‰ã‚’æ¸¬å®šã€‚
        """
        print(
            f"\nâš¡ [Efficiency Test] {model_name} (T=1 Latency) ... ", end="", flush=True)
        try:
            arch = "sformer" if "SFormer" in model_name else "dsa_transformer"
            config = self._get_dummy_config(arch)
            # ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¨ˆæ¸¬ã®ãŸã‚ T=1 ã‚’å¼·åˆ¶
            config["time_steps"] = 1

            model = SNNCore(config=config, vocab_size=100).to(self.device)
            model.eval()

            # å…¥åŠ›é•·: 1 (å˜ä¸€ãƒˆãƒ¼ã‚¯ãƒ³/ãƒ•ãƒ¬ãƒ¼ãƒ )
            input_ids = torch.randint(0, 100, (1, 1)).to(self.device)

            # Warmup
            for _ in range(10):
                _ = model(input_ids)

            num_runs = 100
            total_spikes = 0.0

            # åŒæœŸå‡¦ç† (CUDA/MPS)
            if self.device == "cuda":
                torch.cuda.synchronize()
            elif self.device == "mps":
                torch.mps.synchronize()

            start_time = time.time()
            with torch.no_grad():
                for _ in range(num_runs):
                    if hasattr(model.model, 'reset_spike_stats'):
                        model.model.reset_spike_stats()  # type: ignore

                    # return_spikes=True ã§ã‚¹ãƒ‘ã‚¤ã‚¯æ•°ã‚‚è¨ˆæ¸¬
                    out = model(input_ids, return_spikes=True)

                    spike_data = None
                    if isinstance(out, tuple) and len(out) >= 2:
                        spike_data = out[1]

                    if spike_data is not None:
                        if isinstance(spike_data, torch.Tensor):
                            total_spikes += spike_data.sum().item()
                        elif isinstance(spike_data, list):
                            total_spikes += sum([s.sum().item()
                                                for s in spike_data if isinstance(s, torch.Tensor)])

            if self.device == "cuda":
                torch.cuda.synchronize()
            elif self.device == "mps":
                torch.mps.synchronize()

            end_time = time.time()
            avg_latency = ((end_time - start_time) / num_runs) * 1000
            avg_spikes = total_spikes / num_runs

            print("âœ… DONE")
            print(f"   -> Latency: {avg_latency:.2f} ms / step")

            status = "PASSED"
            if avg_latency > 10.0:
                print("   âš ï¸ WARNING: Latency > 10ms (Target not met)")
                status = "WARNING"

            self.results["tests"][f"efficiency_{model_name}"] = {
                "status": status,
                "latency_ms": avg_latency,
                "avg_spikes": avg_spikes,
                "note": "Measured with T=1 input"
            }
        except Exception as e:
            print(f"âŒ FAILED: {e}")

    def save_report(self):
        json_path = os.path.join(self.output_dir, "benchmark_latest.json")
        with open(json_path, 'w') as f:
            json.dump(self.results, f, indent=2)
        print(f"\nğŸ“ Report saved to {json_path}")


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--mode", type=str, default="all",
                        choices=["smoke", "full", "all"])
    # äº’æ›æ€§å¼•æ•°
    parser.add_argument("--experiment", type=str)
    parser.add_argument("--tag", type=str)
    parser.add_argument("--model_config", type=str)
    parser.add_argument("--epochs", type=int)
    parser.add_argument("--batch_size", type=int)

    args = parser.parse_args()
    suite = BenchmarkSuite()

    # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‹ã‚‰ã®å‘¼ã³å‡ºã—å¯¾å¿œ
    if args.experiment == "health_check_comparison" or args.tag == "HealthCheck":
        suite.run_smoke_test("HealthCheck_Model", args.model_config or "")
        suite.save_report()
        return

    # é€šå¸¸å®Ÿè¡Œ
    models = [
        ("SFormer_T1", "configs/models/phase3_sformer.yaml"),
        ("SNN_DSA", "configs/models/dsa_transformer.yaml")
    ]

    for name, conf in models:
        suite.run_smoke_test(name, conf)

        if args.mode in ["all", "full"]:
            suite.run_efficiency_benchmark(name)
            suite.run_training_benchmark(name)

    suite.save_report()
    print("ğŸ Benchmark Suite Completed.")


if __name__ == "__main__":
    main()
