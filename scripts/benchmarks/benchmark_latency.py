# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: scripts/benchmarks/benchmark_latency.py
# æ—¥æœ¬èªã‚¿ã‚¤ãƒˆãƒ«: Benchmark Latency Tool
# ç›®çš„: ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’æ¸¬å®šã—ã€ãƒ­ã‚°ã«å‡ºåŠ›ã™ã‚‹ã€‚

import time
import torch
import logging
import sys
import os
from omegaconf import OmegaConf
from snn_research.core.snn_core import SNNCore

# Configure logging to stdout
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    stream=sys.stdout
)
logger = logging.getLogger(__name__)


def benchmark(config_path: str):
    print(f"ğŸš€ Benchmarking config: {config_path}")
    logger.info(f"ğŸš€ Benchmarking config: {config_path}")

    if not os.path.exists(config_path):
        error_msg = f"âŒ Config file not found: {config_path}"
        logger.error(error_msg)
        print(error_msg)
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šãŒå­˜åœ¨ã—ãªã„å ´åˆã¯çµ‚äº†ã›ãšè­¦å‘Š
        return None

    try:
        # Load config
        conf = OmegaConf.load(config_path)

        # Initialize model
        vocab_size = 1000
        # configæ§‹é€ ã®å …ç‰¢æ€§ãƒã‚§ãƒƒã‚¯
        model_conf = conf.model if hasattr(conf, 'model') else conf

        logger.info("Initializing SNNCore...")
        model = SNNCore(config=model_conf, vocab_size=vocab_size,
                        backend="spikingjelly")
        model.eval()

        # Dummy input
        batch_size = 1
        seq_len = getattr(model_conf, 'time_steps', 16)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))

        # Warmup
        logger.info("Warmup...")
        with torch.no_grad():
            for _ in range(5):
                _ = model(input_ids)

        # Measure latency
        logger.info("Measuring latency...")
        latencies = []
        with torch.no_grad():
            for _ in range(20):
                start_time = time.time()
                _ = model(input_ids)
                end_time = time.time()
                latencies.append((end_time - start_time) * 1000)  # ms

        avg_latency = sum(latencies) / len(latencies)
        result_msg = f"âš¡ï¸ Average Inference Latency: {avg_latency:.2f} ms"
        logger.info(result_msg)
        print(result_msg)

        with open("latency_result.txt", "w") as f:
            f.write(f"Average Inference Latency: {avg_latency:.2f} ms\n")

        return avg_latency

    except Exception as e:
        logger.error(f"âŒ Benchmark failed: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    config_path = sys.argv[1] if len(
        sys.argv) > 1 else "configs/models/large_scale.yaml"

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹è£œæ­£
    if not os.path.exists(config_path):
        # è©¦ã—ã«ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆç›´ä¸‹ã‚’æ¢ã™
        potential_path = os.path.join(os.getcwd(), config_path)
        if os.path.exists(potential_path):
            config_path = potential_path

    benchmark(config_path)
