# 🎯 Neuromorphic AGI Project - Unified Objectives (v2.1)


## 📌 Most important  

・誤差逆伝播法は使わないこと  
・行列演算は使わないこと  
・GPUに依存しないこと  


  
## 📌 Mission Statement

人間の脳の効率性、可塑性、自律性を模倣した**ニューロモルフィックAGIプロトタイプ**を開発し、**ANNを精度・効率・速度の全てで上回る**システムを超省エネを実現し、コンシューマーハードウェア上で実現すること。継続学習を行えるようにすること。  

**Current Status:** ✅ **Phase 1 完了 / Phase 2 進行中**

---

## 🛠️ Coding Rules (v1)

1. **ファイル管理**: workspaceディレクトリ以外にキャッシュ・データ・モデルなどのファイルを生成・保存しないこと
2. **ディレクトリ構造**: 新規ファイルはディレクトリの整理を維持すること

---

## 📊 Phase 1: Core Performance (完了) ✅

### 達成した成果

| カテゴリ | Phase 1 目標 | 達成値 | 状態 |
|:--------|:-----------|:------|:-----|
| **推論レイテンシ** | < 10ms | **3.02ms** (d=512, SFormer, T=1) | ✅ 目標の30%達成 |
| **消費電力** | ANN比 1/50以下 | 1/20~1/100 | ✅ 達成 |
| **学習安定性** | ≥ 95% | **81.25%** | ⚠️ 改善継続中 |

### 主要成果

#### 1. 高速推論 (System 1) ✅
- **実装**: Spiking Transformer (SFormer)
- **成果**: Apple M4 MPSで **3.02ms** 達成 (T=1タイムステップ)
- **意義**: リアルタイムロボット制御が可能なレベル

#### 2. 深い推論 (System 2) ✅
- **実装**: BitSpikeMamba (1.58bit量子化)
- **成果**: 高エントロピー時のみ起動、80-150msで複雑推論
- **意義**: エネルギー効率的な「遅い思考」の実現

#### 3. 自律性とホメオスタシス ✅
- **実装**: Sleep Consolidator + Intrinsic Motivator
- **成果**: 疲労時に自律的に睡眠し記憶を定着
- **意義**: ユーザー介入なしでの継続学習

#### 4. 社会的知性 ✅
- **実装**: Teacher-Student Communication Protocol
- **成果**: 直接ラベルなしで精度 9% → **32%** に改善
- **意義**: 集合知と文化的伝達の実証

---

## 🚀 Phase 2: ANNを超える (進行中)

### 2025年時点のSNN vs ANN現状分析

#### CIFAR-10ベンチマーク

| アーキテクチャ | タイプ | 精度 (%) | タイムステップ | 備考 |
|:-------------|:------|:--------|:-------------|:-----|
| **STAA-SNN (2025)** | SNN | **97.14%** | T=4 | CVPR 2025, SOTA SNN |
| **PMSM (2025)** | SNN (変換) | **98.5%** | **T=1** | ViT-S, ほぼロスレス変換 |
| Wide ResNet | ANN | **97.69%** | N/A | ANN State-of-art |
| EKAResNet (2025) | ANN | 95.84% | N/A | KAN強化ResNet |
| **現プロジェクト** | SNN | 95-97% | T=1 | Phase 1達成値 |

**分析**: 
- SNNは**既にANNと同等**の精度を達成 (PMSM: 98.5%)
- しかし、T=1での**ネイティブSNN学習**では未達成 (変換手法に依存)
- 我々の目標: **T=1でネイティブ学習97.5%以上**

#### ImageNetベンチマーク

| アーキテクチャ | タイプ | Top-1精度 (%) | Top-5精度 (%) | タイムステップ |
|:-------------|:------|:------------|:------------|:-------------|
| **PMSM (2025)** | SNN (変換) | **81.6%** | N/A | **T=1** |
| ANN-Guided SNN (2025) | SNN | 71.76% | N/A | T=4 |
| ResNet-34 | ANN | 76.32% | ~93% | N/A |
| STAA-SNN (2025) | SNN | 70.40% | N/A | T=4 |
| **現プロジェクト** | SNN | 未測定 | 未測定 | T=1 |

**分析**:
- ImageNetでSNNがANNを超えた例がある (PMSM: 81.6% vs ResNet-34: 76.32%)
- ただし、これは**変換手法**であり、ネイティブ学習ではない
- 我々の目標: **ImageNet Top-1で75%以上** (ResNet-34レベル、T=1ネイティブ学習)

### Phase 2 修正目標: ANNを超える3つの軸

#### 軸1: 精度でANNと同等以上 (Accuracy Parity)

| ベンチマーク | Phase 2 目標 | 2025 SNN SOTA | ANN SOTA | 差分 |
|:-----------|:-----------|:------------|:---------|:----|
| **CIFAR-10** | **≥ 97.5%** | 98.5% (変換) | 97.69% | 我々: ネイティブ学習で97.5% |
| **CIFAR-100** | **≥ 82%** | 89.3% (変換) | ~81.9% | ANNと同等 |
| **ImageNet (Top-1)** | **≥ 75%** | 81.6% (変換) | 76.32% | ResNet-34レベル |
| **ImageNet (Top-5)** | **≥ 92%** | 未公表 | ~93% | ANNと同等 |

**重要**: 
- 「変換」は事前学習済みANNをSNNに変換する手法
- 我々は**ネイティブSNN学習**でこれを達成する
- **タイムステップT=1**を維持 (従来SNNはT=4~8が標準)

#### 軸2: 効率でANNを圧倒 (Efficiency Dominance)

| 評価項目 | Phase 2 目標 | Phase 1 達成値 | ANN比較 |
|:--------|:-----------|:------------|:-------|
| **エネルギー効率** | **同精度でANN比1/100** | 1/20~1/100 | ✅ 維持 |
| **推論レイテンシ** | **< 5ms (T=1)** | 3.02ms | **ANNより高速** |
| **反射神経** | **< 1ms** | 未達成 | 新規目標 |
| **メモリ効率** | **ANN比1/10** | 未測定 | 新規目標 |
| **スパース発火率** | **< 5% active neurons** | 未測定 | 新規目標 |

**差別化ポイント**:
- エッジデバイスで**クラウドAIに不可能な応答速度** (< 5ms)
- 反射的タスク (ゲーム、ロボット制御) で**1ms以下**
- メモリフットプリントを1/10に削減 (組み込みデバイス対応)

#### 軸3: 能力でANNを拡張 (Capability Extension)

| 能力 | Phase 2 目標 | Phase 1 状態 | ANNとの比較 |
|:----|:-----------|:-----------|:----------|
| **Continual Learning** | Catastrophic Forgetting完全回避 | 部分的 | **SNN優位** |
| **Few-Shot Learning** | 10サンプルで90%精度 | 未測定 | **SNN優位** |
| **世界モデル構築** | 因果推論精度80% | 未実装 | ANNと同等目標 |
| **メタ認知** | 不確実性推定精度90% | 部分的 | **SNN優位** (System 2切替) |
| **マルチモーダル** | Vision + Language統合 | 未実装 | ANNと同等目標 |

**ANNにできない能力**:
1. **真の継続学習**: 過去の知識を忘れずに新規学習
2. **超少数サンプル学習**: 神経可塑性による高速適応
3. **動的リソース配分**: 自信度に応じた計算量調整
4. **生物学的妥当性**: 説明可能性とエネルギー代謝制御

---

## 🎯 Phase 2 具体的マイルストーン

### Short-term (2025 Q1-Q2)

#### マイルストーン 1: CIFAR-10で97.5%達成
- [ ] T=1ネイティブ学習で97.5%以上
- [ ] 推論レイテンシ < 5ms維持
- [ ] 学習安定性 95%達成

#### マイルストーン 2: エネルギー効率の定量化
- [ ] 同一精度(97%)でのANN比エネルギー測定
- [ ] 目標: ANN比1/100の実証
- [ ] スパース発火率 < 5%の達成

#### マイルストーン 3: 学習再現性の向上
- [ ] 学習安定性 95%達成 (現在81.25%)
- [ ] Surrogate Gradient精度向上
- [ ] STDP + SGハイブリッドの最適化

### Mid-term (2025 Q3-Q4)

#### マイルストーン 4: ImageNet展開
- [ ] ImageNet Top-1で75%達成
- [ ] ImageNet Top-5で92%達成
- [ ] T=1維持でのスケーラビリティ実証

#### マイルストーン 5: マルチモーダル統合
- [ ] Vision (CNN/ViT) + Language (SFormer) 完全統合
- [ ] d_model=1024でも推論速度 < 10ms
- [ ] マルチモーダルベンチマークで90%

#### マイルストーン 6: 継続学習の完成
- [ ] Catastrophic Forgetting完全回避
- [ ] 50タスク連続学習で精度低下 < 5%
- [ ] Few-Shot Learning (10サンプル) で90%

### Long-term (2026+)

#### マイルストーン 7: 反射神経の実現
- [ ] 推論レイテンシ < 1ms (反射的タスク)
- [ ] リアルタイムゲームAIでの実証
- [ ] ロボット制御での応用

#### マイルストーン 8: 世界モデルとメタ認知
- [ ] 因果推論精度 80%
- [ ] Why推論の実装
- [ ] 不確実性推定精度 90%

#### マイルストーン 9: ニューロモルフィックハードウェア
- [ ] Loihi 3 / SpiNNaker 3対応
- [ ] Event-driven Transformer実装
- [ ] オンチップ学習の実証

---

## 🏆 Technical Specifications (最終版)

| Component | Implementation | Status | Phase 2 Target |
|:----------|:--------------|:-------|:--------------|
| **Spike Encoding** | Universal Encoder / Visual Tokenizer | ✅ Stable | Temporal precision < 1ms |
| **Neuron Model** | Adaptive LIF / Scale-and-Fire | ✅ Optimized | 発火率 0.1-2 Hz (平均) |
| **Backbone 1** | Spiking Transformer (SFormer) | ✅ T=1 Ready | d_model 1024対応 |
| **Backbone 2** | BitSpikeMamba (1.58bit) | ✅ Integrated | 量子化誤差 ±1ms以内 |
| **Learning Rule** | STDP + Surrogate Gradient + Distillation | ✅ Hybrid | 再現性 95%+ |
| **Hardware** | CUDA / MPS (Metal Performance Shaders) | ✅ Supported | Loihi 3 / SpiNNaker 3 |

---

## 📈 競合比較と差別化戦略 (修正版)

### ANNとの精度ギャップ分析 (2025年版)

| 分野 | SNN現状精度 (2025) | ANN精度 | ギャップ | Phase 2目標 |
|:----|:------------------|:-------|:-------|:----------|
| **CIFAR-10** | **97.14%** (STAA-SNN) / **98.5%** (変換) | 97.69% | **既に同等** | 97.5% (ネイティブ学習) |
| **ImageNet** | **81.6%** (変換) / 70.4% (ネイティブ) | 76.32% | 変換で**+5.3%優位** | 75% (ネイティブ学習) |
| **CIFAR-100** | **89.3%** (変換) / 82.05% (ネイティブ) | ~81.9% | **既に同等** | 82% (ネイティブ学習) |
| ノイズ耐性 | **高い** | 中程度 | **SNN優位** | 維持 |
| エネルギー効率 | 1/20~1/100 | 基準 | **SNN優位** | 1/100 (同精度) |
| 学習収束安定性 | 70-80% | 95%+ | **15-25%劣位** | 95%+ |

**重要な発見**:
- **2025年時点でSNNは精度でANNと同等以上**
- 課題は「変換手法」ではなく「ネイ�ィブ学習」で達成すること
- 学習安定性がボトルネック (70-80% → 95%+へ改善必要)

### ボトルネックと改善戦略 (修正版)

| 領域 | 2025年時点の課題 | Phase 2での対策 |
|:----|:---------------|:-------------|
| 学習段階 | ネイティブ学習での精度ギャップ | Surrogate Gradient + STDP最適化 |
| 伝達段階 | T=1での時間情報損失 | Temporal precision < 1ms保証 |
| 実装段階 | 量子化誤差 | スパイク時刻誤差 ±1ms以内制御 |
| モデル構造 | ネイティブSNN構造の未成熟 | SFormer + BitSpikeMambaの最適化 |
| **学習安定性** | **再現性70-80%** | **95%+へ向上 (最優先課題)** |

### 差別化ポイント (修正版)

#### 1. ANNと同等の精度 (Accuracy Parity)
- **目標**: CIFAR-10で97.5%、ImageNetで75% (ネイティブ学習、T=1)
- **戦略**: 変換手法に頼らず、ネイティブSNN学習で達成
- **意義**: SNNの真の潜在能力を実証

#### 2. 圧倒的な効率 (Efficiency Dominance)
- **目標**: 同精度でANN比1/100のエネルギー効率
- **戦略**: スパース発火 < 5%、推論 < 5ms
- **意義**: エッジデバイスでクラウドAIに不可能な応答速度

#### 3. ANNにできない能力 (Unique Capabilities)
- **目標**: Continual Learning、Few-Shot Learning、メタ認知
- **戦略**: 生物学的可塑性の活用
- **意義**: 真のAGIへの道筋

#### 4. 生物学的妥当性 (Biological Plausibility)
- **目標**: 誤差逆伝播に依存しない学習
- **戦略**: STDP + Surrogate Gradientハイブリッド
- **意義**: 説明可能性とエネルギー代謝制御

---

## 🔬 Technology Roadmap (2025-2030)

| カテゴリ | 有望技術 | 実用化見通し | Phase | 優先度 |
|:--------|:--------|:-----------|:------|:------|
| 学習 | Surrogate Gradient + STDP Hybrid | 2025-2026 | Phase 2 | **最優先** |
| 精度 | ネイティブSNN学習の最適化 | 2025-2026 | Phase 2 | **最優先** |
| 構造 | Event-driven Transformer (SNN版) | 2027-2029 | Phase 3 | 高 |
| 実装 | Loihi 3 / SpiNNaker 3 | 2026-2028 | Phase 3 | 中 |
| 評価 | Neuromorphic Benchmark Suite | 2025-2026 | Phase 2 | 高 |

---

## 📊 Benchmark Summary (最終実行結果)

### 推論速度
- **SFormer (d=512)**: **3.02ms** (Apple M4 MPS, T=1)
- **Hybrid Switching**: < 20ms (通常) / ~100ms (複雑)

### 学習性能
- **Learning Stability**: **81.25%** (目標95%に向け改善中)
- **Teacher-Student Transfer**: 9% → 32% (ラベルなし)

### 文明シミュレーション
- **Knowledge Growth**: 50世代で **30倍** 増加 (Genesis/Eden)

---

## 🎯 成功基準の明確化

### Phase 2完了の定義

以下の**3つ全て**を達成した時点でPhase 2完了とする:

1. **精度基準** (最優先):
   - CIFAR-10: ≥ 97.5% (T=1、ネイティブ学習)
   - ImageNet: ≥ 75% Top-1 (T=1、ネイティブ学習)
   - 学習安定性: ≥ 95%

2. **効率基準**:
   - 同精度でANN比エネルギー効率 ≥ 1/100
   - 推論レイテンシ < 5ms (T=1)
   - スパース発火率 < 5%

3. **能力基準**:
   - Continual Learning: 50タスクで精度低下 < 5%
   - Few-Shot Learning: 10サンプルで90%
   - メタ認知: 不確実性推定精度 90%

---

## 📜 Conclusion

### 2025年の現実

- **SNNは既にANNと同等の精度を達成している** (変換手法使用時)
- 課題は「変換手法に頼らないネイティブSNN学習」で同等精度を達成すること
- 学習安定性 (70-80% → 95%+) が最大のボトルネック

### Phase 2の挑戦

我々のプロジェクトは、**ネイティブSNN学習**でANNと同等の精度を達成し、かつエネルギー効率・速度・継続学習能力でANNを圧倒する世界初のシステムを目指す。

これは単なる「ANNの置き換え」ではなく、**SNNの真の潜在能力**を引き出し、人工知能の新しいパラダイムを創造する試みである。

---

## 📝 Implementation Priorities (優先順位)

実装の方向性に迷う時は**人間の脳の挙動を重視**すること:

1. **学習安定性** (最優先: 81.25% → 95%+)
2. **精度** (ネイティブ学習でCIFAR-10 97.5%、ImageNet 75%)
3. エネルギー効率 (代謝コスト最小化)
4. スパース性 (必要最小限の発火)
5. 可塑性 (継続的学習)
6. 自律性 (外部介入最小化)
7. 社会性 (知識共有と文化伝達)

**Phase 2の最重要課題は「学習安定性」と「ネイティブ学習での高精度」である。**
